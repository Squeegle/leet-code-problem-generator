{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Transformation and Cleaning Masterclass 🧹📊\n",
        "\n",
        "## A Comprehensive Learning Journey for Data Science and Analysis\n",
        "\n",
        "Welcome to this comprehensive learning document for transforming and cleaning data using Python! This notebook will take you from basic concepts to advanced techniques used in professional Data Science and Data Analysis workflows.\n",
        "\n",
        "### 🎯 Learning Objectives\n",
        "By the end of this masterclass, you will be able to:\n",
        "- Identify and handle common data quality issues\n",
        "- Transform data into analysis-ready formats\n",
        "- Apply appropriate cleaning techniques for different data types\n",
        "- Build robust data preprocessing pipelines\n",
        "- Handle real-world messy datasets with confidence\n",
        "\n",
        "### 📚 Course Structure\n",
        "This notebook is organized into progressive modules:\n",
        "\n",
        "1. **Foundation** - Basic concepts and setup\n",
        "2. **Data Inspection & Understanding** - Getting to know your data\n",
        "3. **Missing Data Mastery** - Handling incomplete information\n",
        "4. **Data Type Transformations** - Converting and standardizing formats\n",
        "5. **Duplicate Detection & Removal** - Ensuring data uniqueness\n",
        "6. **Text Data Cleaning** - Working with strings and text\n",
        "7. **Outlier Detection & Treatment** - Identifying and handling anomalies\n",
        "8. **Data Transformation Techniques** - Scaling, encoding, and feature engineering\n",
        "9. **Data Reshaping & Aggregation** - Restructuring data for analysis\n",
        "10. **Advanced Topics** - Time series, large datasets, and automation\n",
        "11. **Real-World Case Studies** - Putting it all together\n",
        "\n",
        "### 💡 How to Use This Notebook\n",
        "- Run each cell sequentially to build understanding\n",
        "- Experiment with the provided examples\n",
        "- Try the practice exercises at the end of each section\n",
        "- Reference back to sections as needed during your data science projects\n",
        "\n",
        "Let's begin our journey into the world of data transformation and cleaning!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 1. Foundation 🏗️\n",
        "\n",
        "## Essential Libraries and Concepts\n",
        "\n",
        "Before we dive into data cleaning, let's establish our foundation with the essential libraries and understand what makes data \"dirty\" or \"messy.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Library Versions:\n",
            "pandas: 2.3.0\n",
            "numpy: 2.2.6\n",
            "matplotlib: 3.10.3\n",
            "seaborn: 0.13.2\n",
            "\n",
            "✅ Foundation libraries loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for data cleaning and transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for better visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Print library versions for reproducibility\n",
        "print(\"📦 Library Versions:\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {matplotlib.__version__}\")  # Fixed: use matplotlib.__version__ instead of plt.__version__\n",
        "print(f\"seaborn: {sns.__version__}\")\n",
        "\n",
        "print(\"\\n✅ Foundation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Library Versions:\n",
            "pandas: 2.3.0\n",
            "numpy: 2.2.6\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.pyplot' has no attribute '__version__'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n",
            "\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpd.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnumpy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmatplotlib: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseaborn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msns.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Foundation libraries loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\n",
            "\u001b[31mAttributeError\u001b[39m: module 'matplotlib.pyplot' has no attribute '__version__'"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for data cleaning and transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for better visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Print library versions for reproducibility\n",
        "print(\"📦 Library Versions:\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {matplotlib.__version__}\")  # Fixed: use matplotlib.__version__ instead of plt.__version__\n",
        "print(f\"seaborn: {sns.__version__}\")\n",
        "\n",
        "print(\"\\n✅ Foundation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Library Versions:\n",
            "pandas: 2.3.0\n",
            "numpy: 2.2.6\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.pyplot' has no attribute '__version__'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n",
            "\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpd.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnumpy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmatplotlib: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseaborn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msns.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Foundation libraries loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\n",
            "\u001b[31mAttributeError\u001b[39m: module 'matplotlib.pyplot' has no attribute '__version__'"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for data cleaning and transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for better visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Print library versions for reproducibility\n",
        "print(\"📦 Library Versions:\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {plt.__version__}\")\n",
        "print(f\"seaborn: {sns.__version__}\")\n",
        "\n",
        "print(\"\\n✅ Foundation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Library Versions:\n",
            "pandas: 2.3.0\n",
            "numpy: 2.2.6\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.pyplot' has no attribute '__version__'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n",
            "\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpd.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnumpy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmatplotlib: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseaborn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msns.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Foundation libraries loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\n",
            "\u001b[31mAttributeError\u001b[39m: module 'matplotlib.pyplot' has no attribute '__version__'"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for data cleaning and transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for better visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Print library versions for reproducibility\n",
        "print(\"📦 Library Versions:\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {plt.__version__}\")\n",
        "print(f\"seaborn: {sns.__version__}\")\n",
        "\n",
        "print(\"\\n✅ Foundation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Library Versions:\n",
            "pandas: 2.3.0\n",
            "numpy: 2.2.6\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.pyplot' has no attribute '__version__'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n",
            "\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpd.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnumpy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmatplotlib: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseaborn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msns.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Foundation libraries loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\n",
            "\u001b[31mAttributeError\u001b[39m: module 'matplotlib.pyplot' has no attribute '__version__'"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for data cleaning and transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for better visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Print library versions for reproducibility\n",
        "print(\"📦 Library Versions:\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {plt.__version__}\")\n",
        "print(f\"seaborn: {sns.__version__}\")\n",
        "\n",
        "print(\"\\n✅ Foundation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Library Versions:\n",
            "pandas: 2.3.0\n",
            "numpy: 2.2.6\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.pyplot' has no attribute '__version__'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n",
            "\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpd.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnumpy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmatplotlib: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseaborn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msns.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Foundation libraries loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\n",
            "\u001b[31mAttributeError\u001b[39m: module 'matplotlib.pyplot' has no attribute '__version__'"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for data cleaning and transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for better visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Print library versions for reproducibility\n",
        "print(\"📦 Library Versions:\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {plt.__version__}\")\n",
        "print(f\"seaborn: {sns.__version__}\")\n",
        "\n",
        "print(\"\\n✅ Foundation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Library Versions:\n",
            "pandas: 2.3.0\n",
            "numpy: 2.2.6\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.pyplot' has no attribute '__version__'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n",
            "\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpd.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnumpy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmatplotlib: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseaborn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msns.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Foundation libraries loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\n",
            "\u001b[31mAttributeError\u001b[39m: module 'matplotlib.pyplot' has no attribute '__version__'"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for data cleaning and transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for better visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Print library versions for reproducibility\n",
        "print(\"📦 Library Versions:\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {plt.__version__}\")\n",
        "print(f\"seaborn: {sns.__version__}\")\n",
        "\n",
        "print(\"\\n✅ Foundation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Library Versions:\n",
            "pandas: 2.3.0\n",
            "numpy: 2.2.6\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.pyplot' has no attribute '__version__'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n",
            "\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpd.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnumpy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmatplotlib: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseaborn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msns.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Foundation libraries loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\n",
            "\u001b[31mAttributeError\u001b[39m: module 'matplotlib.pyplot' has no attribute '__version__'"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for data cleaning and transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for better visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Print library versions for reproducibility\n",
        "print(\"📦 Library Versions:\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {plt.__version__}\")\n",
        "print(f\"seaborn: {sns.__version__}\")\n",
        "\n",
        "print(\"\\n✅ Foundation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Library Versions:\n",
            "pandas: 2.3.0\n",
            "numpy: 2.2.6\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.pyplot' has no attribute '__version__'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n",
            "\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpd.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnumpy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmatplotlib: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseaborn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msns.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Foundation libraries loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\n",
            "\u001b[31mAttributeError\u001b[39m: module 'matplotlib.pyplot' has no attribute '__version__'"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for data cleaning and transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for better visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Print library versions for reproducibility\n",
        "print(\"📦 Library Versions:\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {plt.__version__}\")\n",
        "print(f\"seaborn: {sns.__version__}\")\n",
        "\n",
        "print(\"\\n✅ Foundation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Understanding Data Quality Issues\n",
        "\n",
        "Real-world data is rarely perfect. Let's understand the common types of data quality problems we encounter:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample \"messy\" dataset to demonstrate common data quality issues\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic messy data\n",
        "messy_data = {\n",
        "    'customer_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
        "    'name': ['John Doe', 'jane smith', 'MIKE JONES', 'Sarah Wilson', 'john doe', \n",
        "             'Bob Brown', None, 'Alice Johnson', 'Tom Davis', 'jane smith',\n",
        "             'Lisa Miller', 'David Lee', 'Emma White', 'Chris Black', 'Anna Green'],\n",
        "    'email': ['john@email.com', 'Jane@Email.Com', 'mike@email.com', 'sarah@invalid',\n",
        "              'john@email.com', 'bob@email.com', 'missing@email.com', 'alice@email.com',\n",
        "              'tom@email.com', 'Jane@Email.Com', '', 'david@email.com', \n",
        "              'emma@email.com', 'chris@email.com', np.nan],\n",
        "    'age': [25, 30, None, 22, 25, 35, 28, 150, 29, 30, 32, 27, 24, 31, 26],\n",
        "    'salary': [50000, 60000, 70000, None, 50000, 80000, 55000, 90000, 65000, \n",
        "               60000, 75000, 58000, 52000, 67000, 54000],\n",
        "    'department': ['Sales', 'marketing', 'ENGINEERING', 'Sales', 'Sales', \n",
        "                   'Engineering', 'Marketing', 'sales', 'Engineering', 'marketing',\n",
        "                   'HR', 'Engineering', 'Sales', 'Marketing', 'HR'],\n",
        "    'join_date': ['2020-01-15', '2021/03/10', '2019-12-01', '2022-06-15', '2020-01-15',\n",
        "                  '2021-08-20', '2020-11-05', 'invalid_date', '2021-05-10', '2021/03/10',\n",
        "                  '2022-01-30', '2020-09-12', '2021-11-25', '2022-04-18', '2020-07-08'],\n",
        "    'performance_score': [85.5, 92.0, 78.5, None, 85.5, 95.0, 88.5, 72.0, 91.5, 92.0,\n",
        "                          89.0, 87.5, 83.0, 90.5, 86.0]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df_messy = pd.DataFrame(messy_data)\n",
        "\n",
        "print(\"🔍 Sample Messy Dataset:\")\n",
        "print(\"=\" * 50)\n",
        "print(df_messy)\n",
        "\n",
        "print(\"\\n📊 Data Info:\")\n",
        "print(\"=\" * 30)\n",
        "print(df_messy.info())\n",
        "\n",
        "print(\"\\n🚨 Data Quality Issues Present:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"• Missing values (None, NaN, empty strings)\")\n",
        "print(\"• Inconsistent formatting (case variations)\")\n",
        "print(\"• Duplicate records\")\n",
        "print(\"• Invalid data (age = 150, invalid email formats)\")\n",
        "print(\"• Inconsistent date formats\") \n",
        "print(\"• Data type issues\")\n",
        "print(\"• Inconsistent categorical values\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 2. Data Inspection & Understanding 🔍\n",
        "\n",
        "## Getting to Know Your Data\n",
        "\n",
        "The first step in any data cleaning process is understanding what you're working with. Let's explore various techniques to inspect and understand our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive data inspection toolkit\n",
        "\n",
        "def comprehensive_data_inspection(df, sample_size=5):\n",
        "    \"\"\"\n",
        "    Perform a comprehensive inspection of a DataFrame\n",
        "    \n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The DataFrame to inspect\n",
        "    sample_size (int): Number of sample rows to display\n",
        "    \"\"\"\n",
        "    print(\"🔍 COMPREHENSIVE DATA INSPECTION REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 1. Basic Information\n",
        "    print(f\"\\n📏 Dataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
        "    print(f\"💾 Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # 2. Column Information\n",
        "    print(f\"\\n📋 Column Information:\")\n",
        "    print(\"-\" * 30)\n",
        "    for i, col in enumerate(df.columns, 1):\n",
        "        dtype = df[col].dtype\n",
        "        null_count = df[col].isnull().sum()\n",
        "        null_pct = (null_count / len(df)) * 100\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"{i:2d}. {col:<20} | {str(dtype):<12} | Nulls: {null_count:3d} ({null_pct:5.1f}%) | Unique: {unique_count}\")\n",
        "    \n",
        "    # 3. Sample Data\n",
        "    print(f\"\\n📄 First {sample_size} rows:\")\n",
        "    print(\"-\" * 30)\n",
        "    display(df.head(sample_size))\n",
        "    \n",
        "    print(f\"\\n📄 Last {sample_size} rows:\")\n",
        "    print(\"-\" * 30)\n",
        "    display(df.tail(sample_size))\n",
        "    \n",
        "    # 4. Statistical Summary\n",
        "    print(f\"\\n📊 Statistical Summary:\")\n",
        "    print(\"-\" * 30)\n",
        "    display(df.describe(include='all'))\n",
        "    \n",
        "    # 5. Missing Data Analysis\n",
        "    print(f\"\\n❌ Missing Data Analysis:\")\n",
        "    print(\"-\" * 30)\n",
        "    missing_data = df.isnull().sum()\n",
        "    missing_pct = (missing_data / len(df)) * 100\n",
        "    missing_summary = pd.DataFrame({\n",
        "        'Missing Count': missing_data,\n",
        "        'Missing Percentage': missing_pct\n",
        "    }).sort_values('Missing Count', ascending=False)\n",
        "    display(missing_summary[missing_summary['Missing Count'] > 0])\n",
        "    \n",
        "    # 6. Data Types Issues\n",
        "    print(f\"\\n⚠️  Potential Data Type Issues:\")\n",
        "    print(\"-\" * 30)\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            # Check for numeric values stored as strings\n",
        "            sample_values = df[col].dropna().astype(str).head(10).tolist()\n",
        "            print(f\"{col}: {sample_values}\")\n",
        "    \n",
        "    return missing_summary\n",
        "\n",
        "# Run comprehensive inspection on our messy dataset\n",
        "missing_analysis = comprehensive_data_inspection(df_messy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual inspection techniques\n",
        "\n",
        "def visualize_data_quality(df):\n",
        "    \"\"\"Create visualizations to understand data quality issues\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Data Quality Visualization Dashboard', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Missing Data Heatmap\n",
        "    ax1 = axes[0, 0]\n",
        "    sns.heatmap(df.isnull(), cbar=True, ax=ax1, cmap='viridis', \n",
        "                yticklabels=False, cbar_kws={'label': 'Missing Data'})\n",
        "    ax1.set_title('Missing Data Pattern', fontweight='bold')\n",
        "    ax1.set_xlabel('Columns')\n",
        "    \n",
        "    # 2. Missing Data Bar Chart\n",
        "    ax2 = axes[0, 1]\n",
        "    missing_counts = df.isnull().sum()\n",
        "    missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
        "    if len(missing_counts) > 0:\n",
        "        missing_counts.plot(kind='bar', ax=ax2, color='coral')\n",
        "        ax2.set_title('Missing Data Count by Column', fontweight='bold')\n",
        "        ax2.set_ylabel('Number of Missing Values')\n",
        "        ax2.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 3. Data Types Distribution\n",
        "    ax3 = axes[1, 0]\n",
        "    dtype_counts = df.dtypes.value_counts()\n",
        "    dtype_counts.plot(kind='pie', ax=ax3, autopct='%1.1f%%', startangle=90)\n",
        "    ax3.set_title('Data Types Distribution', fontweight='bold')\n",
        "    ax3.set_ylabel('')\n",
        "    \n",
        "    # 4. Unique Values per Column\n",
        "    ax4 = axes[1, 1]\n",
        "    unique_counts = df.nunique().sort_values(ascending=True)\n",
        "    unique_counts.plot(kind='barh', ax=ax4, color='lightblue')\n",
        "    ax4.set_title('Unique Values per Column', fontweight='bold')\n",
        "    ax4.set_xlabel('Number of Unique Values')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize our messy data quality\n",
        "print(\"📊 Data Quality Visualization:\")\n",
        "print(\"=\" * 40)\n",
        "visualize_data_quality(df_messy)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 3. Missing Data Mastery 🕳️\n",
        "\n",
        "## Handling Incomplete Information\n",
        "\n",
        "Missing data is one of the most common challenges in data science. Let's explore different strategies for identifying and handling missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Identify different types of missing values\n",
        "print(\"🔍 IDENTIFYING MISSING VALUES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create a copy of our messy data for missing value analysis\n",
        "df_missing = df_messy.copy()\n",
        "\n",
        "# Replace empty strings with NaN for consistent handling\n",
        "df_missing = df_missing.replace('', np.nan)\n",
        "\n",
        "print(\"Original missing value patterns:\")\n",
        "print(\"-\" * 35)\n",
        "for col in df_missing.columns:\n",
        "    total_missing = df_missing[col].isnull().sum()\n",
        "    if total_missing > 0:\n",
        "        print(f\"{col}: {total_missing} missing values\")\n",
        "        # Show unique patterns of missing data\n",
        "        missing_mask = df_missing[col].isnull()\n",
        "        print(f\"  Indices with missing data: {df_missing[missing_mask].index.tolist()}\")\n",
        "        print()\n",
        "\n",
        "# Advanced missing value detection\n",
        "def detect_missing_patterns(df):\n",
        "    \"\"\"Detect various patterns of missing data\"\"\"\n",
        "    \n",
        "    patterns = {}\n",
        "    \n",
        "    # 1. Explicit missing values (NaN, None)\n",
        "    explicit_missing = df.isnull().sum()\n",
        "    patterns['explicit'] = explicit_missing[explicit_missing > 0]\n",
        "    \n",
        "    # 2. Implicit missing values (empty strings, specific values)\n",
        "    implicit_missing = {}\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        # Count empty strings\n",
        "        empty_strings = (df[col] == '').sum()\n",
        "        # Count placeholder values\n",
        "        placeholder_values = df[col].isin(['N/A', 'n/a', 'NULL', 'null', 'missing', 'Missing']).sum()\n",
        "        \n",
        "        if empty_strings > 0 or placeholder_values > 0:\n",
        "            implicit_missing[col] = {\n",
        "                'empty_strings': empty_strings,\n",
        "                'placeholders': placeholder_values\n",
        "            }\n",
        "    \n",
        "    patterns['implicit'] = implicit_missing\n",
        "    \n",
        "    # 3. Pattern-based missing (e.g., all missing together)\n",
        "    # Find columns that tend to be missing together\n",
        "    missing_combinations = {}\n",
        "    for col1 in df.columns:\n",
        "        for col2 in df.columns:\n",
        "            if col1 != col2:\n",
        "                both_missing = ((df[col1].isnull()) & (df[col2].isnull())).sum()\n",
        "                if both_missing > 0:\n",
        "                    key = tuple(sorted([col1, col2]))\n",
        "                    if key not in missing_combinations:\n",
        "                        missing_combinations[key] = both_missing\n",
        "    \n",
        "    patterns['combinations'] = missing_combinations\n",
        "    \n",
        "    return patterns\n",
        "\n",
        "# Analyze missing patterns\n",
        "missing_patterns = detect_missing_patterns(df_missing)\n",
        "\n",
        "print(\"🎯 Missing Value Patterns Analysis:\")\n",
        "print(\"-\" * 35)\n",
        "print(f\"Explicit missing (NaN/None): {dict(missing_patterns['explicit'])}\")\n",
        "print(f\"Implicit missing: {missing_patterns['implicit']}\")\n",
        "print(f\"Combined missing patterns: {missing_patterns['combinations']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Missing Value Treatment Strategies\n",
        "\n",
        "print(\"🛠️ MISSING VALUE TREATMENT STRATEGIES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Strategy 1: Deletion Methods\n",
        "print(\"1️⃣ DELETION METHODS\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Remove rows with any missing values\n",
        "df_drop_any = df_missing.dropna()\n",
        "print(f\"Original shape: {df_missing.shape}\")\n",
        "print(f\"After dropping rows with ANY missing: {df_drop_any.shape}\")\n",
        "\n",
        "# Remove rows with all missing values\n",
        "df_drop_all = df_missing.dropna(how='all')\n",
        "print(f\"After dropping rows with ALL missing: {df_drop_all.shape}\")\n",
        "\n",
        "# Remove rows with missing values in specific columns\n",
        "df_drop_subset = df_missing.dropna(subset=['age', 'salary'])\n",
        "print(f\"After dropping rows with missing age OR salary: {df_drop_subset.shape}\")\n",
        "\n",
        "# Remove columns with too many missing values (>50%)\n",
        "threshold = len(df_missing) * 0.5  # 50% threshold\n",
        "df_drop_cols = df_missing.dropna(axis=1, thresh=threshold)\n",
        "print(f\"After dropping columns with >50% missing: {df_drop_cols.shape}\")\n",
        "print()\n",
        "\n",
        "# Strategy 2: Simple Imputation\n",
        "print(\"2️⃣ SIMPLE IMPUTATION METHODS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "df_imputed = df_missing.copy()\n",
        "\n",
        "# Numerical columns: Mean, Median, Mode\n",
        "numerical_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "for col in numerical_cols:\n",
        "    if df_imputed[col].isnull().sum() > 0:\n",
        "        # Calculate different measures\n",
        "        mean_val = df_imputed[col].mean()\n",
        "        median_val = df_imputed[col].median()\n",
        "        mode_val = df_imputed[col].mode().iloc[0] if not df_imputed[col].mode().empty else mean_val\n",
        "        \n",
        "        print(f\"{col}:\")\n",
        "        print(f\"  Mean: {mean_val:.2f}, Median: {median_val:.2f}, Mode: {mode_val:.2f}\")\n",
        "        \n",
        "        # Use median for robust imputation (less sensitive to outliers)\n",
        "        df_imputed[col].fillna(median_val, inplace=True)\n",
        "        print(f\"  → Filled with median: {median_val:.2f}\")\n",
        "\n",
        "# Categorical columns: Mode or 'Unknown'\n",
        "categorical_cols = df_imputed.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if df_imputed[col].isnull().sum() > 0:\n",
        "        mode_val = df_imputed[col].mode().iloc[0] if not df_imputed[col].mode().empty else 'Unknown'\n",
        "        \n",
        "        print(f\"{col}:\")\n",
        "        print(f\"  Mode: {mode_val}\")\n",
        "        \n",
        "        # Fill with mode\n",
        "        df_imputed[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"  → Filled with mode: {mode_val}\")\n",
        "\n",
        "print(f\"\\nMissing values after simple imputation:\")\n",
        "print(df_imputed.isnull().sum().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 3: Advanced Imputation Techniques\n",
        "print(\"3️⃣ ADVANCED IMPUTATION METHODS\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Forward Fill and Backward Fill (for time series or ordered data)\n",
        "df_ffill = df_missing.copy().sort_values('customer_id')\n",
        "print(\"Forward Fill (use previous value):\")\n",
        "df_ffill_demo = df_ffill[['customer_id', 'age', 'salary']].copy()\n",
        "df_ffill_demo['age_ffill'] = df_ffill_demo['age'].fillna(method='ffill')\n",
        "df_ffill_demo['salary_ffill'] = df_ffill_demo['salary'].fillna(method='ffill')\n",
        "print(df_ffill_demo)\n",
        "\n",
        "print(\"\\nBackward Fill (use next value):\")\n",
        "df_bfill_demo = df_ffill[['customer_id', 'age', 'salary']].copy()\n",
        "df_bfill_demo['age_bfill'] = df_bfill_demo['age'].fillna(method='bfill')\n",
        "df_bfill_demo['salary_bfill'] = df_bfill_demo['salary'].fillna(method='bfill')\n",
        "print(df_bfill_demo)\n",
        "\n",
        "# Interpolation (for numerical data)\n",
        "print(\"\\nInterpolation (estimate values between known points):\")\n",
        "df_interp = df_missing[['customer_id', 'age', 'salary']].copy().sort_values('customer_id')\n",
        "df_interp['age_interpolated'] = df_interp['age'].interpolate()\n",
        "df_interp['salary_interpolated'] = df_interp['salary'].interpolate()\n",
        "print(df_interp)\n",
        "\n",
        "# KNN Imputation (using similar records)\n",
        "print(\"\\n🧠 KNN Imputation (using similar records):\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "# Prepare data for KNN imputation (only numerical columns)\n",
        "numerical_data = df_missing[['age', 'salary', 'performance_score']].copy()\n",
        "\n",
        "# KNN Imputer\n",
        "knn_imputer = KNNImputer(n_neighbors=3)\n",
        "numerical_imputed = knn_imputer.fit_transform(numerical_data)\n",
        "\n",
        "# Create DataFrame with imputed values\n",
        "df_knn = df_missing.copy()\n",
        "df_knn[['age', 'salary', 'performance_score']] = numerical_imputed\n",
        "\n",
        "print(\"Before KNN imputation:\")\n",
        "print(df_missing[['customer_id', 'age', 'salary', 'performance_score']].head(10))\n",
        "\n",
        "print(\"\\nAfter KNN imputation:\")\n",
        "print(df_knn[['customer_id', 'age', 'salary', 'performance_score']].head(10))\n",
        "\n",
        "# Compare different imputation strategies\n",
        "print(\"\\n📊 IMPUTATION STRATEGY COMPARISON\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "strategies = {\n",
        "    'Original': df_missing['age'].copy(),\n",
        "    'Mean': df_missing['age'].fillna(df_missing['age'].mean()),\n",
        "    'Median': df_missing['age'].fillna(df_missing['age'].median()),\n",
        "    'KNN': df_knn['age'].copy()\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(strategies)\n",
        "print(\"Age column comparison:\")\n",
        "print(comparison_df.head(10))\n",
        "\n",
        "print(\"\\nStatistical differences:\")\n",
        "for strategy, values in strategies.items():\n",
        "    if strategy != 'Original':\n",
        "        print(f\"{strategy}: Mean={values.mean():.2f}, Std={values.std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📋 Practice Exercise: Missing Data\n",
        "\n",
        "**Challenge**: Create a dataset with intentional missing values and apply different imputation strategies. Compare the results and determine which strategy works best for different scenarios.\n",
        "\n",
        "**Your Task**:\n",
        "1. Create a synthetic dataset with missing values\n",
        "2. Apply at least 3 different imputation strategies\n",
        "3. Evaluate the impact on data distribution\n",
        "4. Choose the best strategy and justify your choice\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 4. Data Type Transformations 🔄\n",
        "\n",
        "## Converting and Standardizing Formats\n",
        "\n",
        "Data often comes in the wrong format for analysis. Let's explore how to convert between data types and standardize formats across your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Working with our imputed dataset for data type transformations\n",
        "df_types = df_imputed.copy()\n",
        "\n",
        "print(\"🔍 CURRENT DATA TYPES\")\n",
        "print(\"=\" * 30)\n",
        "print(df_types.dtypes)\n",
        "print()\n",
        "\n",
        "# 1. NUMERICAL DATA TYPE CONVERSIONS\n",
        "print(\"1️⃣ NUMERICAL DATA TYPE CONVERSIONS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Convert float to int (when appropriate)\n",
        "print(\"Converting customer_id to integer:\")\n",
        "print(f\"Before: {df_types['customer_id'].dtype}\")\n",
        "df_types['customer_id'] = df_types['customer_id'].astype('int32')\n",
        "print(f\"After: {df_types['customer_id'].dtype}\")\n",
        "\n",
        "# Handle mixed types in numerical columns\n",
        "print(\"\\nHandling mixed types in age column:\")\n",
        "print(\"Original age values and types:\")\n",
        "for i, val in enumerate(df_types['age'].head()):\n",
        "    print(f\"  Row {i}: {val} (type: {type(val)})\")\n",
        "\n",
        "# Convert to numeric, handling errors\n",
        "df_types['age'] = pd.to_numeric(df_types['age'], errors='coerce')\n",
        "print(f\"After conversion: {df_types['age'].dtype}\")\n",
        "\n",
        "# Memory optimization for integers\n",
        "print(\"\\nMemory optimization:\")\n",
        "print(f\"Original age dtype: {df_types['age'].dtype}\")\n",
        "# Check if values fit in smaller integer types\n",
        "if df_types['age'].min() >= 0 and df_types['age'].max() <= 255:\n",
        "    df_types['age'] = df_types['age'].astype('uint8')\n",
        "    print(f\"Optimized to: {df_types['age'].dtype}\")\n",
        "\n",
        "print(f\"Original salary dtype: {df_types['salary'].dtype}\")\n",
        "if df_types['salary'].min() >= -2147483648 and df_types['salary'].max() <= 2147483647:\n",
        "    df_types['salary'] = df_types['salary'].astype('int32')\n",
        "    print(f\"Optimized to: {df_types['salary'].dtype}\")\n",
        "\n",
        "# Memory usage comparison\n",
        "print(f\"\\nMemory usage improvement:\")\n",
        "print(f\"Before optimization: {df_messy.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "print(f\"After optimization: {df_types.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. CATEGORICAL DATA HANDLING\n",
        "print(\"2️⃣ CATEGORICAL DATA HANDLING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Convert to categorical data type for memory efficiency\n",
        "print(\"Converting department to categorical:\")\n",
        "print(f\"Unique departments: {df_types['department'].unique()}\")\n",
        "print(f\"Before: {df_types['department'].dtype}, Memory: {df_types['department'].memory_usage(deep=True)} bytes\")\n",
        "\n",
        "df_types['department'] = df_types['department'].astype('category')\n",
        "print(f\"After: {df_types['department'].dtype}, Memory: {df_types['department'].memory_usage(deep=True)} bytes\")\n",
        "\n",
        "# Standardize categorical values (case sensitivity)\n",
        "print(\"\\nStandardizing department names:\")\n",
        "print(\"Before standardization:\")\n",
        "print(df_types['department'].value_counts())\n",
        "\n",
        "# Convert to lowercase and then to proper case\n",
        "df_types['department'] = df_types['department'].str.lower().str.title()\n",
        "print(\"\\nAfter standardization:\")\n",
        "print(df_types['department'].value_counts())\n",
        "\n",
        "# Create ordered categorical (for ordinal data)\n",
        "print(\"\\n📊 Creating ordered categorical for performance ratings:\")\n",
        "performance_ranges = pd.cut(df_types['performance_score'], \n",
        "                           bins=[0, 70, 80, 90, 100], \n",
        "                           labels=['Poor', 'Fair', 'Good', 'Excellent'],\n",
        "                           ordered=True)\n",
        "df_types['performance_category'] = performance_ranges\n",
        "print(\"Performance categories:\")\n",
        "print(df_types['performance_category'].value_counts().sort_index())\n",
        "\n",
        "# Label Encoding for machine learning\n",
        "print(\"\\n🤖 Label Encoding (for ML algorithms):\")\n",
        "label_encoder = LabelEncoder()\n",
        "df_types['department_encoded'] = label_encoder.fit_transform(df_types['department'])\n",
        "\n",
        "encoding_map = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Label encoding mapping:\")\n",
        "for original, encoded in encoding_map.items():\n",
        "    print(f\"  {original} → {encoded}\")\n",
        "\n",
        "# One-Hot Encoding\n",
        "print(\"\\n🔥 One-Hot Encoding:\")\n",
        "dept_dummies = pd.get_dummies(df_types['department'], prefix='dept')\n",
        "print(\"One-hot encoded columns:\")\n",
        "print(dept_dummies.head())\n",
        "\n",
        "# Add one-hot encoded columns to main DataFrame\n",
        "df_types = pd.concat([df_types, dept_dummies], axis=1)\n",
        "print(f\"\\nDataFrame shape after one-hot encoding: {df_types.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. DATE AND TIME HANDLING\n",
        "print(\"3️⃣ DATE AND TIME HANDLING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"Original join_date column:\")\n",
        "print(df_types['join_date'].head(10))\n",
        "print(f\"Data type: {df_types['join_date'].dtype}\")\n",
        "\n",
        "# Convert string dates to datetime\n",
        "print(\"\\nConverting to datetime:\")\n",
        "# Handle multiple date formats\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse dates in multiple formats\"\"\"\n",
        "    if pd.isna(date_str) or date_str == 'invalid_date':\n",
        "        return pd.NaT\n",
        "    \n",
        "    # Try different formats\n",
        "    formats = ['%Y-%m-%d', '%Y/%m/%d', '%d/%m/%Y', '%m/%d/%Y']\n",
        "    \n",
        "    for fmt in formats:\n",
        "        try:\n",
        "            return pd.to_datetime(date_str, format=fmt)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    \n",
        "    # If all formats fail, try pandas' automatic parsing\n",
        "    try:\n",
        "        return pd.to_datetime(date_str)\n",
        "    except:\n",
        "        return pd.NaT\n",
        "\n",
        "# Apply date parsing\n",
        "df_types['join_date_parsed'] = df_types['join_date'].apply(parse_date)\n",
        "print(\"After parsing:\")\n",
        "print(df_types[['join_date', 'join_date_parsed']].head(10))\n",
        "\n",
        "# Extract useful date components\n",
        "print(\"\\n📅 Extracting date components:\")\n",
        "df_types['join_year'] = df_types['join_date_parsed'].dt.year\n",
        "df_types['join_month'] = df_types['join_date_parsed'].dt.month\n",
        "df_types['join_quarter'] = df_types['join_date_parsed'].dt.quarter\n",
        "df_types['join_day_of_week'] = df_types['join_date_parsed'].dt.day_name()\n",
        "df_types['days_since_joining'] = (pd.Timestamp.now() - df_types['join_date_parsed']).dt.days\n",
        "\n",
        "print(\"Date components extracted:\")\n",
        "date_components = df_types[['join_date_parsed', 'join_year', 'join_month', \n",
        "                           'join_quarter', 'join_day_of_week', 'days_since_joining']].head()\n",
        "print(date_components)\n",
        "\n",
        "# Create time-based features\n",
        "print(\"\\n⏰ Creating time-based features:\")\n",
        "df_types['is_recent_hire'] = df_types['days_since_joining'] < 365  # Joined within last year\n",
        "df_types['tenure_category'] = pd.cut(df_types['days_since_joining'], \n",
        "                                    bins=[0, 365, 730, float('inf')], \n",
        "                                    labels=['New', 'Experienced', 'Veteran'])\n",
        "\n",
        "print(\"Time-based features:\")\n",
        "print(df_types[['join_date_parsed', 'is_recent_hire', 'tenure_category']].head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 5. Duplicate Detection & Removal 🔍\n",
        "\n",
        "## Ensuring Data Uniqueness\n",
        "\n",
        "Duplicate records can skew analysis results and waste computational resources. Let's learn how to identify and handle different types of duplicates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start with our cleaned dataset for duplicate analysis\n",
        "df_dupes = df_types.copy()\n",
        "\n",
        "print(\"🔍 DUPLICATE DETECTION ANALYSIS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# 1. Exact Duplicates\n",
        "print(\"1️⃣ EXACT DUPLICATES\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Check for completely identical rows\n",
        "exact_duplicates = df_dupes.duplicated()\n",
        "print(f\"Number of exact duplicate rows: {exact_duplicates.sum()}\")\n",
        "\n",
        "if exact_duplicates.sum() > 0:\n",
        "    print(\"Duplicate rows:\")\n",
        "    print(df_dupes[exact_duplicates])\n",
        "    \n",
        "    # Show the original rows that have duplicates\n",
        "    print(\"\\nOriginal rows with duplicates:\")\n",
        "    duplicate_indices = df_dupes[exact_duplicates].index\n",
        "    original_indices = []\n",
        "    for idx in duplicate_indices:\n",
        "        # Find the first occurrence of this duplicate\n",
        "        mask = df_dupes.eq(df_dupes.iloc[idx]).all(axis=1)\n",
        "        first_occurrence = df_dupes[mask].index[0]\n",
        "        if first_occurrence not in original_indices:\n",
        "            original_indices.append(first_occurrence)\n",
        "    \n",
        "    print(df_dupes.loc[original_indices])\n",
        "\n",
        "# 2. Duplicates based on specific columns\n",
        "print(\"\\n2️⃣ DUPLICATES BASED ON KEY COLUMNS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Check for duplicates based on business logic (e.g., same person)\n",
        "key_columns = ['name', 'email']\n",
        "key_duplicates = df_dupes.duplicated(subset=key_columns, keep=False)\n",
        "print(f\"Rows with duplicate name/email combinations: {key_duplicates.sum()}\")\n",
        "\n",
        "if key_duplicates.sum() > 0:\n",
        "    print(\"Rows with duplicate key combinations:\")\n",
        "    duplicate_rows = df_dupes[key_duplicates].sort_values(key_columns)\n",
        "    print(duplicate_rows[['customer_id', 'name', 'email', 'age', 'salary']])\n",
        "\n",
        "# 3. Fuzzy Duplicates (similar but not identical)\n",
        "print(\"\\n3️⃣ FUZZY DUPLICATES (SIMILAR RECORDS)\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "def find_fuzzy_duplicates(df, column, threshold=0.8):\n",
        "    \"\"\"Find fuzzy duplicates using string similarity\"\"\"\n",
        "    from difflib import SequenceMatcher\n",
        "    \n",
        "    duplicates = []\n",
        "    values = df[column].dropna().str.lower().unique()\n",
        "    \n",
        "    for i, val1 in enumerate(values):\n",
        "        for val2 in values[i+1:]:\n",
        "            similarity = SequenceMatcher(None, val1, val2).ratio()\n",
        "            if similarity >= threshold:\n",
        "                duplicates.append((val1, val2, similarity))\n",
        "    \n",
        "    return duplicates\n",
        "\n",
        "# Find fuzzy duplicates in names\n",
        "name_fuzzy_dupes = find_fuzzy_duplicates(df_dupes, 'name', threshold=0.7)\n",
        "print(\"Fuzzy duplicate names found:\")\n",
        "for name1, name2, similarity in name_fuzzy_dupes:\n",
        "    print(f\"  '{name1}' ↔ '{name2}' (similarity: {similarity:.2f})\")\n",
        "    \n",
        "    # Show the actual records\n",
        "    mask1 = df_dupes['name'].str.lower() == name1\n",
        "    mask2 = df_dupes['name'].str.lower() == name2\n",
        "    similar_records = df_dupes[mask1 | mask2][['customer_id', 'name', 'email']]\n",
        "    print(similar_records)\n",
        "    print()\n",
        "\n",
        "# 4. Remove duplicates with different strategies\n",
        "print(\"4️⃣ DUPLICATE REMOVAL STRATEGIES\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"Original dataset shape:\", df_dupes.shape)\n",
        "\n",
        "# Strategy 1: Keep first occurrence\n",
        "df_keep_first = df_dupes.drop_duplicates(subset=key_columns, keep='first')\n",
        "print(f\"After removing duplicates (keep first): {df_keep_first.shape}\")\n",
        "\n",
        "# Strategy 2: Keep last occurrence\n",
        "df_keep_last = df_dupes.drop_duplicates(subset=key_columns, keep='last')\n",
        "print(f\"After removing duplicates (keep last): {df_keep_last.shape}\")\n",
        "\n",
        "# Strategy 3: Keep the record with more complete information\n",
        "def keep_most_complete(group):\n",
        "    \"\"\"Keep the record with the least missing values\"\"\"\n",
        "    missing_counts = group.isnull().sum(axis=1)\n",
        "    return group.loc[missing_counts.idxmin()]\n",
        "\n",
        "# Group by key columns and keep most complete record\n",
        "df_most_complete = df_dupes.groupby(key_columns, as_index=False).apply(keep_most_complete).reset_index(drop=True)\n",
        "print(f\"After keeping most complete records: {df_most_complete.shape}\")\n",
        "\n",
        "# Strategy 4: Aggregate information from duplicates\n",
        "print(\"\\n📊 AGGREGATING DUPLICATE INFORMATION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# For numerical columns, we can aggregate (mean, max, etc.)\n",
        "# For categorical columns, we can take the mode or concatenate unique values\n",
        "def aggregate_duplicates(group):\n",
        "    \"\"\"Aggregate information from duplicate records\"\"\"\n",
        "    result = {}\n",
        "    \n",
        "    # Keep the first customer_id\n",
        "    result['customer_id'] = group['customer_id'].iloc[0]\n",
        "    \n",
        "    # For categorical data, take the first non-null value\n",
        "    result['name'] = group['name'].iloc[0]\n",
        "    result['email'] = group['email'].iloc[0]\n",
        "    result['department'] = group['department'].iloc[0]\n",
        "    \n",
        "    # For numerical data, take the mean\n",
        "    result['age'] = group['age'].mean()\n",
        "    result['salary'] = group['salary'].mean()\n",
        "    result['performance_score'] = group['performance_score'].mean()\n",
        "    \n",
        "    # For dates, take the earliest\n",
        "    result['join_date_parsed'] = group['join_date_parsed'].min()\n",
        "    \n",
        "    return pd.Series(result)\n",
        "\n",
        "# Apply aggregation to duplicates\n",
        "if key_duplicates.sum() > 0:\n",
        "    df_aggregated = df_dupes.groupby(key_columns).apply(aggregate_duplicates).reset_index(drop=True)\n",
        "    print(f\"After aggregating duplicates: {df_aggregated.shape}\")\n",
        "    print(\"Sample aggregated records:\")\n",
        "    print(df_aggregated.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 6. Text Data Cleaning 📝\n",
        "\n",
        "## Working with Strings and Text\n",
        "\n",
        "Text data often requires extensive cleaning and standardization. Let's explore techniques for handling messy text data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text cleaning examples using our dataset\n",
        "df_text = df_keep_first.copy()  # Use deduplicated dataset\n",
        "\n",
        "print(\"🧹 TEXT DATA CLEANING TECHNIQUES\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# 1. BASIC STRING OPERATIONS\n",
        "print(\"1️⃣ BASIC STRING OPERATIONS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"Original name column:\")\n",
        "print(df_text['name'].tolist())\n",
        "\n",
        "# Case standardization\n",
        "df_text['name_clean'] = df_text['name'].str.title()\n",
        "print(\"\\nAfter title case standardization:\")\n",
        "print(df_text['name_clean'].tolist())\n",
        "\n",
        "# Remove extra whitespace\n",
        "df_text['name_clean'] = df_text['name_clean'].str.strip()\n",
        "\n",
        "# Handle multiple spaces\n",
        "df_text['name_clean'] = df_text['name_clean'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "print(\"\\nAfter whitespace cleaning:\")\n",
        "print(df_text['name_clean'].tolist())\n",
        "\n",
        "# 2. EMAIL VALIDATION AND CLEANING\n",
        "print(\"\\n2️⃣ EMAIL VALIDATION AND CLEANING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"Original email column:\")\n",
        "print(df_text['email'].tolist())\n",
        "\n",
        "# Basic email validation using regex\n",
        "email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
        "\n",
        "def validate_email(email):\n",
        "    \"\"\"Validate email format\"\"\"\n",
        "    if pd.isna(email):\n",
        "        return False\n",
        "    return bool(re.match(email_pattern, email))\n",
        "\n",
        "# Apply email validation\n",
        "df_text['email_valid'] = df_text['email'].apply(validate_email)\n",
        "print(\"\\nEmail validation results:\")\n",
        "validation_results = df_text[['email', 'email_valid']]\n",
        "print(validation_results)\n",
        "\n",
        "# Clean email addresses\n",
        "def clean_email(email):\n",
        "    \"\"\"Clean and standardize email addresses\"\"\"\n",
        "    if pd.isna(email):\n",
        "        return email\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    email = email.lower().strip()\n",
        "    \n",
        "    # Remove extra spaces\n",
        "    email = re.sub(r'\\s+', '', email)\n",
        "    \n",
        "    # Basic validation\n",
        "    if validate_email(email):\n",
        "        return email\n",
        "    else:\n",
        "        return None  # Mark invalid emails as None\n",
        "\n",
        "df_text['email_clean'] = df_text['email'].apply(clean_email)\n",
        "print(\"\\nCleaned emails:\")\n",
        "print(df_text[['email', 'email_clean']].head(10))\n",
        "\n",
        "# 3. ADVANCED TEXT PROCESSING\n",
        "print(\"\\n3️⃣ ADVANCED TEXT PROCESSING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Example with a more complex text column\n",
        "text_examples = [\n",
        "    \"John   DOE\",\n",
        "    \"mary-jane SMITH\",\n",
        "    \"BOB   o'connor\",\n",
        "    \"Ana   María   García\",\n",
        "    \"李小明\",\n",
        "    \"محمد الأحمد\",\n",
        "    \"   Dr. Sarah  Wilson   PhD  \",\n",
        "    \"MR. james   brown, jr.\",\n",
        "    \"Ms   Linda   Davis-Johnson\"\n",
        "]\n",
        "\n",
        "df_text_sample = pd.DataFrame({'raw_names': text_examples})\n",
        "print(\"Complex text examples:\")\n",
        "print(df_text_sample['raw_names'].tolist())\n",
        "\n",
        "def advanced_name_cleaning(name):\n",
        "    \"\"\"Advanced name cleaning and standardization\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return name\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    name = re.sub(r'\\s+', ' ', name.strip())\n",
        "    \n",
        "    # Handle common prefixes and suffixes\n",
        "    prefixes = ['mr', 'mrs', 'ms', 'dr', 'prof']\n",
        "    suffixes = ['jr', 'sr', 'phd', 'md', 'esq']\n",
        "    \n",
        "    # Split into parts\n",
        "    parts = name.lower().split()\n",
        "    \n",
        "    # Remove prefixes\n",
        "    while parts and parts[0].replace('.', '') in prefixes:\n",
        "        parts.pop(0)\n",
        "    \n",
        "    # Remove suffixes\n",
        "    while parts and parts[-1].replace('.', '').replace(',', '') in suffixes:\n",
        "        parts.pop()\n",
        "    \n",
        "    # Rejoin and apply title case\n",
        "    cleaned_name = ' '.join(parts).title()\n",
        "    \n",
        "    # Handle special cases (hyphens, apostrophes)\n",
        "    cleaned_name = re.sub(r\"(\\w)'(\\w)\", r\"\\1'\\2\", cleaned_name)  # O'Connor\n",
        "    cleaned_name = re.sub(r\"(\\w)-(\\w)\", lambda m: f\"{m.group(1)}-{m.group(2).title()}\", cleaned_name)  # Davis-Johnson\n",
        "    \n",
        "    return cleaned_name\n",
        "\n",
        "df_text_sample['cleaned_names'] = df_text_sample['raw_names'].apply(advanced_name_cleaning)\n",
        "print(\"\\nAfter advanced cleaning:\")\n",
        "comparison = df_text_sample[['raw_names', 'cleaned_names']]\n",
        "print(comparison)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 7. Outlier Detection & Treatment ⚠️\n",
        "\n",
        "## Identifying and Handling Anomalies\n",
        "\n",
        "Outliers can significantly impact analysis results. Let's explore different methods for detecting and treating outliers in numerical data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outlier detection and treatment\n",
        "df_outliers = df_text.copy()\n",
        "\n",
        "print(\"🎯 OUTLIER DETECTION METHODS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Focus on numerical columns\n",
        "numerical_columns = ['age', 'salary', 'performance_score']\n",
        "\n",
        "# 1. STATISTICAL METHODS\n",
        "print(\"1️⃣ STATISTICAL OUTLIER DETECTION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detect outliers using Interquartile Range (IQR) method\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    \n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    \n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "def detect_outliers_zscore(data, column, threshold=3):\n",
        "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
        "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
        "    outliers = data[z_scores > threshold]\n",
        "    \n",
        "    return outliers, z_scores\n",
        "\n",
        "# Apply IQR method\n",
        "print(\"IQR Method Results:\")\n",
        "for col in numerical_columns:\n",
        "    outliers_iqr, lower, upper = detect_outliers_iqr(df_outliers, col)\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Valid range: {lower:.2f} to {upper:.2f}\")\n",
        "    print(f\"  Outliers found: {len(outliers_iqr)}\")\n",
        "    \n",
        "    if len(outliers_iqr) > 0:\n",
        "        print(f\"  Outlier values: {outliers_iqr[col].tolist()}\")\n",
        "\n",
        "# Apply Z-score method\n",
        "print(\"\\n\\nZ-Score Method Results (threshold=3):\")\n",
        "for col in numerical_columns:\n",
        "    outliers_z, z_scores = detect_outliers_zscore(df_outliers, col, threshold=3)\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Outliers found: {len(outliers_z)}\")\n",
        "    \n",
        "    if len(outliers_z) > 0:\n",
        "        print(f\"  Outlier values: {outliers_z[col].tolist()}\")\n",
        "\n",
        "# 2. VISUALIZATION OF OUTLIERS\n",
        "print(\"\\n2️⃣ OUTLIER VISUALIZATION\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Outlier Detection Visualization', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, col in enumerate(numerical_columns):\n",
        "    # Box plot\n",
        "    ax1 = axes[0, i]\n",
        "    sns.boxplot(data=df_outliers, y=col, ax=ax1)\n",
        "    ax1.set_title(f'{col} - Box Plot')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Histogram with normal distribution overlay\n",
        "    ax2 = axes[1, i]\n",
        "    ax2.hist(df_outliers[col].dropna(), bins=20, density=True, alpha=0.7, color='skyblue')\n",
        "    \n",
        "    # Overlay normal distribution\n",
        "    mu, sigma = df_outliers[col].mean(), df_outliers[col].std()\n",
        "    x = np.linspace(df_outliers[col].min(), df_outliers[col].max(), 100)\n",
        "    normal_dist = stats.norm.pdf(x, mu, sigma)\n",
        "    ax2.plot(x, normal_dist, 'r-', linewidth=2, label='Normal Distribution')\n",
        "    \n",
        "    ax2.set_title(f'{col} - Distribution')\n",
        "    ax2.set_xlabel(col)\n",
        "    ax2.set_ylabel('Density')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. OUTLIER TREATMENT STRATEGIES\n",
        "print(\"3️⃣ OUTLIER TREATMENT STRATEGIES\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Strategy 1: Remove outliers\n",
        "df_no_outliers = df_outliers.copy()\n",
        "print(f\"Original dataset shape: {df_no_outliers.shape}\")\n",
        "\n",
        "for col in numerical_columns:\n",
        "    outliers_iqr, lower, upper = detect_outliers_iqr(df_no_outliers, col)\n",
        "    # Remove outliers\n",
        "    df_no_outliers = df_no_outliers[(df_no_outliers[col] >= lower) & (df_no_outliers[col] <= upper)]\n",
        "\n",
        "print(f\"After removing outliers: {df_no_outliers.shape}\")\n",
        "\n",
        "# Strategy 2: Cap outliers (Winsorization)\n",
        "df_capped = df_outliers.copy()\n",
        "\n",
        "def cap_outliers(data, column, percentile_range=(5, 95)):\n",
        "    \"\"\"Cap outliers at specified percentiles\"\"\"\n",
        "    lower_percentile = np.percentile(data[column].dropna(), percentile_range[0])\n",
        "    upper_percentile = np.percentile(data[column].dropna(), percentile_range[1])\n",
        "    \n",
        "    data[column] = np.clip(data[column], lower_percentile, upper_percentile)\n",
        "    return data\n",
        "\n",
        "print(\"\\nWinsorization (capping at 5th and 95th percentiles):\")\n",
        "for col in numerical_columns:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Before: min={df_capped[col].min():.2f}, max={df_capped[col].max():.2f}\")\n",
        "    df_capped = cap_outliers(df_capped, col, percentile_range=(5, 95))\n",
        "    print(f\"  After:  min={df_capped[col].min():.2f}, max={df_capped[col].max():.2f}\")\n",
        "\n",
        "# Strategy 3: Transform outliers\n",
        "df_transformed = df_outliers.copy()\n",
        "\n",
        "print(\"\\nLog transformation (for right-skewed data):\")\n",
        "for col in ['salary']:  # Apply to salary which might be right-skewed\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Original range: {df_transformed[col].min():.2f} to {df_transformed[col].max():.2f}\")\n",
        "    print(f\"  Original skewness: {df_transformed[col].skew():.3f}\")\n",
        "    \n",
        "    # Apply log transformation\n",
        "    df_transformed[f'{col}_log'] = np.log1p(df_transformed[col])  # log1p handles values close to 0\n",
        "    print(f\"  Log-transformed skewness: {df_transformed[f'{col}_log'].skew():.3f}\")\n",
        "\n",
        "# Compare the effect of different treatments\n",
        "print(\"\\n📊 TREATMENT COMPARISON\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "treatments = {\n",
        "    'Original': df_outliers['age'].describe(),\n",
        "    'Outliers Removed': df_no_outliers['age'].describe(),\n",
        "    'Outliers Capped': df_capped['age'].describe()\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(treatments).round(2)\n",
        "print(\"Age column comparison across treatments:\")\n",
        "print(comparison_df)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 8. Data Transformation Techniques 🔄\n",
        "\n",
        "## Scaling, Encoding, and Feature Engineering\n",
        "\n",
        "Transform your data to make it suitable for machine learning algorithms and statistical analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data transformation techniques\n",
        "df_transform = df_capped.copy()  # Use the dataset with capped outliers\n",
        "\n",
        "print(\"🔄 DATA TRANSFORMATION TECHNIQUES\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# 1. SCALING AND NORMALIZATION\n",
        "print(\"1️⃣ SCALING AND NORMALIZATION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Prepare numerical data for scaling\n",
        "scaling_columns = ['age', 'salary', 'performance_score']\n",
        "scaling_data = df_transform[scaling_columns].copy()\n",
        "\n",
        "print(\"Original data statistics:\")\n",
        "print(scaling_data.describe().round(2))\n",
        "\n",
        "# Standard Scaling (Z-score normalization)\n",
        "scaler_standard = StandardScaler()\n",
        "scaled_standard = scaler_standard.fit_transform(scaling_data)\n",
        "df_standard_scaled = pd.DataFrame(scaled_standard, columns=[f'{col}_standard' for col in scaling_columns])\n",
        "\n",
        "print(\"\\nAfter Standard Scaling (mean=0, std=1):\")\n",
        "print(df_standard_scaled.describe().round(2))\n",
        "\n",
        "# Min-Max Scaling (normalization to 0-1 range)\n",
        "scaler_minmax = MinMaxScaler()\n",
        "scaled_minmax = scaler_minmax.fit_transform(scaling_data)\n",
        "df_minmax_scaled = pd.DataFrame(scaled_minmax, columns=[f'{col}_minmax' for col in scaling_columns])\n",
        "\n",
        "print(\"\\nAfter Min-Max Scaling (range 0-1):\")\n",
        "print(df_minmax_scaled.describe().round(2))\n",
        "\n",
        "# Robust Scaling (using median and IQR)\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler_robust = RobustScaler()\n",
        "scaled_robust = scaler_robust.fit_transform(scaling_data)\n",
        "df_robust_scaled = pd.DataFrame(scaled_robust, columns=[f'{col}_robust' for col in scaling_columns])\n",
        "\n",
        "print(\"\\nAfter Robust Scaling (median=0, IQR=1):\")\n",
        "print(df_robust_scaled.describe().round(2))\n",
        "\n",
        "# 2. FEATURE ENGINEERING\n",
        "print(\"\\n2️⃣ FEATURE ENGINEERING\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Create new features from existing data\n",
        "df_features = df_transform.copy()\n",
        "\n",
        "# Age groups (binning)\n",
        "df_features['age_group'] = pd.cut(df_features['age'], \n",
        "                                 bins=[0, 25, 35, 45, 100], \n",
        "                                 labels=['Young', 'Mid-Career', 'Senior', 'Veteran'])\n",
        "\n",
        "# Salary categories\n",
        "df_features['salary_category'] = pd.cut(df_features['salary'], \n",
        "                                       bins=[0, 50000, 70000, 90000, float('inf')], \n",
        "                                       labels=['Entry', 'Mid', 'Senior', 'Executive'])\n",
        "\n",
        "# Performance vs Age ratio\n",
        "df_features['performance_per_age'] = df_features['performance_score'] / df_features['age']\n",
        "\n",
        "# Salary per performance point\n",
        "df_features['salary_per_performance'] = df_features['salary'] / df_features['performance_score']\n",
        "\n",
        "# Department size (frequency encoding)\n",
        "dept_counts = df_features['department'].value_counts()\n",
        "df_features['department_size'] = df_features['department'].map(dept_counts)\n",
        "\n",
        "print(\"New engineered features:\")\n",
        "new_features = ['age_group', 'salary_category', 'performance_per_age', \n",
        "                'salary_per_performance', 'department_size']\n",
        "print(df_features[new_features].head())\n",
        "\n",
        "# 3. POLYNOMIAL FEATURES\n",
        "print(\"\\n3️⃣ POLYNOMIAL FEATURES\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Create polynomial features (degree 2)\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "age_salary_data = df_features[['age', 'salary']].copy()\n",
        "\n",
        "# Generate polynomial features\n",
        "poly_transformed = poly_features.fit_transform(age_salary_data)\n",
        "poly_feature_names = poly_features.get_feature_names_out(['age', 'salary'])\n",
        "\n",
        "df_poly = pd.DataFrame(poly_transformed, columns=poly_feature_names)\n",
        "print(\"Polynomial features (degree 2):\")\n",
        "print(df_poly.head())\n",
        "\n",
        "# 4. INTERACTION FEATURES\n",
        "print(\"\\n4️⃣ INTERACTION FEATURES\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Create interaction features manually\n",
        "df_features['age_salary_interaction'] = df_features['age'] * df_features['salary']\n",
        "df_features['age_performance_interaction'] = df_features['age'] * df_features['performance_score']\n",
        "\n",
        "# Boolean interactions\n",
        "df_features['high_performer_senior'] = ((df_features['performance_score'] > 90) & \n",
        "                                       (df_features['age'] > 35)).astype(int)\n",
        "\n",
        "print(\"Interaction features:\")\n",
        "interaction_features = ['age_salary_interaction', 'age_performance_interaction', 'high_performer_senior']\n",
        "print(df_features[interaction_features].head())\n",
        "\n",
        "# 5. VISUALIZATION OF TRANSFORMATIONS\n",
        "print(\"\\n5️⃣ TRANSFORMATION VISUALIZATION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Data Transformation Effects', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Original vs Scaled data\n",
        "ax1 = axes[0, 0]\n",
        "ax1.scatter(df_transform['age'], df_transform['salary'], alpha=0.6)\n",
        "ax1.set_xlabel('Age (Original)')\n",
        "ax1.set_ylabel('Salary (Original)')\n",
        "ax1.set_title('Original Data')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2 = axes[0, 1]\n",
        "ax2.scatter(df_standard_scaled['age_standard'], df_standard_scaled['salary_standard'], alpha=0.6)\n",
        "ax2.set_xlabel('Age (Standardized)')\n",
        "ax2.set_ylabel('Salary (Standardized)')\n",
        "ax2.set_title('Standard Scaled Data')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Feature distributions\n",
        "ax3 = axes[1, 0]\n",
        "df_features['age_group'].value_counts().plot(kind='bar', ax=ax3)\n",
        "ax3.set_title('Age Group Distribution')\n",
        "ax3.set_xlabel('Age Group')\n",
        "ax3.set_ylabel('Count')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "ax4 = axes[1, 1]\n",
        "ax4.scatter(df_features['performance_score'], df_features['performance_per_age'], alpha=0.6)\n",
        "ax4.set_xlabel('Performance Score')\n",
        "ax4.set_ylabel('Performance per Age')\n",
        "ax4.set_title('Engineered Feature: Performance per Age')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Data transformation completed!\")\n",
        "print(f\"Original features: {len(df_transform.columns)}\")\n",
        "print(f\"After feature engineering: {len(df_features.columns)}\")\n",
        "print(f\"New features added: {len(df_features.columns) - len(df_transform.columns)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 9. Data Cleaning Pipeline 🔧\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's create a comprehensive, reusable data cleaning pipeline that incorporates all the techniques we've learned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataCleaningPipeline:\n",
        "    \"\"\"\n",
        "    A comprehensive data cleaning pipeline that applies multiple cleaning techniques\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config=None):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline with configuration parameters\n",
        "        \n",
        "        Parameters:\n",
        "        config (dict): Configuration dictionary with cleaning parameters\n",
        "        \"\"\"\n",
        "        self.config = config or self._default_config()\n",
        "        self.cleaning_report = {}\n",
        "        \n",
        "    def _default_config(self):\n",
        "        \"\"\"Default configuration for the cleaning pipeline\"\"\"\n",
        "        return {\n",
        "            'handle_missing': True,\n",
        "            'missing_strategy': 'median',  # 'mean', 'median', 'mode', 'drop', 'knn'\n",
        "            'handle_duplicates': True,\n",
        "            'duplicate_subset': None,  # Columns to check for duplicates\n",
        "            'handle_outliers': True,\n",
        "            'outlier_method': 'iqr',  # 'iqr', 'zscore', 'cap'\n",
        "            'standardize_text': True,\n",
        "            'validate_data_types': True,\n",
        "            'create_features': False,\n",
        "            'scaling_method': None,  # 'standard', 'minmax', 'robust'\n",
        "            'verbose': True\n",
        "        }\n",
        "    \n",
        "    def clean_data(self, df):\n",
        "        \"\"\"\n",
        "        Main method to clean the DataFrame\n",
        "        \n",
        "        Parameters:\n",
        "        df (pd.DataFrame): Input DataFrame to clean\n",
        "        \n",
        "        Returns:\n",
        "        pd.DataFrame: Cleaned DataFrame\n",
        "        dict: Cleaning report with statistics\n",
        "        \"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"🧹 STARTING DATA CLEANING PIPELINE\")\n",
        "            print(\"=\" * 50)\n",
        "            print(f\"Original dataset shape: {df.shape}\")\n",
        "        \n",
        "        # Create a copy to avoid modifying original data\n",
        "        df_clean = df.copy()\n",
        "        \n",
        "        # Initialize cleaning report\n",
        "        self.cleaning_report = {\n",
        "            'original_shape': df.shape,\n",
        "            'steps': []\n",
        "        }\n",
        "        \n",
        "        # Step 1: Handle missing values\n",
        "        if self.config['handle_missing']:\n",
        "            df_clean = self._handle_missing_values(df_clean)\n",
        "        \n",
        "        # Step 2: Handle duplicates\n",
        "        if self.config['handle_duplicates']:\n",
        "            df_clean = self._handle_duplicates(df_clean)\n",
        "        \n",
        "        # Step 3: Standardize text data\n",
        "        if self.config['standardize_text']:\n",
        "            df_clean = self._standardize_text(df_clean)\n",
        "        \n",
        "        # Step 4: Validate and convert data types\n",
        "        if self.config['validate_data_types']:\n",
        "            df_clean = self._validate_data_types(df_clean)\n",
        "        \n",
        "        # Step 5: Handle outliers\n",
        "        if self.config['handle_outliers']:\n",
        "            df_clean = self._handle_outliers(df_clean)\n",
        "        \n",
        "        # Step 6: Create features (optional)\n",
        "        if self.config['create_features']:\n",
        "            df_clean = self._create_features(df_clean)\n",
        "        \n",
        "        # Step 7: Apply scaling (optional)\n",
        "        if self.config['scaling_method']:\n",
        "            df_clean = self._apply_scaling(df_clean)\n",
        "        \n",
        "        # Finalize report\n",
        "        self.cleaning_report['final_shape'] = df_clean.shape\n",
        "        self.cleaning_report['rows_removed'] = df.shape[0] - df_clean.shape[0]\n",
        "        self.cleaning_report['columns_added'] = df_clean.shape[1] - df.shape[1]\n",
        "        \n",
        "        if self.config['verbose']:\n",
        "            self._print_final_report()\n",
        "        \n",
        "        return df_clean, self.cleaning_report\n",
        "    \n",
        "    def _handle_missing_values(self, df):\n",
        "        \"\"\"Handle missing values based on the configured strategy\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n1️⃣ Handling Missing Values\")\n",
        "            print(\"-\" * 30)\n",
        "        \n",
        "        missing_before = df.isnull().sum().sum()\n",
        "        \n",
        "        if self.config['missing_strategy'] == 'drop':\n",
        "            df = df.dropna()\n",
        "        else:\n",
        "            # Handle numerical columns\n",
        "            numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "            for col in numerical_cols:\n",
        "                if df[col].isnull().sum() > 0:\n",
        "                    if self.config['missing_strategy'] == 'mean':\n",
        "                        df[col].fillna(df[col].mean(), inplace=True)\n",
        "                    elif self.config['missing_strategy'] == 'median':\n",
        "                        df[col].fillna(df[col].median(), inplace=True)\n",
        "                    elif self.config['missing_strategy'] == 'knn':\n",
        "                        # Simple KNN imputation for numerical data\n",
        "                        from sklearn.impute import KNNImputer\n",
        "                        imputer = KNNImputer(n_neighbors=3)\n",
        "                        df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
        "            \n",
        "            # Handle categorical columns\n",
        "            categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "            for col in categorical_cols:\n",
        "                if df[col].isnull().sum() > 0:\n",
        "                    mode_val = df[col].mode().iloc[0] if not df[col].mode().empty else 'Unknown'\n",
        "                    df[col].fillna(mode_val, inplace=True)\n",
        "        \n",
        "        missing_after = df.isnull().sum().sum()\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Missing Values',\n",
        "            'missing_before': missing_before,\n",
        "            'missing_after': missing_after,\n",
        "            'strategy': self.config['missing_strategy']\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose']:\n",
        "            print(f\"Missing values before: {missing_before}\")\n",
        "            print(f\"Missing values after: {missing_after}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _handle_duplicates(self, df):\n",
        "        \"\"\"Remove duplicate rows\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n2️⃣ Handling Duplicates\")\n",
        "            print(\"-\" * 25)\n",
        "        \n",
        "        duplicates_before = df.duplicated().sum()\n",
        "        \n",
        "        if self.config['duplicate_subset']:\n",
        "            df = df.drop_duplicates(subset=self.config['duplicate_subset'], keep='first')\n",
        "        else:\n",
        "            df = df.drop_duplicates(keep='first')\n",
        "        \n",
        "        duplicates_after = df.duplicated().sum()\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Duplicates',\n",
        "            'duplicates_before': duplicates_before,\n",
        "            'duplicates_after': duplicates_after,\n",
        "            'subset': self.config['duplicate_subset']\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose']:\n",
        "            print(f\"Duplicate rows before: {duplicates_before}\")\n",
        "            print(f\"Duplicate rows after: {duplicates_after}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _standardize_text(self, df):\n",
        "        \"\"\"Standardize text columns\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n3️⃣ Standardizing Text\")\n",
        "            print(\"-\" * 25)\n",
        "        \n",
        "        text_cols = df.select_dtypes(include=['object']).columns\n",
        "        \n",
        "        for col in text_cols:\n",
        "            if col in df.columns:\n",
        "                # Convert to string and handle NaN\n",
        "                df[col] = df[col].astype(str)\n",
        "                \n",
        "                # Basic text cleaning\n",
        "                df[col] = df[col].str.strip()  # Remove leading/trailing whitespace\n",
        "                df[col] = df[col].str.replace(r'\\\\s+', ' ', regex=True)  # Multiple spaces\n",
        "                \n",
        "                # Check if it looks like a name or categorical field\n",
        "                if 'name' in col.lower():\n",
        "                    df[col] = df[col].str.title()\n",
        "                elif df[col].nunique() / len(df) < 0.5:  # Likely categorical\n",
        "                    df[col] = df[col].str.title()\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Text Standardization',\n",
        "            'columns_processed': list(text_cols)\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose']:\n",
        "            print(f\"Text columns standardized: {list(text_cols)}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _validate_data_types(self, df):\n",
        "        \"\"\"Validate and convert data types\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n4️⃣ Validating Data Types\")\n",
        "            print(\"-\" * 28)\n",
        "        \n",
        "        conversions = {}\n",
        "        \n",
        "        for col in df.columns:\n",
        "            original_dtype = df[col].dtype\n",
        "            \n",
        "            # Try to convert object columns to numeric if possible\n",
        "            if df[col].dtype == 'object':\n",
        "                # Try numeric conversion\n",
        "                numeric_converted = pd.to_numeric(df[col], errors='coerce')\n",
        "                if not numeric_converted.isna().all():\n",
        "                    # If most values can be converted to numeric\n",
        "                    na_ratio = numeric_converted.isna().sum() / len(df)\n",
        "                    if na_ratio < 0.1:  # Less than 10% would become NaN\n",
        "                        df[col] = numeric_converted\n",
        "                        conversions[col] = f\"{original_dtype} → {df[col].dtype}\"\n",
        "            \n",
        "            # Optimize integer types\n",
        "            elif df[col].dtype in ['int64', 'float64']:\n",
        "                if df[col].min() >= 0 and df[col].max() <= 255:\n",
        "                    df[col] = df[col].astype('uint8')\n",
        "                    conversions[col] = f\"{original_dtype} → uint8\"\n",
        "                elif df[col].min() >= -128 and df[col].max() <= 127:\n",
        "                    df[col] = df[col].astype('int8')\n",
        "                    conversions[col] = f\"{original_dtype} → int8\"\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Data Type Validation',\n",
        "            'conversions': conversions\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose'] and conversions:\n",
        "            print(\"Data type conversions:\")\n",
        "            for col, conversion in conversions.items():\n",
        "                print(f\"  {col}: {conversion}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _handle_outliers(self, df):\n",
        "        \"\"\"Handle outliers in numerical columns\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n5️⃣ Handling Outliers\")\n",
        "            print(\"-\" * 22)\n",
        "        \n",
        "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        outliers_handled = {}\n",
        "        \n",
        "        for col in numerical_cols:\n",
        "            if self.config['outlier_method'] == 'iqr':\n",
        "                Q1 = df[col].quantile(0.25)\n",
        "                Q3 = df[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                \n",
        "                outliers_before = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
        "                \n",
        "                if outliers_before > 0:\n",
        "                    # Cap outliers\n",
        "                    df[col] = np.clip(df[col], lower_bound, upper_bound)\n",
        "                    outliers_handled[col] = outliers_before\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Outlier Handling',\n",
        "            'method': self.config['outlier_method'],\n",
        "            'outliers_handled': outliers_handled\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose'] and outliers_handled:\n",
        "            print(\"Outliers handled:\")\n",
        "            for col, count in outliers_handled.items():\n",
        "                print(f\"  {col}: {count} outliers capped\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _create_features(self, df):\n",
        "        \"\"\"Create basic engineered features\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n6️⃣ Creating Features\")\n",
        "            print(\"-\" * 22)\n",
        "        \n",
        "        # This is a basic implementation - can be extended\n",
        "        features_created = []\n",
        "        \n",
        "        # Example: Create age groups if age column exists\n",
        "        if 'age' in df.columns:\n",
        "            df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 50, 100], \n",
        "                                   labels=['Young', 'Adult', 'Middle', 'Senior'])\n",
        "            features_created.append('age_group')\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Feature Creation',\n",
        "            'features_created': features_created\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose'] and features_created:\n",
        "            print(f\"Features created: {features_created}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _apply_scaling(self, df):\n",
        "        \"\"\"Apply scaling to numerical columns\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(f\"\\n7️⃣ Applying {self.config['scaling_method'].title()} Scaling\")\n",
        "            print(\"-\" * 30)\n",
        "        \n",
        "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        \n",
        "        if self.config['scaling_method'] == 'standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif self.config['scaling_method'] == 'minmax':\n",
        "            scaler = MinMaxScaler()\n",
        "        elif self.config['scaling_method'] == 'robust':\n",
        "            from sklearn.preprocessing import RobustScaler\n",
        "            scaler = RobustScaler()\n",
        "        \n",
        "        if len(numerical_cols) > 0:\n",
        "            scaled_data = scaler.fit_transform(df[numerical_cols])\n",
        "            df[numerical_cols] = scaled_data\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Scaling',\n",
        "            'method': self.config['scaling_method'],\n",
        "            'columns_scaled': list(numerical_cols)\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose']:\n",
        "            print(f\"Columns scaled: {list(numerical_cols)}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _print_final_report(self):\n",
        "        \"\"\"Print final cleaning report\"\"\"\n",
        "        print(\"\\n📊 CLEANING PIPELINE SUMMARY\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"Original shape: {self.cleaning_report['original_shape']}\")\n",
        "        print(f\"Final shape: {self.cleaning_report['final_shape']}\")\n",
        "        print(f\"Rows removed: {self.cleaning_report['rows_removed']}\")\n",
        "        print(f\"Columns added: {self.cleaning_report['columns_added']}\")\n",
        "        print(\"\\nSteps completed:\")\n",
        "        for i, step in enumerate(self.cleaning_report['steps'], 1):\n",
        "            print(f\"  {i}. {step['step']}\")\n",
        "\n",
        "# Test the pipeline with our messy dataset\n",
        "print(\"🚀 TESTING THE DATA CLEANING PIPELINE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Configure the pipeline\n",
        "pipeline_config = {\n",
        "    'handle_missing': True,\n",
        "    'missing_strategy': 'median',\n",
        "    'handle_duplicates': True,\n",
        "    'duplicate_subset': ['name', 'email'],\n",
        "    'handle_outliers': True,\n",
        "    'outlier_method': 'iqr',\n",
        "    'standardize_text': True,\n",
        "    'validate_data_types': True,\n",
        "    'create_features': True,\n",
        "    'scaling_method': None,  # Don't scale for this example\n",
        "    'verbose': True\n",
        "}\n",
        "\n",
        "# Create and run the pipeline\n",
        "pipeline = DataCleaningPipeline(config=pipeline_config)\n",
        "df_final, report = pipeline.clean_data(df_messy)\n",
        "\n",
        "print(\"\\n🎉 PIPELINE COMPLETED!\")\n",
        "print(f\"Check the df_final variable for your cleaned dataset.\")\n",
        "print(f\"Check the report variable for detailed cleaning statistics.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 10. Final Practice Challenges 🎯\n",
        "\n",
        "## Test Your Knowledge\n",
        "\n",
        "Now that you've learned the essential data cleaning and transformation techniques, let's put your skills to the test with some real-world challenges!\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 🏆 Challenge 1: E-commerce Dataset\n",
        "**Scenario**: You've received a messy e-commerce dataset with customer orders.\n",
        "\n",
        "**Your Mission**:\n",
        "1. Clean inconsistent product categories\n",
        "2. Handle missing shipping addresses\n",
        "3. Standardize date formats\n",
        "4. Detect and handle price outliers\n",
        "5. Create features for customer segmentation\n",
        "\n",
        "### 🏆 Challenge 2: Time Series Sensor Data\n",
        "**Scenario**: IoT sensor data with missing readings and anomalous values.\n",
        "\n",
        "**Your Mission**:\n",
        "1. Handle missing time series data using interpolation\n",
        "2. Detect and clean sensor anomalies\n",
        "3. Create rolling averages and trend features\n",
        "4. Standardize timestamps across different time zones\n",
        "\n",
        "### 🏆 Challenge 3: Social Media Analytics\n",
        "**Scenario**: Social media posts with mixed languages, emojis, and hashtags.\n",
        "\n",
        "**Your Mission**:\n",
        "1. Clean and standardize text content\n",
        "2. Extract hashtags and mentions\n",
        "3. Handle multilingual content\n",
        "4. Create engagement rate features\n",
        "5. Detect duplicate or spam content\n",
        "\n",
        "### 📚 Additional Resources for Continued Learning\n",
        "\n",
        "**Books**:\n",
        "- \"Python for Data Analysis\" by Wes McKinney\n",
        "- \"Data Science from Scratch\" by Joel Grus\n",
        "- \"Hands-On Data Preprocessing in Python\" by Roy Jafari\n",
        "\n",
        "**Online Courses**:\n",
        "- Kaggle Learn: Data Cleaning\n",
        "- DataCamp: Data Manipulation with Python\n",
        "- Coursera: Data Science Specialization\n",
        "\n",
        "**Documentation**:\n",
        "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
        "- [Scikit-learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
        "- [NumPy Documentation](https://numpy.org/doc/)\n",
        "\n",
        "---\n",
        "\n",
        "## 🎉 Congratulations!\n",
        "\n",
        "You've completed the Data Transformation and Cleaning Masterclass! You now have the knowledge and tools to:\n",
        "\n",
        "✅ **Identify** common data quality issues  \n",
        "✅ **Handle** missing values with appropriate strategies  \n",
        "✅ **Transform** data types and formats  \n",
        "✅ **Detect and treat** outliers  \n",
        "✅ **Clean and standardize** text data  \n",
        "✅ **Create** meaningful features  \n",
        "✅ **Build** automated cleaning pipelines  \n",
        "\n",
        "### 🚀 Next Steps\n",
        "\n",
        "1. **Practice** with real datasets from Kaggle, UCI ML Repository, or your own projects\n",
        "2. **Experiment** with different techniques on various data types\n",
        "3. **Build** your own custom cleaning functions for specific domain problems\n",
        "4. **Share** your knowledge by contributing to open-source projects or mentoring others\n",
        "\n",
        "Remember: Data cleaning is both an art and a science. The more you practice, the better you'll become at making smart decisions about how to handle messy data!\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Data Cleaning!** 🧹✨\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
