{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Transformation and Cleaning Masterclass üßπüìä\n",
        "\n",
        "## A Comprehensive Learning Journey for Data Science and Analysis\n",
        "\n",
        "Welcome to this comprehensive learning document for transforming and cleaning data using Python! This notebook will take you from basic concepts to advanced techniques used in professional Data Science and Data Analysis workflows.\n",
        "\n",
        "### üéØ Learning Objectives\n",
        "By the end of this masterclass, you will be able to:\n",
        "- Identify and handle common data quality issues\n",
        "- Transform data into analysis-ready formats\n",
        "- Apply appropriate cleaning techniques for different data types\n",
        "- Build robust data preprocessing pipelines\n",
        "- Handle real-world messy datasets with confidence\n",
        "\n",
        "### üìö Course Structure\n",
        "This notebook is organized into progressive modules:\n",
        "\n",
        "1. **Foundation** - Basic concepts and setup\n",
        "2. **Data Inspection & Understanding** - Getting to know your data\n",
        "3. **Missing Data Mastery** - Handling incomplete information\n",
        "4. **Data Type Transformations** - Converting and standardizing formats\n",
        "5. **Duplicate Detection & Removal** - Ensuring data uniqueness\n",
        "6. **Text Data Cleaning** - Working with strings and text\n",
        "7. **Outlier Detection & Treatment** - Identifying and handling anomalies\n",
        "8. **Data Transformation Techniques** - Scaling, encoding, and feature engineering\n",
        "9. **Data Reshaping & Aggregation** - Restructuring data for analysis\n",
        "10. **Advanced Topics** - Time series, large datasets, and automation\n",
        "11. **Real-World Case Studies** - Putting it all together\n",
        "\n",
        "### üí° How to Use This Notebook\n",
        "- Run each cell sequentially to build understanding\n",
        "- Experiment with the provided examples\n",
        "- Try the practice exercises at the end of each section\n",
        "- Reference back to sections as needed during your data science projects\n",
        "\n",
        "Let's begin our journey into the world of data transformation and cleaning!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 1. Foundation üèóÔ∏è\n",
        "\n",
        "## Essential Libraries and Concepts\n",
        "\n",
        "Before we dive into data cleaning, let's establish our foundation with the essential libraries and understand what makes data \"dirty\" or \"messy.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for data cleaning and transformation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for better visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Print library versions for reproducibility\n",
        "print(\"üì¶ Library Versions:\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {matplotlib.__version__}\")  # Fixed: use matplotlib.__version__ instead of plt.__version__\n",
        "print(f\"seaborn: {sns.__version__}\")\n",
        "\n",
        "print(\"\\n‚úÖ Foundation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Understanding Data Quality Issues\n",
        "\n",
        "Real-world data is rarely perfect. Let's understand the common types of data quality problems we encounter:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Sample Messy Dataset:\n",
            "==================================================\n",
            "    customer_id           name              email    age   salary  \\\n",
            "0             1       John Doe     john@email.com   25.0  50000.0   \n",
            "1             2     jane smith     Jane@Email.Com   30.0  60000.0   \n",
            "2             3     MIKE JONES     mike@email.com    NaN  70000.0   \n",
            "3             4   Sarah Wilson      sarah@invalid   22.0      NaN   \n",
            "4             5       john doe     john@email.com   25.0  50000.0   \n",
            "5             6      Bob Brown      bob@email.com   35.0  80000.0   \n",
            "6             7           None  missing@email.com   28.0  55000.0   \n",
            "7             8  Alice Johnson    alice@email.com  150.0  90000.0   \n",
            "8             9      Tom Davis      tom@email.com   29.0  65000.0   \n",
            "9            10     jane smith     Jane@Email.Com   30.0  60000.0   \n",
            "10           11    Lisa Miller                      32.0  75000.0   \n",
            "11           12      David Lee    david@email.com   27.0  58000.0   \n",
            "12           13     Emma White     emma@email.com   24.0  52000.0   \n",
            "13           14    Chris Black    chris@email.com   31.0  67000.0   \n",
            "14           15     Anna Green                NaN   26.0  54000.0   \n",
            "\n",
            "     department     join_date  performance_score  \n",
            "0         Sales    2020-01-15               85.5  \n",
            "1     marketing    2021/03/10               92.0  \n",
            "2   ENGINEERING    2019-12-01               78.5  \n",
            "3         Sales    2022-06-15                NaN  \n",
            "4         Sales    2020-01-15               85.5  \n",
            "5   Engineering    2021-08-20               95.0  \n",
            "6     Marketing    2020-11-05               88.5  \n",
            "7         sales  invalid_date               72.0  \n",
            "8   Engineering    2021-05-10               91.5  \n",
            "9     marketing    2021/03/10               92.0  \n",
            "10           HR    2022-01-30               89.0  \n",
            "11  Engineering    2020-09-12               87.5  \n",
            "12        Sales    2021-11-25               83.0  \n",
            "13    Marketing    2022-04-18               90.5  \n",
            "14           HR    2020-07-08               86.0  \n",
            "\n",
            "üìä Data Info:\n",
            "==============================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15 entries, 0 to 14\n",
            "Data columns (total 8 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   customer_id        15 non-null     int64  \n",
            " 1   name               14 non-null     object \n",
            " 2   email              14 non-null     object \n",
            " 3   age                14 non-null     float64\n",
            " 4   salary             14 non-null     float64\n",
            " 5   department         15 non-null     object \n",
            " 6   join_date          15 non-null     object \n",
            " 7   performance_score  14 non-null     float64\n",
            "dtypes: float64(3), int64(1), object(4)\n",
            "memory usage: 1.1+ KB\n",
            "None\n",
            "\n",
            "üö® Data Quality Issues Present:\n",
            "========================================\n",
            "‚Ä¢ Missing values (None, NaN, empty strings)\n",
            "‚Ä¢ Inconsistent formatting (case variations)\n",
            "‚Ä¢ Duplicate records\n",
            "‚Ä¢ Invalid data (age = 150, invalid email formats)\n",
            "‚Ä¢ Inconsistent date formats\n",
            "‚Ä¢ Data type issues\n",
            "‚Ä¢ Inconsistent categorical values\n"
          ]
        }
      ],
      "source": [
        "# Create a sample \"messy\" dataset to demonstrate common data quality issues\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic messy data\n",
        "messy_data = {\n",
        "    'customer_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
        "    'name': ['John Doe', 'jane smith', 'MIKE JONES', 'Sarah Wilson', 'john doe', \n",
        "             'Bob Brown', None, 'Alice Johnson', 'Tom Davis', 'jane smith',\n",
        "             'Lisa Miller', 'David Lee', 'Emma White', 'Chris Black', 'Anna Green'],\n",
        "    'email': ['john@email.com', 'Jane@Email.Com', 'mike@email.com', 'sarah@invalid',\n",
        "              'john@email.com', 'bob@email.com', 'missing@email.com', 'alice@email.com',\n",
        "              'tom@email.com', 'Jane@Email.Com', '', 'david@email.com', \n",
        "              'emma@email.com', 'chris@email.com', np.nan],\n",
        "    'age': [25, 30, None, 22, 25, 35, 28, 150, 29, 30, 32, 27, 24, 31, 26],\n",
        "    'salary': [50000, 60000, 70000, None, 50000, 80000, 55000, 90000, 65000, \n",
        "               60000, 75000, 58000, 52000, 67000, 54000],\n",
        "    'department': ['Sales', 'marketing', 'ENGINEERING', 'Sales', 'Sales', \n",
        "                   'Engineering', 'Marketing', 'sales', 'Engineering', 'marketing',\n",
        "                   'HR', 'Engineering', 'Sales', 'Marketing', 'HR'],\n",
        "    'join_date': ['2020-01-15', '2021/03/10', '2019-12-01', '2022-06-15', '2020-01-15',\n",
        "                  '2021-08-20', '2020-11-05', 'invalid_date', '2021-05-10', '2021/03/10',\n",
        "                  '2022-01-30', '2020-09-12', '2021-11-25', '2022-04-18', '2020-07-08'],\n",
        "    'performance_score': [85.5, 92.0, 78.5, None, 85.5, 95.0, 88.5, 72.0, 91.5, 92.0,\n",
        "                          89.0, 87.5, 83.0, 90.5, 86.0]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df_messy = pd.DataFrame(messy_data)\n",
        "\n",
        "print(\"üîç Sample Messy Dataset:\")\n",
        "print(\"=\" * 50)\n",
        "print(df_messy)\n",
        "\n",
        "print(\"\\nüìä Data Info:\")\n",
        "print(\"=\" * 30)\n",
        "print(df_messy.info())\n",
        "\n",
        "print(\"\\nüö® Data Quality Issues Present:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"‚Ä¢ Missing values (None, NaN, empty strings)\")\n",
        "print(\"‚Ä¢ Inconsistent formatting (case variations)\")\n",
        "print(\"‚Ä¢ Duplicate records\")\n",
        "print(\"‚Ä¢ Invalid data (age = 150, invalid email formats)\")\n",
        "print(\"‚Ä¢ Inconsistent date formats\") \n",
        "print(\"‚Ä¢ Data type issues\")\n",
        "print(\"‚Ä¢ Inconsistent categorical values\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 2. Data Inspection & Understanding üîç\n",
        "\n",
        "## Getting to Know Your Data\n",
        "\n",
        "The first step in any data cleaning process is understanding what you're working with. Let's explore various techniques to inspect and understand our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç COMPREHENSIVE DATA INSPECTION REPORT\n",
            "============================================================\n",
            "\n",
            "üìè Dataset Shape: 15 rows √ó 8 columns\n",
            "üíæ Memory Usage: 0.00 MB\n",
            "\n",
            "üìã Column Information:\n",
            "------------------------------\n",
            " 1. customer_id          | int64        | Nulls:   0 (  0.0%) | Unique: 15\n",
            " 2. name                 | object       | Nulls:   1 (  6.7%) | Unique: 13\n",
            " 3. email                | object       | Nulls:   1 (  6.7%) | Unique: 12\n",
            " 4. age                  | float64      | Nulls:   1 (  6.7%) | Unique: 12\n",
            " 5. salary               | float64      | Nulls:   1 (  6.7%) | Unique: 12\n",
            " 6. department           | object       | Nulls:   0 (  0.0%) | Unique: 7\n",
            " 7. join_date            | object       | Nulls:   0 (  0.0%) | Unique: 13\n",
            " 8. performance_score    | float64      | Nulls:   1 (  6.7%) | Unique: 12\n",
            "\n",
            "üìÑ First 5 rows:\n",
            "------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_id</th>\n",
              "      <th>name</th>\n",
              "      <th>email</th>\n",
              "      <th>age</th>\n",
              "      <th>salary</th>\n",
              "      <th>department</th>\n",
              "      <th>join_date</th>\n",
              "      <th>performance_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>John Doe</td>\n",
              "      <td>john@email.com</td>\n",
              "      <td>25.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>Sales</td>\n",
              "      <td>2020-01-15</td>\n",
              "      <td>85.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>jane smith</td>\n",
              "      <td>Jane@Email.Com</td>\n",
              "      <td>30.0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>marketing</td>\n",
              "      <td>2021/03/10</td>\n",
              "      <td>92.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>MIKE JONES</td>\n",
              "      <td>mike@email.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>ENGINEERING</td>\n",
              "      <td>2019-12-01</td>\n",
              "      <td>78.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Sarah Wilson</td>\n",
              "      <td>sarah@invalid</td>\n",
              "      <td>22.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sales</td>\n",
              "      <td>2022-06-15</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>john doe</td>\n",
              "      <td>john@email.com</td>\n",
              "      <td>25.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>Sales</td>\n",
              "      <td>2020-01-15</td>\n",
              "      <td>85.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   customer_id          name           email   age   salary   department  \\\n",
              "0            1      John Doe  john@email.com  25.0  50000.0        Sales   \n",
              "1            2    jane smith  Jane@Email.Com  30.0  60000.0    marketing   \n",
              "2            3    MIKE JONES  mike@email.com   NaN  70000.0  ENGINEERING   \n",
              "3            4  Sarah Wilson   sarah@invalid  22.0      NaN        Sales   \n",
              "4            5      john doe  john@email.com  25.0  50000.0        Sales   \n",
              "\n",
              "    join_date  performance_score  \n",
              "0  2020-01-15               85.5  \n",
              "1  2021/03/10               92.0  \n",
              "2  2019-12-01               78.5  \n",
              "3  2022-06-15                NaN  \n",
              "4  2020-01-15               85.5  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìÑ Last 5 rows:\n",
            "------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_id</th>\n",
              "      <th>name</th>\n",
              "      <th>email</th>\n",
              "      <th>age</th>\n",
              "      <th>salary</th>\n",
              "      <th>department</th>\n",
              "      <th>join_date</th>\n",
              "      <th>performance_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>Lisa Miller</td>\n",
              "      <td></td>\n",
              "      <td>32.0</td>\n",
              "      <td>75000.0</td>\n",
              "      <td>HR</td>\n",
              "      <td>2022-01-30</td>\n",
              "      <td>89.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>David Lee</td>\n",
              "      <td>david@email.com</td>\n",
              "      <td>27.0</td>\n",
              "      <td>58000.0</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>2020-09-12</td>\n",
              "      <td>87.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>Emma White</td>\n",
              "      <td>emma@email.com</td>\n",
              "      <td>24.0</td>\n",
              "      <td>52000.0</td>\n",
              "      <td>Sales</td>\n",
              "      <td>2021-11-25</td>\n",
              "      <td>83.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>Chris Black</td>\n",
              "      <td>chris@email.com</td>\n",
              "      <td>31.0</td>\n",
              "      <td>67000.0</td>\n",
              "      <td>Marketing</td>\n",
              "      <td>2022-04-18</td>\n",
              "      <td>90.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>Anna Green</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26.0</td>\n",
              "      <td>54000.0</td>\n",
              "      <td>HR</td>\n",
              "      <td>2020-07-08</td>\n",
              "      <td>86.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    customer_id         name            email   age   salary   department  \\\n",
              "10           11  Lisa Miller                   32.0  75000.0           HR   \n",
              "11           12    David Lee  david@email.com  27.0  58000.0  Engineering   \n",
              "12           13   Emma White   emma@email.com  24.0  52000.0        Sales   \n",
              "13           14  Chris Black  chris@email.com  31.0  67000.0    Marketing   \n",
              "14           15   Anna Green              NaN  26.0  54000.0           HR   \n",
              "\n",
              "     join_date  performance_score  \n",
              "10  2022-01-30               89.0  \n",
              "11  2020-09-12               87.5  \n",
              "12  2021-11-25               83.0  \n",
              "13  2022-04-18               90.5  \n",
              "14  2020-07-08               86.0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Statistical Summary:\n",
            "------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_id</th>\n",
              "      <th>name</th>\n",
              "      <th>email</th>\n",
              "      <th>age</th>\n",
              "      <th>salary</th>\n",
              "      <th>department</th>\n",
              "      <th>join_date</th>\n",
              "      <th>performance_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>15.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>14.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>jane smith</td>\n",
              "      <td>john@email.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sales</td>\n",
              "      <td>2020-01-15</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>8.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36.714286</td>\n",
              "      <td>63285.714286</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>86.892857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.472136</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>32.791717</td>\n",
              "      <td>12015.557681</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.022964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>72.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>4.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25.250000</td>\n",
              "      <td>54250.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>85.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>28.500000</td>\n",
              "      <td>60000.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>88.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>11.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.750000</td>\n",
              "      <td>69250.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>91.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>15.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>90000.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>95.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        customer_id        name           email         age        salary  \\\n",
              "count     15.000000          14              14   14.000000     14.000000   \n",
              "unique          NaN          13              12         NaN           NaN   \n",
              "top             NaN  jane smith  john@email.com         NaN           NaN   \n",
              "freq            NaN           2               2         NaN           NaN   \n",
              "mean       8.000000         NaN             NaN   36.714286  63285.714286   \n",
              "std        4.472136         NaN             NaN   32.791717  12015.557681   \n",
              "min        1.000000         NaN             NaN   22.000000  50000.000000   \n",
              "25%        4.500000         NaN             NaN   25.250000  54250.000000   \n",
              "50%        8.000000         NaN             NaN   28.500000  60000.000000   \n",
              "75%       11.500000         NaN             NaN   30.750000  69250.000000   \n",
              "max       15.000000         NaN             NaN  150.000000  90000.000000   \n",
              "\n",
              "       department   join_date  performance_score  \n",
              "count          15          15          14.000000  \n",
              "unique          7          13                NaN  \n",
              "top         Sales  2020-01-15                NaN  \n",
              "freq            4           2                NaN  \n",
              "mean          NaN         NaN          86.892857  \n",
              "std           NaN         NaN           6.022964  \n",
              "min           NaN         NaN          72.000000  \n",
              "25%           NaN         NaN          85.500000  \n",
              "50%           NaN         NaN          88.000000  \n",
              "75%           NaN         NaN          91.250000  \n",
              "max           NaN         NaN          95.000000  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ùå Missing Data Analysis:\n",
            "------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Missing Count</th>\n",
              "      <th>Missing Percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>name</th>\n",
              "      <td>1</td>\n",
              "      <td>6.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>email</th>\n",
              "      <td>1</td>\n",
              "      <td>6.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>1</td>\n",
              "      <td>6.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>salary</th>\n",
              "      <td>1</td>\n",
              "      <td>6.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>performance_score</th>\n",
              "      <td>1</td>\n",
              "      <td>6.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Missing Count  Missing Percentage\n",
              "name                           1            6.666667\n",
              "email                          1            6.666667\n",
              "age                            1            6.666667\n",
              "salary                         1            6.666667\n",
              "performance_score              1            6.666667"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ö†Ô∏è  Potential Data Type Issues:\n",
            "------------------------------\n",
            "name: ['John Doe', 'jane smith', 'MIKE JONES', 'Sarah Wilson', 'john doe', 'Bob Brown', 'Alice Johnson', 'Tom Davis', 'jane smith', 'Lisa Miller']\n",
            "email: ['john@email.com', 'Jane@Email.Com', 'mike@email.com', 'sarah@invalid', 'john@email.com', 'bob@email.com', 'missing@email.com', 'alice@email.com', 'tom@email.com', 'Jane@Email.Com']\n",
            "department: ['Sales', 'marketing', 'ENGINEERING', 'Sales', 'Sales', 'Engineering', 'Marketing', 'sales', 'Engineering', 'marketing']\n",
            "join_date: ['2020-01-15', '2021/03/10', '2019-12-01', '2022-06-15', '2020-01-15', '2021-08-20', '2020-11-05', 'invalid_date', '2021-05-10', '2021/03/10']\n"
          ]
        }
      ],
      "source": [
        "# Comprehensive data inspection toolkit\n",
        "\n",
        "def comprehensive_data_inspection(df, sample_size=5):\n",
        "    \"\"\"\n",
        "    Perform a comprehensive inspection of a DataFrame\n",
        "    \n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The DataFrame to inspect\n",
        "    sample_size (int): Number of sample rows to display\n",
        "    \"\"\"\n",
        "    print(\"üîç COMPREHENSIVE DATA INSPECTION REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 1. Basic Information\n",
        "    print(f\"\\nüìè Dataset Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "    print(f\"üíæ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # 2. Column Information\n",
        "    print(f\"\\nüìã Column Information:\")\n",
        "    print(\"-\" * 30)\n",
        "    for i, col in enumerate(df.columns, 1):\n",
        "        dtype = df[col].dtype\n",
        "        null_count = df[col].isnull().sum()\n",
        "        null_pct = (null_count / len(df)) * 100\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"{i:2d}. {col:<20} | {str(dtype):<12} | Nulls: {null_count:3d} ({null_pct:5.1f}%) | Unique: {unique_count}\")\n",
        "    \n",
        "    # 3. Sample Data\n",
        "    print(f\"\\nüìÑ First {sample_size} rows:\")\n",
        "    print(\"-\" * 30)\n",
        "    display(df.head(sample_size))\n",
        "    \n",
        "    print(f\"\\nüìÑ Last {sample_size} rows:\")\n",
        "    print(\"-\" * 30)\n",
        "    display(df.tail(sample_size))\n",
        "    \n",
        "    # 4. Statistical Summary\n",
        "    print(f\"\\nüìä Statistical Summary:\")\n",
        "    print(\"-\" * 30)\n",
        "    display(df.describe(include='all'))\n",
        "    \n",
        "    # 5. Missing Data Analysis\n",
        "    print(f\"\\n‚ùå Missing Data Analysis:\")\n",
        "    print(\"-\" * 30)\n",
        "    missing_data = df.isnull().sum()\n",
        "    missing_pct = (missing_data / len(df)) * 100\n",
        "    missing_summary = pd.DataFrame({\n",
        "        'Missing Count': missing_data,\n",
        "        'Missing Percentage': missing_pct\n",
        "    }).sort_values('Missing Count', ascending=False)\n",
        "    display(missing_summary[missing_summary['Missing Count'] > 0])\n",
        "    \n",
        "    # 6. Data Types Issues\n",
        "    print(f\"\\n‚ö†Ô∏è  Potential Data Type Issues:\")\n",
        "    print(\"-\" * 30)\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            # Check for numeric values stored as strings\n",
        "            sample_values = df[col].dropna().astype(str).head(10).tolist()\n",
        "            print(f\"{col}: {sample_values}\")\n",
        "    \n",
        "    return missing_summary\n",
        "\n",
        "# Run comprehensive inspection on our messy dataset\n",
        "missing_analysis = comprehensive_data_inspection(df_messy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Data Quality Visualization:\n",
            "========================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä Data Quality Visualization:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mvisualize_data_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_messy\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mvisualize_data_quality\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvisualize_data_quality\u001b[39m(df):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create visualizations to understand data quality issues\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     fig, axes = \u001b[43mplt\u001b[49m.subplots(\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m10\u001b[39m))\n\u001b[32m      7\u001b[39m     fig.suptitle(\u001b[33m'\u001b[39m\u001b[33mData Quality Visualization Dashboard\u001b[39m\u001b[33m'\u001b[39m, fontsize=\u001b[32m16\u001b[39m, fontweight=\u001b[33m'\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# 1. Missing Data Heatmap\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "# Visual inspection techniques\n",
        "\n",
        "def visualize_data_quality(df):\n",
        "    \"\"\"Create visualizations to understand data quality issues\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Data Quality Visualization Dashboard', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Missing Data Heatmap\n",
        "    ax1 = axes[0, 0]\n",
        "    sns.heatmap(df.isnull(), cbar=True, ax=ax1, cmap='viridis', \n",
        "                yticklabels=False, cbar_kws={'label': 'Missing Data'})\n",
        "    ax1.set_title('Missing Data Pattern', fontweight='bold')\n",
        "    ax1.set_xlabel('Columns')\n",
        "    \n",
        "    # 2. Missing Data Bar Chart\n",
        "    ax2 = axes[0, 1]\n",
        "    missing_counts = df.isnull().sum()\n",
        "    missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
        "    if len(missing_counts) > 0:\n",
        "        missing_counts.plot(kind='bar', ax=ax2, color='coral')\n",
        "        ax2.set_title('Missing Data Count by Column', fontweight='bold')\n",
        "        ax2.set_ylabel('Number of Missing Values')\n",
        "        ax2.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 3. Data Types Distribution\n",
        "    ax3 = axes[1, 0]\n",
        "    dtype_counts = df.dtypes.value_counts()\n",
        "    dtype_counts.plot(kind='pie', ax=ax3, autopct='%1.1f%%', startangle=90)\n",
        "    ax3.set_title('Data Types Distribution', fontweight='bold')\n",
        "    ax3.set_ylabel('')\n",
        "    \n",
        "    # 4. Unique Values per Column\n",
        "    ax4 = axes[1, 1]\n",
        "    unique_counts = df.nunique().sort_values(ascending=True)\n",
        "    unique_counts.plot(kind='barh', ax=ax4, color='lightblue')\n",
        "    ax4.set_title('Unique Values per Column', fontweight='bold')\n",
        "    ax4.set_xlabel('Number of Unique Values')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize our messy data quality\n",
        "print(\"üìä Data Quality Visualization:\")\n",
        "print(\"=\" * 40)\n",
        "visualize_data_quality(df_messy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 3. Missing Data Mastery üï≥Ô∏è\n",
        "\n",
        "## Handling Incomplete Information\n",
        "\n",
        "Missing data is one of the most common challenges in data science. Let's explore different strategies for identifying and handling missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç IDENTIFYING MISSING VALUES\n",
            "==================================================\n",
            "Original missing value patterns:\n",
            "-----------------------------------\n",
            "name: 1 missing values\n",
            "  Indices with missing data: [6]\n",
            "\n",
            "email: 2 missing values\n",
            "  Indices with missing data: [10, 14]\n",
            "\n",
            "age: 1 missing values\n",
            "  Indices with missing data: [2]\n",
            "\n",
            "salary: 1 missing values\n",
            "  Indices with missing data: [3]\n",
            "\n",
            "performance_score: 1 missing values\n",
            "  Indices with missing data: [3]\n",
            "\n",
            "üéØ Missing Value Patterns Analysis:\n",
            "-----------------------------------\n",
            "Explicit missing (NaN/None): {'name': np.int64(1), 'email': np.int64(2), 'age': np.int64(1), 'salary': np.int64(1), 'performance_score': np.int64(1)}\n",
            "Implicit missing: {}\n",
            "Combined missing patterns: {('performance_score', 'salary'): np.int64(1)}\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Identify different types of missing values\n",
        "print(\"üîç IDENTIFYING MISSING VALUES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create a copy of our messy data for missing value analysis\n",
        "df_missing = df_messy.copy()\n",
        "\n",
        "# Replace empty strings with NaN for consistent handling\n",
        "df_missing = df_missing.replace('', np.nan)\n",
        "\n",
        "print(\"Original missing value patterns:\")\n",
        "print(\"-\" * 35)\n",
        "for col in df_missing.columns:\n",
        "    total_missing = df_missing[col].isnull().sum()\n",
        "    if total_missing > 0:\n",
        "        print(f\"{col}: {total_missing} missing values\")\n",
        "        # Show unique patterns of missing data\n",
        "        missing_mask = df_missing[col].isnull()\n",
        "        print(f\"  Indices with missing data: {df_missing[missing_mask].index.tolist()}\")\n",
        "        print()\n",
        "\n",
        "# Advanced missing value detection\n",
        "def detect_missing_patterns(df):\n",
        "    \"\"\"Detect various patterns of missing data\"\"\"\n",
        "    \n",
        "    patterns = {}\n",
        "    \n",
        "    # 1. Explicit missing values (NaN, None)\n",
        "    explicit_missing = df.isnull().sum()\n",
        "    patterns['explicit'] = explicit_missing[explicit_missing > 0]\n",
        "    \n",
        "    # 2. Implicit missing values (empty strings, specific values)\n",
        "    implicit_missing = {}\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        # Count empty strings\n",
        "        empty_strings = (df[col] == '').sum()\n",
        "        # Count placeholder values\n",
        "        placeholder_values = df[col].isin(['N/A', 'n/a', 'NULL', 'null', 'missing', 'Missing']).sum()\n",
        "        \n",
        "        if empty_strings > 0 or placeholder_values > 0:\n",
        "            implicit_missing[col] = {\n",
        "                'empty_strings': empty_strings,\n",
        "                'placeholders': placeholder_values\n",
        "            }\n",
        "    \n",
        "    patterns['implicit'] = implicit_missing\n",
        "    \n",
        "    # 3. Pattern-based missing (e.g., all missing together)\n",
        "    # Find columns that tend to be missing together\n",
        "    missing_combinations = {}\n",
        "    for col1 in df.columns:\n",
        "        for col2 in df.columns:\n",
        "            if col1 != col2:\n",
        "                both_missing = ((df[col1].isnull()) & (df[col2].isnull())).sum()\n",
        "                if both_missing > 0:\n",
        "                    key = tuple(sorted([col1, col2]))\n",
        "                    if key not in missing_combinations:\n",
        "                        missing_combinations[key] = both_missing\n",
        "    \n",
        "    patterns['combinations'] = missing_combinations\n",
        "    \n",
        "    return patterns\n",
        "\n",
        "# Analyze missing patterns\n",
        "missing_patterns = detect_missing_patterns(df_missing)\n",
        "\n",
        "print(\"üéØ Missing Value Patterns Analysis:\")\n",
        "print(\"-\" * 35)\n",
        "print(f\"Explicit missing (NaN/None): {dict(missing_patterns['explicit'])}\")\n",
        "print(f\"Implicit missing: {missing_patterns['implicit']}\")\n",
        "print(f\"Combined missing patterns: {missing_patterns['combinations']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è MISSING VALUE TREATMENT STRATEGIES\n",
            "==================================================\n",
            "1Ô∏è‚É£ DELETION METHODS\n",
            "-------------------------\n",
            "Original shape: (15, 8)\n",
            "After dropping rows with ANY missing: (10, 8)\n",
            "After dropping rows with ALL missing: (15, 8)\n",
            "After dropping rows with missing age OR salary: (13, 8)\n",
            "After dropping columns with >50% missing: (15, 8)\n",
            "\n",
            "2Ô∏è‚É£ SIMPLE IMPUTATION METHODS\n",
            "------------------------------\n",
            "age:\n",
            "  Mean: 36.71, Median: 28.50, Mode: 25.00\n",
            "  ‚Üí Filled with median: 28.50\n",
            "salary:\n",
            "  Mean: 63285.71, Median: 60000.00, Mode: 50000.00\n",
            "  ‚Üí Filled with median: 60000.00\n",
            "performance_score:\n",
            "  Mean: 86.89, Median: 88.00, Mode: 85.50\n",
            "  ‚Üí Filled with median: 88.00\n",
            "name:\n",
            "  Mode: jane smith\n",
            "  ‚Üí Filled with mode: jane smith\n",
            "email:\n",
            "  Mode: Jane@Email.Com\n",
            "  ‚Üí Filled with mode: Jane@Email.Com\n",
            "\n",
            "Missing values after simple imputation:\n",
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5292/962880715.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_imputed[col].fillna(median_val, inplace=True)\n",
            "/tmp/ipykernel_5292/962880715.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_imputed[col].fillna(median_val, inplace=True)\n",
            "/tmp/ipykernel_5292/962880715.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_imputed[col].fillna(median_val, inplace=True)\n",
            "/tmp/ipykernel_5292/962880715.py:63: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_imputed[col].fillna(mode_val, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Missing Value Treatment Strategies\n",
        "\n",
        "print(\"üõ†Ô∏è MISSING VALUE TREATMENT STRATEGIES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Strategy 1: Deletion Methods\n",
        "print(\"1Ô∏è‚É£ DELETION METHODS\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Remove rows with any missing values\n",
        "df_drop_any = df_missing.dropna()\n",
        "print(f\"Original shape: {df_missing.shape}\")\n",
        "print(f\"After dropping rows with ANY missing: {df_drop_any.shape}\")\n",
        "\n",
        "# Remove rows with all missing values\n",
        "df_drop_all = df_missing.dropna(how='all')\n",
        "print(f\"After dropping rows with ALL missing: {df_drop_all.shape}\")\n",
        "\n",
        "# Remove rows with missing values in specific columns\n",
        "df_drop_subset = df_missing.dropna(subset=['age', 'salary'])\n",
        "print(f\"After dropping rows with missing age OR salary: {df_drop_subset.shape}\")\n",
        "\n",
        "# Remove columns with too many missing values (>50%)\n",
        "threshold = len(df_missing) * 0.5  # 50% threshold\n",
        "df_drop_cols = df_missing.dropna(axis=1, thresh=threshold)\n",
        "print(f\"After dropping columns with >50% missing: {df_drop_cols.shape}\")\n",
        "print()\n",
        "\n",
        "# Strategy 2: Simple Imputation\n",
        "print(\"2Ô∏è‚É£ SIMPLE IMPUTATION METHODS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "df_imputed = df_missing.copy()\n",
        "\n",
        "# Numerical columns: Mean, Median, Mode\n",
        "numerical_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "for col in numerical_cols:\n",
        "    if df_imputed[col].isnull().sum() > 0:\n",
        "        # Calculate different measures\n",
        "        mean_val = df_imputed[col].mean()\n",
        "        median_val = df_imputed[col].median()\n",
        "        mode_val = df_imputed[col].mode().iloc[0] if not df_imputed[col].mode().empty else mean_val\n",
        "        \n",
        "        print(f\"{col}:\")\n",
        "        print(f\"  Mean: {mean_val:.2f}, Median: {median_val:.2f}, Mode: {mode_val:.2f}\")\n",
        "        \n",
        "        # Use median for robust imputation (less sensitive to outliers)\n",
        "        df_imputed[col].fillna(median_val, inplace=True)\n",
        "        print(f\"  ‚Üí Filled with median: {median_val:.2f}\")\n",
        "\n",
        "# Categorical columns: Mode or 'Unknown'\n",
        "categorical_cols = df_imputed.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if df_imputed[col].isnull().sum() > 0:\n",
        "        mode_val = df_imputed[col].mode().iloc[0] if not df_imputed[col].mode().empty else 'Unknown'\n",
        "        \n",
        "        print(f\"{col}:\")\n",
        "        print(f\"  Mode: {mode_val}\")\n",
        "        \n",
        "        # Fill with mode\n",
        "        df_imputed[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"  ‚Üí Filled with mode: {mode_val}\")\n",
        "\n",
        "print(f\"\\nMissing values after simple imputation:\")\n",
        "print(df_imputed.isnull().sum().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3Ô∏è‚É£ ADVANCED IMPUTATION METHODS\n",
            "-----------------------------------\n",
            "Forward Fill (use previous value):\n",
            "    customer_id    age   salary  age_ffill  salary_ffill\n",
            "0             1   25.0  50000.0       25.0       50000.0\n",
            "1             2   30.0  60000.0       30.0       60000.0\n",
            "2             3    NaN  70000.0       30.0       70000.0\n",
            "3             4   22.0      NaN       22.0       70000.0\n",
            "4             5   25.0  50000.0       25.0       50000.0\n",
            "5             6   35.0  80000.0       35.0       80000.0\n",
            "6             7   28.0  55000.0       28.0       55000.0\n",
            "7             8  150.0  90000.0      150.0       90000.0\n",
            "8             9   29.0  65000.0       29.0       65000.0\n",
            "9            10   30.0  60000.0       30.0       60000.0\n",
            "10           11   32.0  75000.0       32.0       75000.0\n",
            "11           12   27.0  58000.0       27.0       58000.0\n",
            "12           13   24.0  52000.0       24.0       52000.0\n",
            "13           14   31.0  67000.0       31.0       67000.0\n",
            "14           15   26.0  54000.0       26.0       54000.0\n",
            "\n",
            "Backward Fill (use next value):\n",
            "    customer_id    age   salary  age_bfill  salary_bfill\n",
            "0             1   25.0  50000.0       25.0       50000.0\n",
            "1             2   30.0  60000.0       30.0       60000.0\n",
            "2             3    NaN  70000.0       22.0       70000.0\n",
            "3             4   22.0      NaN       22.0       50000.0\n",
            "4             5   25.0  50000.0       25.0       50000.0\n",
            "5             6   35.0  80000.0       35.0       80000.0\n",
            "6             7   28.0  55000.0       28.0       55000.0\n",
            "7             8  150.0  90000.0      150.0       90000.0\n",
            "8             9   29.0  65000.0       29.0       65000.0\n",
            "9            10   30.0  60000.0       30.0       60000.0\n",
            "10           11   32.0  75000.0       32.0       75000.0\n",
            "11           12   27.0  58000.0       27.0       58000.0\n",
            "12           13   24.0  52000.0       24.0       52000.0\n",
            "13           14   31.0  67000.0       31.0       67000.0\n",
            "14           15   26.0  54000.0       26.0       54000.0\n",
            "\n",
            "Interpolation (estimate values between known points):\n",
            "    customer_id    age   salary  age_interpolated  salary_interpolated\n",
            "0             1   25.0  50000.0              25.0              50000.0\n",
            "1             2   30.0  60000.0              30.0              60000.0\n",
            "2             3    NaN  70000.0              26.0              70000.0\n",
            "3             4   22.0      NaN              22.0              60000.0\n",
            "4             5   25.0  50000.0              25.0              50000.0\n",
            "5             6   35.0  80000.0              35.0              80000.0\n",
            "6             7   28.0  55000.0              28.0              55000.0\n",
            "7             8  150.0  90000.0             150.0              90000.0\n",
            "8             9   29.0  65000.0              29.0              65000.0\n",
            "9            10   30.0  60000.0              30.0              60000.0\n",
            "10           11   32.0  75000.0              32.0              75000.0\n",
            "11           12   27.0  58000.0              27.0              58000.0\n",
            "12           13   24.0  52000.0              24.0              52000.0\n",
            "13           14   31.0  67000.0              31.0              67000.0\n",
            "14           15   26.0  54000.0              26.0              54000.0\n",
            "\n",
            "üß† KNN Imputation (using similar records):\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5292/3777948802.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_ffill_demo['age_ffill'] = df_ffill_demo['age'].fillna(method='ffill')\n",
            "/tmp/ipykernel_5292/3777948802.py:10: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_ffill_demo['salary_ffill'] = df_ffill_demo['salary'].fillna(method='ffill')\n",
            "/tmp/ipykernel_5292/3777948802.py:15: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_bfill_demo['age_bfill'] = df_bfill_demo['age'].fillna(method='bfill')\n",
            "/tmp/ipykernel_5292/3777948802.py:16: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_bfill_demo['salary_bfill'] = df_bfill_demo['salary'].fillna(method='bfill')\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'KNNImputer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m numerical_data = df_missing[[\u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msalary\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mperformance_score\u001b[39m\u001b[33m'\u001b[39m]].copy()\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# KNN Imputer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m knn_imputer = \u001b[43mKNNImputer\u001b[49m(n_neighbors=\u001b[32m3\u001b[39m)\n\u001b[32m     35\u001b[39m numerical_imputed = knn_imputer.fit_transform(numerical_data)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Create DataFrame with imputed values\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'KNNImputer' is not defined"
          ]
        }
      ],
      "source": [
        "# Strategy 3: Advanced Imputation Techniques\n",
        "print(\"3Ô∏è‚É£ ADVANCED IMPUTATION METHODS\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Forward Fill and Backward Fill (for time series or ordered data)\n",
        "df_ffill = df_missing.copy().sort_values('customer_id')\n",
        "print(\"Forward Fill (use previous value):\")\n",
        "df_ffill_demo = df_ffill[['customer_id', 'age', 'salary']].copy()\n",
        "df_ffill_demo['age_ffill'] = df_ffill_demo['age'].fillna(method='ffill')\n",
        "df_ffill_demo['salary_ffill'] = df_ffill_demo['salary'].fillna(method='ffill')\n",
        "print(df_ffill_demo)\n",
        "\n",
        "print(\"\\nBackward Fill (use next value):\")\n",
        "df_bfill_demo = df_ffill[['customer_id', 'age', 'salary']].copy()\n",
        "df_bfill_demo['age_bfill'] = df_bfill_demo['age'].fillna(method='bfill')\n",
        "df_bfill_demo['salary_bfill'] = df_bfill_demo['salary'].fillna(method='bfill')\n",
        "print(df_bfill_demo)\n",
        "\n",
        "# Interpolation (for numerical data)\n",
        "print(\"\\nInterpolation (estimate values between known points):\")\n",
        "df_interp = df_missing[['customer_id', 'age', 'salary']].copy().sort_values('customer_id')\n",
        "df_interp['age_interpolated'] = df_interp['age'].interpolate()\n",
        "df_interp['salary_interpolated'] = df_interp['salary'].interpolate()\n",
        "print(df_interp)\n",
        "\n",
        "# KNN Imputation (using similar records)\n",
        "print(\"\\nüß† KNN Imputation (using similar records):\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "# Prepare data for KNN imputation (only numerical columns)\n",
        "numerical_data = df_missing[['age', 'salary', 'performance_score']].copy()\n",
        "\n",
        "# KNN Imputer\n",
        "knn_imputer = KNNImputer(n_neighbors=3)\n",
        "numerical_imputed = knn_imputer.fit_transform(numerical_data)\n",
        "\n",
        "# Create DataFrame with imputed values\n",
        "df_knn = df_missing.copy()\n",
        "df_knn[['age', 'salary', 'performance_score']] = numerical_imputed\n",
        "\n",
        "print(\"Before KNN imputation:\")\n",
        "print(df_missing[['customer_id', 'age', 'salary', 'performance_score']].head(10))\n",
        "\n",
        "print(\"\\nAfter KNN imputation:\")\n",
        "print(df_knn[['customer_id', 'age', 'salary', 'performance_score']].head(10))\n",
        "\n",
        "# Compare different imputation strategies\n",
        "print(\"\\nüìä IMPUTATION STRATEGY COMPARISON\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "strategies = {\n",
        "    'Original': df_missing['age'].copy(),\n",
        "    'Mean': df_missing['age'].fillna(df_missing['age'].mean()),\n",
        "    'Median': df_missing['age'].fillna(df_missing['age'].median()),\n",
        "    'KNN': df_knn['age'].copy()\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(strategies)\n",
        "print(\"Age column comparison:\")\n",
        "print(comparison_df.head(10))\n",
        "\n",
        "print(\"\\nStatistical differences:\")\n",
        "for strategy, values in strategies.items():\n",
        "    if strategy != 'Original':\n",
        "        print(f\"{strategy}: Mean={values.mean():.2f}, Std={values.std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìã Practice Exercise: Missing Data\n",
        "\n",
        "**Challenge**: Create a dataset with intentional missing values and apply different imputation strategies. Compare the results and determine which strategy works best for different scenarios.\n",
        "\n",
        "**Your Task**:\n",
        "1. Create a synthetic dataset with missing values\n",
        "2. Apply at least 3 different imputation strategies\n",
        "3. Evaluate the impact on data distribution\n",
        "4. Choose the best strategy and justify your choice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a synthetic dataset with intentional missing values\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Generate random data\n",
        "n_samples = 100\n",
        "\n",
        "# Create mock data with realistic ranges and distributions\n",
        "data = {\n",
        "    'employee_id': range(1, n_samples + 1),\n",
        "    'age': np.random.normal(35, 8, n_samples).astype(int),  # Ages centered around 35\n",
        "    'salary': np.random.normal(60000, 15000, n_samples),    # Salaries centered around 60k\n",
        "    'years_experience': np.random.normal(8, 4, n_samples),   # Experience centered around 8 years\n",
        "    'performance_rating': np.random.normal(7.5, 1.5, n_samples)  # Ratings from 0-10\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df_copy = pd.DataFrame(data)\n",
        "\n",
        "# Ensure realistic bounds\n",
        "df_copy['age'] = df_copy['age'].clip(22, 65)  # Reasonable age range\n",
        "df_copy['salary'] = df_copy['salary'].clip(30000, 120000)  # Reasonable salary range\n",
        "df_copy['years_experience'] = df_copy['years_experience'].clip(0, 40)  # Max 40 years experience\n",
        "df_copy['performance_rating'] = df_copy['performance_rating'].clip(1, 10)  # Rating between 1-10\n",
        "\n",
        "# Add categorical data\n",
        "departments = ['Sales', 'Marketing', 'Engineering', 'HR', 'Finance']\n",
        "education = ['Bachelor', 'Master', 'PhD', 'High School']\n",
        "\n",
        "df_copy['department'] = np.random.choice(departments, n_samples)\n",
        "df_copy['education'] = np.random.choice(education, n_samples)\n",
        "\n",
        "# Introduce missing values randomly (approximately 10% of data)\n",
        "for column in ['age', 'salary', 'years_experience', 'performance_rating', 'department', 'education']:\n",
        "    mask = np.random.random(n_samples) < 0.1  # 10% missing rate\n",
        "    df_copy.loc[mask, column] = np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 100\n",
            "Columns: 7\n",
            "Memory: 0.01 MB\n",
            "==================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 7 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   employee_id         100 non-null    int64  \n",
            " 1   age                 89 non-null     float64\n",
            " 2   salary              94 non-null     float64\n",
            " 3   years_experience    96 non-null     float64\n",
            " 4   performance_rating  88 non-null     float64\n",
            " 5   department          84 non-null     object \n",
            " 6   education           87 non-null     object \n",
            "dtypes: float64(4), int64(1), object(2)\n",
            "memory usage: 5.6+ KB\n",
            "None\n",
            "==================================================\n",
            "       employee_id        age         salary  years_experience  \\\n",
            "count   100.000000  89.000000      94.000000         96.000000   \n",
            "mean     50.500000  34.044944   60184.860140          8.253768   \n",
            "std      29.011492   6.822273   14610.503973          4.251900   \n",
            "min       1.000000  22.000000   31218.431771          0.000000   \n",
            "25%      25.750000  30.000000   47813.594535          5.321731   \n",
            "50%      50.500000  33.000000   60965.948886          8.390783   \n",
            "75%      75.250000  38.000000   67681.114170         10.725053   \n",
            "max     100.000000  49.000000  100802.537499         23.410926   \n",
            "\n",
            "       performance_rating  \n",
            "count           88.000000  \n",
            "mean             7.618948  \n",
            "std              1.252889  \n",
            "min              4.314156  \n",
            "25%              6.649471  \n",
            "50%              7.575236  \n",
            "75%              8.462621  \n",
            "max             10.000000  \n",
            "==================================================\n",
            "employee_id: 0 nulls (0.00%)\n",
            "age: 11 nulls (11.00%)\n",
            "salary: 6 nulls (6.00%)\n",
            "years_experience: 4 nulls (4.00%)\n",
            "performance_rating: 12 nulls (12.00%)\n",
            "department: 16 nulls (16.00%)\n",
            "education: 13 nulls (13.00%)\n",
            "==================================================\n",
            "   employee_id   age        salary  years_experience  performance_rating  \\\n",
            "0            1  38.0  38769.438869          9.431149            6.256507   \n",
            "1            2  33.0  53690.320159         10.243138            6.659728   \n",
            "2            3  40.0  54859.282252         12.332205            8.620940   \n",
            "3            4  47.0  47965.840962         12.215208            8.415555   \n",
            "4            5  33.0  57580.714325          2.489323            7.468648   \n",
            "\n",
            "  department    education  \n",
            "0        NaN     Bachelor  \n",
            "1        NaN     Bachelor  \n",
            "2        NaN  High School  \n",
            "3        NaN       Master  \n",
            "4      Sales       Master  \n",
            "==================================================\n",
            "    employee_id   age        salary  years_experience  performance_rating  \\\n",
            "95           96  23.0  65779.760696          5.228362                 NaN   \n",
            "96           97  37.0  46742.138457         11.598400            4.930298   \n",
            "97           98  37.0  62305.876589          9.229198            9.530809   \n",
            "98           99  35.0  60873.130777         11.251448            7.328190   \n",
            "99          100  33.0  42855.445533         10.518515                 NaN   \n",
            "\n",
            "   department    education  \n",
            "95         HR       Master  \n",
            "96  Marketing          PhD  \n",
            "97    Finance          NaN  \n",
            "98    Finance  High School  \n",
            "99      Sales     Bachelor  \n"
          ]
        }
      ],
      "source": [
        "# Data Inspection\n",
        "\n",
        "rows = df_copy.shape[0]\n",
        "cols = df_copy.shape[1]\n",
        "memory = df_copy.memory_usage(deep=True).sum() / 1024**2\n",
        "\n",
        "print(f\"Rows: {rows}\")\n",
        "print(f\"Columns: {cols}\")\n",
        "print(f\"Memory: {memory:.2f} MB\")\n",
        "\n",
        "print('='*50)\n",
        "\n",
        "print(df_copy.info())\n",
        "\n",
        "print('='*50)\n",
        "\n",
        "print(df_copy.describe())\n",
        "\n",
        "print('='*50)\n",
        "\n",
        "# Column null counts and percentage\n",
        "for col in df_copy.columns:\n",
        "    null_count = df_copy[col].isnull().sum()\n",
        "    null_percentage = null_count / len(df_copy)\n",
        "    print(f\"{col}: {null_count} nulls ({null_percentage:.2%})\")\n",
        "\n",
        "print('='*50)\n",
        "\n",
        "# Display first 5 rows of the dataframe\n",
        "print(df_copy.head())\n",
        "\n",
        "print('='*50)\n",
        "\n",
        "# Display last 5 rows of the dataframe\n",
        "print(df_copy.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5292/3963086173.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_copy['age'].fillna(df_copy['age'].median(), inplace=True)\n",
            "/tmp/ipykernel_5292/3963086173.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_copy['salary'].fillna(df_copy['salary'].median(), inplace=True)\n",
            "/tmp/ipykernel_5292/3963086173.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_copy['years_experience'].fillna(df_copy['years_experience'].median(), inplace=True)\n",
            "/tmp/ipykernel_5292/3963086173.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_copy['performance_rating'].fillna(df_copy['performance_rating'].mean(), inplace=True)\n",
            "/tmp/ipykernel_5292/3963086173.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_copy['department'].fillna(df_copy['department'].mode()[0], inplace=True)\n",
            "/tmp/ipykernel_5292/3963086173.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_copy['education'].fillna(df_copy['education'].mode()[0], inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Handling Missing Values\n",
        "\n",
        "df_copy = df_copy.copy()\n",
        "\n",
        "df_copy.replace('', np.nan)\n",
        "\n",
        "#fill missing age values with median\n",
        "\n",
        "df_copy['age'].fillna(df_copy['age'].median(), inplace=True)\n",
        "\n",
        "#fill missing salary values with median\n",
        "\n",
        "df_copy['salary'].fillna(df_copy['salary'].median(), inplace=True)\n",
        "\n",
        "#fill missing years_experience values with median\n",
        "\n",
        "df_copy['years_experience'].fillna(df_copy['years_experience'].median(), inplace=True)\n",
        "\n",
        "#fill missing performance_rating values with mean\n",
        "\n",
        "df_copy['performance_rating'].fillna(df_copy['performance_rating'].mean(), inplace=True)\n",
        "\n",
        "#fill missing department values with mode\n",
        "\n",
        "df_copy['department'].fillna(df_copy['department'].mode()[0], inplace=True)\n",
        "\n",
        "#fill missing education values with mode\n",
        "\n",
        "df_copy['education'].fillna(df_copy['education'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 100\n",
            "Columns: 7\n",
            "Memory: 0.01 MB\n",
            "==================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 7 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   employee_id         100 non-null    int64  \n",
            " 1   age                 100 non-null    float64\n",
            " 2   salary              100 non-null    float64\n",
            " 3   years_experience    100 non-null    float64\n",
            " 4   performance_rating  100 non-null    float64\n",
            " 5   department          100 non-null    object \n",
            " 6   education           100 non-null    object \n",
            "dtypes: float64(4), int64(1), object(2)\n",
            "memory usage: 5.6+ KB\n",
            "None\n",
            "==================================================\n",
            "       employee_id         age         salary  years_experience  \\\n",
            "count   100.000000  100.000000     100.000000        100.000000   \n",
            "mean     50.500000   33.930000   60231.725465          8.259248   \n",
            "std      29.011492    6.440489   14162.069063          4.165204   \n",
            "min       1.000000   22.000000   31218.431771          0.000000   \n",
            "25%      25.750000   30.000000   48179.860702          5.378226   \n",
            "50%      50.500000   33.000000   60965.948886          8.390783   \n",
            "75%      75.250000   37.000000   67139.891917         10.665990   \n",
            "max     100.000000   49.000000  100802.537499         23.410926   \n",
            "\n",
            "       performance_rating  \n",
            "count          100.000000  \n",
            "mean             7.618948  \n",
            "std              1.174504  \n",
            "min              4.314156  \n",
            "25%              6.732531  \n",
            "50%              7.618948  \n",
            "75%              8.398441  \n",
            "max             10.000000  \n",
            "==================================================\n",
            "employee_id: 0 nulls (0.00%)\n",
            "age: 0 nulls (0.00%)\n",
            "salary: 0 nulls (0.00%)\n",
            "years_experience: 0 nulls (0.00%)\n",
            "performance_rating: 0 nulls (0.00%)\n",
            "department: 0 nulls (0.00%)\n",
            "education: 0 nulls (0.00%)\n",
            "==================================================\n",
            "   employee_id   age        salary  years_experience  performance_rating  \\\n",
            "0            1  38.0  38769.438869          9.431149            6.256507   \n",
            "1            2  33.0  53690.320159         10.243138            6.659728   \n",
            "2            3  40.0  54859.282252         12.332205            8.620940   \n",
            "3            4  47.0  47965.840962         12.215208            8.415555   \n",
            "4            5  33.0  57580.714325          2.489323            7.468648   \n",
            "\n",
            "  department    education  \n",
            "0    Finance     Bachelor  \n",
            "1    Finance     Bachelor  \n",
            "2    Finance  High School  \n",
            "3    Finance       Master  \n",
            "4      Sales       Master  \n",
            "==================================================\n",
            "    employee_id   age        salary  years_experience  performance_rating  \\\n",
            "95           96  23.0  65779.760696          5.228362            7.618948   \n",
            "96           97  37.0  46742.138457         11.598400            4.930298   \n",
            "97           98  37.0  62305.876589          9.229198            9.530809   \n",
            "98           99  35.0  60873.130777         11.251448            7.328190   \n",
            "99          100  33.0  42855.445533         10.518515            7.618948   \n",
            "\n",
            "   department    education  \n",
            "95         HR       Master  \n",
            "96  Marketing          PhD  \n",
            "97    Finance          PhD  \n",
            "98    Finance  High School  \n",
            "99      Sales     Bachelor  \n"
          ]
        }
      ],
      "source": [
        "# df_copy Data Inspection\n",
        "\n",
        "rows = df_copy.shape[0]\n",
        "cols = df_copy.shape[1]\n",
        "memory = df_copy.memory_usage(deep=True).sum() / 1024**2\n",
        "\n",
        "print(f\"Rows: {rows}\")\n",
        "print(f\"Columns: {cols}\")\n",
        "print(f\"Memory: {memory:.2f} MB\")\n",
        "\n",
        "print('='*50)\n",
        "\n",
        "print(df_copy.info())\n",
        "\n",
        "print('='*50)\n",
        "\n",
        "print(df_copy.describe())\n",
        "\n",
        "print('='*50)\n",
        "\n",
        "# Column null counts and percentage\n",
        "for col in df_copy.columns:\n",
        "    null_count = df_copy[col].isnull().sum()\n",
        "    null_percentage = null_count / len(df_copy)\n",
        "    print(f\"{col}: {null_count} nulls ({null_percentage:.2%})\")\n",
        "\n",
        "print('='*50)\n",
        "\n",
        "# Display first 5 rows of the dataframe\n",
        "print(df_copy.head())\n",
        "\n",
        "print('='*50)\n",
        "\n",
        "# Display last 5 rows of the dataframe\n",
        "print(df_copy.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 4. Data Type Transformations üîÑ\n",
        "\n",
        "## Converting and Standardizing Formats\n",
        "\n",
        "Data often comes in the wrong format for analysis. Let's explore how to convert between data types and standardize formats across your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç CURRENT DATA TYPES\n",
            "==============================\n",
            "customer_id            int64\n",
            "name                  object\n",
            "email                 object\n",
            "age                  float64\n",
            "salary               float64\n",
            "department            object\n",
            "join_date             object\n",
            "performance_score    float64\n",
            "dtype: object\n",
            "\n",
            "1Ô∏è‚É£ NUMERICAL DATA TYPE CONVERSIONS\n",
            "----------------------------------------\n",
            "Converting customer_id to integer:\n",
            "Before: int64\n",
            "After: int32\n",
            "\n",
            "Handling mixed types in age column:\n",
            "Original age values and types:\n",
            "  Row 0: 25.0 (type: <class 'float'>)\n",
            "  Row 1: 30.0 (type: <class 'float'>)\n",
            "  Row 2: 28.5 (type: <class 'float'>)\n",
            "  Row 3: 22.0 (type: <class 'float'>)\n",
            "  Row 4: 25.0 (type: <class 'float'>)\n",
            "After conversion: float64\n",
            "\n",
            "Memory optimization:\n",
            "Original age dtype: float64\n",
            "Optimized to: uint8\n",
            "Original salary dtype: float64\n",
            "Optimized to: int32\n",
            "\n",
            "Memory usage improvement:\n",
            "Before optimization: 4.00 KB\n",
            "After optimization: 3.86 KB\n"
          ]
        }
      ],
      "source": [
        "# Working with our imputed dataset for data type transformations\n",
        "df_types = df_imputed.copy()\n",
        "\n",
        "print(\"üîç CURRENT DATA TYPES\")\n",
        "print(\"=\" * 30)\n",
        "print(df_types.dtypes)\n",
        "print()\n",
        "\n",
        "# 1. NUMERICAL DATA TYPE CONVERSIONS\n",
        "print(\"1Ô∏è‚É£ NUMERICAL DATA TYPE CONVERSIONS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Convert float to int (when appropriate)\n",
        "print(\"Converting customer_id to integer:\")\n",
        "print(f\"Before: {df_types['customer_id'].dtype}\")\n",
        "df_types['customer_id'] = df_types['customer_id'].astype('int32')\n",
        "print(f\"After: {df_types['customer_id'].dtype}\")\n",
        "\n",
        "# Handle mixed types in numerical columns\n",
        "print(\"\\nHandling mixed types in age column:\")\n",
        "print(\"Original age values and types:\")\n",
        "for i, val in enumerate(df_types['age'].head()):\n",
        "    print(f\"  Row {i}: {val} (type: {type(val)})\")\n",
        "\n",
        "# Convert to numeric, handling errors\n",
        "df_types['age'] = pd.to_numeric(df_types['age'], errors='coerce')\n",
        "print(f\"After conversion: {df_types['age'].dtype}\")\n",
        "\n",
        "# Memory optimization for integers\n",
        "print(\"\\nMemory optimization:\")\n",
        "print(f\"Original age dtype: {df_types['age'].dtype}\")\n",
        "# Check if values fit in smaller integer types\n",
        "if df_types['age'].min() >= 0 and df_types['age'].max() <= 255:\n",
        "    df_types['age'] = df_types['age'].astype('uint8')\n",
        "    print(f\"Optimized to: {df_types['age'].dtype}\")\n",
        "\n",
        "print(f\"Original salary dtype: {df_types['salary'].dtype}\")\n",
        "if df_types['salary'].min() >= -2147483648 and df_types['salary'].max() <= 2147483647:\n",
        "    df_types['salary'] = df_types['salary'].astype('int32')\n",
        "    print(f\"Optimized to: {df_types['salary'].dtype}\")\n",
        "\n",
        "# Memory usage comparison\n",
        "print(f\"\\nMemory usage improvement:\")\n",
        "print(f\"Before optimization: {df_messy.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "print(f\"After optimization: {df_types.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2Ô∏è‚É£ CATEGORICAL DATA HANDLING\n",
            "-----------------------------------\n",
            "Converting department to categorical:\n",
            "Unique departments: ['Sales' 'marketing' 'ENGINEERING' 'Engineering' 'Marketing' 'sales' 'HR']\n",
            "Before: object, Memory: 976 bytes\n",
            "After: category, Memory: 842 bytes\n",
            "\n",
            "Standardizing department names:\n",
            "Before standardization:\n",
            "department\n",
            "Sales          4\n",
            "Engineering    3\n",
            "HR             2\n",
            "marketing      2\n",
            "Marketing      2\n",
            "ENGINEERING    1\n",
            "sales          1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After standardization:\n",
            "department\n",
            "Sales          5\n",
            "Marketing      4\n",
            "Engineering    4\n",
            "Hr             2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üìä Creating ordered categorical for performance ratings:\n",
            "Performance categories:\n",
            "performance_category\n",
            "Poor         0\n",
            "Fair         2\n",
            "Good         8\n",
            "Excellent    5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ü§ñ Label Encoding (for ML algorithms):\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'LabelEncoder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Label Encoding for machine learning\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mü§ñ Label Encoding (for ML algorithms):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m label_encoder = \u001b[43mLabelEncoder\u001b[49m()\n\u001b[32m     36\u001b[39m df_types[\u001b[33m'\u001b[39m\u001b[33mdepartment_encoded\u001b[39m\u001b[33m'\u001b[39m] = label_encoder.fit_transform(df_types[\u001b[33m'\u001b[39m\u001b[33mdepartment\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     38\u001b[39m encoding_map = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
            "\u001b[31mNameError\u001b[39m: name 'LabelEncoder' is not defined"
          ]
        }
      ],
      "source": [
        "# 2. CATEGORICAL DATA HANDLING\n",
        "print(\"2Ô∏è‚É£ CATEGORICAL DATA HANDLING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Convert to categorical data type for memory efficiency\n",
        "print(\"Converting department to categorical:\")\n",
        "print(f\"Unique departments: {df_types['department'].unique()}\")\n",
        "print(f\"Before: {df_types['department'].dtype}, Memory: {df_types['department'].memory_usage(deep=True)} bytes\")\n",
        "\n",
        "df_types['department'] = df_types['department'].astype('category')\n",
        "print(f\"After: {df_types['department'].dtype}, Memory: {df_types['department'].memory_usage(deep=True)} bytes\")\n",
        "\n",
        "# Standardize categorical values (case sensitivity)\n",
        "print(\"\\nStandardizing department names:\")\n",
        "print(\"Before standardization:\")\n",
        "print(df_types['department'].value_counts())\n",
        "\n",
        "# Convert to lowercase and then to proper case\n",
        "df_types['department'] = df_types['department'].str.lower().str.title()\n",
        "print(\"\\nAfter standardization:\")\n",
        "print(df_types['department'].value_counts())\n",
        "\n",
        "# Create ordered categorical (for ordinal data)\n",
        "print(\"\\nüìä Creating ordered categorical for performance ratings:\")\n",
        "performance_ranges = pd.cut(df_types['performance_score'], \n",
        "                           bins=[0, 70, 80, 90, 100], \n",
        "                           labels=['Poor', 'Fair', 'Good', 'Excellent'],\n",
        "                           ordered=True)\n",
        "df_types['performance_category'] = performance_ranges\n",
        "print(\"Performance categories:\")\n",
        "print(df_types['performance_category'].value_counts().sort_index())\n",
        "\n",
        "# Label Encoding for machine learning\n",
        "print(\"\\nü§ñ Label Encoding (for ML algorithms):\")\n",
        "label_encoder = LabelEncoder()\n",
        "df_types['department_encoded'] = label_encoder.fit_transform(df_types['department'])\n",
        "\n",
        "encoding_map = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Label encoding mapping:\")\n",
        "for original, encoded in encoding_map.items():\n",
        "    print(f\"  {original} ‚Üí {encoded}\")\n",
        "\n",
        "# One-Hot Encoding\n",
        "print(\"\\nüî• One-Hot Encoding:\")\n",
        "dept_dummies = pd.get_dummies(df_types['department'], prefix='dept')\n",
        "print(\"One-hot encoded columns:\")\n",
        "print(dept_dummies.head())\n",
        "\n",
        "# Add one-hot encoded columns to main DataFrame\n",
        "df_types = pd.concat([df_types, dept_dummies], axis=1)\n",
        "print(f\"\\nDataFrame shape after one-hot encoding: {df_types.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3Ô∏è‚É£ DATE AND TIME HANDLING\n",
            "-----------------------------------\n",
            "Original join_date column:\n",
            "0      2020-01-15\n",
            "1      2021/03/10\n",
            "2      2019-12-01\n",
            "3      2022-06-15\n",
            "4      2020-01-15\n",
            "5      2021-08-20\n",
            "6      2020-11-05\n",
            "7    invalid_date\n",
            "8      2021-05-10\n",
            "9      2021/03/10\n",
            "Name: join_date, dtype: object\n",
            "Data type: object\n",
            "\n",
            "Converting to datetime:\n",
            "After parsing:\n",
            "      join_date join_date_parsed\n",
            "0    2020-01-15       2020-01-15\n",
            "1    2021/03/10       2021-03-10\n",
            "2    2019-12-01       2019-12-01\n",
            "3    2022-06-15       2022-06-15\n",
            "4    2020-01-15       2020-01-15\n",
            "5    2021-08-20       2021-08-20\n",
            "6    2020-11-05       2020-11-05\n",
            "7  invalid_date              NaT\n",
            "8    2021-05-10       2021-05-10\n",
            "9    2021/03/10       2021-03-10\n",
            "\n",
            "üìÖ Extracting date components:\n",
            "Date components extracted:\n",
            "  join_date_parsed  join_year  join_month  join_quarter join_day_of_week  \\\n",
            "0       2020-01-15     2020.0         1.0           1.0        Wednesday   \n",
            "1       2021-03-10     2021.0         3.0           1.0        Wednesday   \n",
            "2       2019-12-01     2019.0        12.0           4.0           Sunday   \n",
            "3       2022-06-15     2022.0         6.0           2.0        Wednesday   \n",
            "4       2020-01-15     2020.0         1.0           1.0        Wednesday   \n",
            "\n",
            "   days_since_joining  \n",
            "0              1975.0  \n",
            "1              1555.0  \n",
            "2              2020.0  \n",
            "3              1093.0  \n",
            "4              1975.0  \n",
            "\n",
            "‚è∞ Creating time-based features:\n",
            "Time-based features:\n",
            "  join_date_parsed  is_recent_hire tenure_category\n",
            "0       2020-01-15           False         Veteran\n",
            "1       2021-03-10           False         Veteran\n",
            "2       2019-12-01           False         Veteran\n",
            "3       2022-06-15           False         Veteran\n",
            "4       2020-01-15           False         Veteran\n"
          ]
        }
      ],
      "source": [
        "# 3. DATE AND TIME HANDLING\n",
        "print(\"3Ô∏è‚É£ DATE AND TIME HANDLING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"Original join_date column:\")\n",
        "print(df_types['join_date'].head(10))\n",
        "print(f\"Data type: {df_types['join_date'].dtype}\")\n",
        "\n",
        "# Convert string dates to datetime\n",
        "print(\"\\nConverting to datetime:\")\n",
        "# Handle multiple date formats\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse dates in multiple formats\"\"\"\n",
        "    if pd.isna(date_str) or date_str == 'invalid_date':\n",
        "        return pd.NaT\n",
        "    \n",
        "    # Try different formats\n",
        "    formats = ['%Y-%m-%d', '%Y/%m/%d', '%d/%m/%Y', '%m/%d/%Y']\n",
        "    \n",
        "    for fmt in formats:\n",
        "        try:\n",
        "            return pd.to_datetime(date_str, format=fmt)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    \n",
        "    # If all formats fail, try pandas' automatic parsing\n",
        "    try:\n",
        "        return pd.to_datetime(date_str)\n",
        "    except:\n",
        "        return pd.NaT\n",
        "\n",
        "# Apply date parsing\n",
        "df_types['join_date_parsed'] = df_types['join_date'].apply(parse_date)\n",
        "print(\"After parsing:\")\n",
        "print(df_types[['join_date', 'join_date_parsed']].head(10))\n",
        "\n",
        "# Extract useful date components\n",
        "print(\"\\nüìÖ Extracting date components:\")\n",
        "df_types['join_year'] = df_types['join_date_parsed'].dt.year\n",
        "df_types['join_month'] = df_types['join_date_parsed'].dt.month\n",
        "df_types['join_quarter'] = df_types['join_date_parsed'].dt.quarter\n",
        "df_types['join_day_of_week'] = df_types['join_date_parsed'].dt.day_name()\n",
        "df_types['days_since_joining'] = (pd.Timestamp.now() - df_types['join_date_parsed']).dt.days\n",
        "\n",
        "print(\"Date components extracted:\")\n",
        "date_components = df_types[['join_date_parsed', 'join_year', 'join_month', \n",
        "                           'join_quarter', 'join_day_of_week', 'days_since_joining']].head()\n",
        "print(date_components)\n",
        "\n",
        "# Create time-based features\n",
        "print(\"\\n‚è∞ Creating time-based features:\")\n",
        "df_types['is_recent_hire'] = df_types['days_since_joining'] < 365  # Joined within last year\n",
        "df_types['tenure_category'] = pd.cut(df_types['days_since_joining'], \n",
        "                                    bins=[0, 365, 730, float('inf')], \n",
        "                                    labels=['New', 'Experienced', 'Veteran'])\n",
        "\n",
        "print(\"Time-based features:\")\n",
        "print(df_types[['join_date_parsed', 'is_recent_hire', 'tenure_category']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 5. Duplicate Detection & Removal üîç\n",
        "\n",
        "## Ensuring Data Uniqueness\n",
        "\n",
        "Duplicate records can skew analysis results and waste computational resources. Let's learn how to identify and handle different types of duplicates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç DUPLICATE DETECTION ANALYSIS\n",
            "========================================\n",
            "1Ô∏è‚É£ EXACT DUPLICATES\n",
            "--------------------\n",
            "Number of exact duplicate rows: 0\n",
            "\n",
            "2Ô∏è‚É£ DUPLICATES BASED ON KEY COLUMNS\n",
            "----------------------------------------\n",
            "Rows with duplicate name/email combinations: 2\n",
            "Rows with duplicate key combinations:\n",
            "   customer_id        name           email  age  salary\n",
            "1            2  jane smith  Jane@Email.Com   30   60000\n",
            "9           10  jane smith  Jane@Email.Com   30   60000\n",
            "\n",
            "3Ô∏è‚É£ FUZZY DUPLICATES (SIMILAR RECORDS)\n",
            "---------------------------------------------\n",
            "Fuzzy duplicate names found:\n",
            "4Ô∏è‚É£ DUPLICATE REMOVAL STRATEGIES\n",
            "-----------------------------------\n",
            "Original dataset shape: (15, 17)\n",
            "After removing duplicates (keep first): (14, 17)\n",
            "After removing duplicates (keep last): (14, 17)\n",
            "After keeping most complete records: (14, 17)\n",
            "\n",
            "üìä AGGREGATING DUPLICATE INFORMATION\n",
            "----------------------------------------\n",
            "After aggregating duplicates: (14, 8)\n",
            "Sample aggregated records:\n",
            "   customer_id           name            email   department    age   salary  \\\n",
            "0            8  Alice Johnson  alice@email.com        Sales  150.0  90000.0   \n",
            "1           15     Anna Green   Jane@Email.Com           Hr   26.0  54000.0   \n",
            "2            6      Bob Brown    bob@email.com  Engineering   35.0  80000.0   \n",
            "3           14    Chris Black  chris@email.com    Marketing   31.0  67000.0   \n",
            "4           12      David Lee  david@email.com  Engineering   27.0  58000.0   \n",
            "\n",
            "   performance_score join_date_parsed  \n",
            "0               72.0              NaT  \n",
            "1               86.0       2020-07-08  \n",
            "2               95.0       2021-08-20  \n",
            "3               90.5       2022-04-18  \n",
            "4               87.5       2020-09-12  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5292/1125613137.py:99: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_most_complete = df_dupes.groupby(key_columns, as_index=False).apply(keep_most_complete).reset_index(drop=True)\n",
            "/tmp/ipykernel_5292/1125613137.py:132: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_aggregated = df_dupes.groupby(key_columns).apply(aggregate_duplicates).reset_index(drop=True)\n"
          ]
        }
      ],
      "source": [
        "# Start with our cleaned dataset for duplicate analysis\n",
        "df_dupes = df_types.copy()\n",
        "\n",
        "print(\"üîç DUPLICATE DETECTION ANALYSIS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# 1. Exact Duplicates\n",
        "print(\"1Ô∏è‚É£ EXACT DUPLICATES\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Check for completely identical rows\n",
        "exact_duplicates = df_dupes.duplicated()\n",
        "print(f\"Number of exact duplicate rows: {exact_duplicates.sum()}\")\n",
        "\n",
        "if exact_duplicates.sum() > 0:\n",
        "    print(\"Duplicate rows:\")\n",
        "    print(df_dupes[exact_duplicates])\n",
        "    \n",
        "    # Show the original rows that have duplicates\n",
        "    print(\"\\nOriginal rows with duplicates:\")\n",
        "    duplicate_indices = df_dupes[exact_duplicates].index\n",
        "    original_indices = []\n",
        "    for idx in duplicate_indices:\n",
        "        # Find the first occurrence of this duplicate\n",
        "        mask = df_dupes.eq(df_dupes.iloc[idx]).all(axis=1)\n",
        "        first_occurrence = df_dupes[mask].index[0]\n",
        "        if first_occurrence not in original_indices:\n",
        "            original_indices.append(first_occurrence)\n",
        "    \n",
        "    print(df_dupes.loc[original_indices])\n",
        "\n",
        "# 2. Duplicates based on specific columns\n",
        "print(\"\\n2Ô∏è‚É£ DUPLICATES BASED ON KEY COLUMNS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Check for duplicates based on business logic (e.g., same person)\n",
        "key_columns = ['name', 'email']\n",
        "key_duplicates = df_dupes.duplicated(subset=key_columns, keep=False)\n",
        "print(f\"Rows with duplicate name/email combinations: {key_duplicates.sum()}\")\n",
        "\n",
        "if key_duplicates.sum() > 0:\n",
        "    print(\"Rows with duplicate key combinations:\")\n",
        "    duplicate_rows = df_dupes[key_duplicates].sort_values(key_columns)\n",
        "    print(duplicate_rows[['customer_id', 'name', 'email', 'age', 'salary']])\n",
        "\n",
        "# 3. Fuzzy Duplicates (similar but not identical)\n",
        "print(\"\\n3Ô∏è‚É£ FUZZY DUPLICATES (SIMILAR RECORDS)\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "def find_fuzzy_duplicates(df, column, threshold=0.8):\n",
        "    \"\"\"Find fuzzy duplicates using string similarity\"\"\"\n",
        "    from difflib import SequenceMatcher\n",
        "    \n",
        "    duplicates = []\n",
        "    values = df[column].dropna().str.lower().unique()\n",
        "    \n",
        "    for i, val1 in enumerate(values):\n",
        "        for val2 in values[i+1:]:\n",
        "            similarity = SequenceMatcher(None, val1, val2).ratio()\n",
        "            if similarity >= threshold:\n",
        "                duplicates.append((val1, val2, similarity))\n",
        "    \n",
        "    return duplicates\n",
        "\n",
        "# Find fuzzy duplicates in names\n",
        "name_fuzzy_dupes = find_fuzzy_duplicates(df_dupes, 'name', threshold=0.7)\n",
        "print(\"Fuzzy duplicate names found:\")\n",
        "for name1, name2, similarity in name_fuzzy_dupes:\n",
        "    print(f\"  '{name1}' ‚Üî '{name2}' (similarity: {similarity:.2f})\")\n",
        "    \n",
        "    # Show the actual records\n",
        "    mask1 = df_dupes['name'].str.lower() == name1\n",
        "    mask2 = df_dupes['name'].str.lower() == name2\n",
        "    similar_records = df_dupes[mask1 | mask2][['customer_id', 'name', 'email']]\n",
        "    print(similar_records)\n",
        "    print()\n",
        "\n",
        "# 4. Remove duplicates with different strategies\n",
        "print(\"4Ô∏è‚É£ DUPLICATE REMOVAL STRATEGIES\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"Original dataset shape:\", df_dupes.shape)\n",
        "\n",
        "# Strategy 1: Keep first occurrence\n",
        "df_keep_first = df_dupes.drop_duplicates(subset=key_columns, keep='first')\n",
        "print(f\"After removing duplicates (keep first): {df_keep_first.shape}\")\n",
        "\n",
        "# Strategy 2: Keep last occurrence\n",
        "df_keep_last = df_dupes.drop_duplicates(subset=key_columns, keep='last')\n",
        "print(f\"After removing duplicates (keep last): {df_keep_last.shape}\")\n",
        "\n",
        "# Strategy 3: Keep the record with more complete information\n",
        "def keep_most_complete(group):\n",
        "    \"\"\"Keep the record with the least missing values\"\"\"\n",
        "    missing_counts = group.isnull().sum(axis=1)\n",
        "    return group.loc[missing_counts.idxmin()]\n",
        "\n",
        "# Group by key columns and keep most complete record\n",
        "df_most_complete = df_dupes.groupby(key_columns, as_index=False).apply(keep_most_complete).reset_index(drop=True)\n",
        "print(f\"After keeping most complete records: {df_most_complete.shape}\")\n",
        "\n",
        "# Strategy 4: Aggregate information from duplicates\n",
        "print(\"\\nüìä AGGREGATING DUPLICATE INFORMATION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# For numerical columns, we can aggregate (mean, max, etc.)\n",
        "# For categorical columns, we can take the mode or concatenate unique values\n",
        "def aggregate_duplicates(group):\n",
        "    \"\"\"Aggregate information from duplicate records\"\"\"\n",
        "    result = {}\n",
        "    \n",
        "    # Keep the first customer_id\n",
        "    result['customer_id'] = group['customer_id'].iloc[0]\n",
        "    \n",
        "    # For categorical data, take the first non-null value\n",
        "    result['name'] = group['name'].iloc[0]\n",
        "    result['email'] = group['email'].iloc[0]\n",
        "    result['department'] = group['department'].iloc[0]\n",
        "    \n",
        "    # For numerical data, take the mean\n",
        "    result['age'] = group['age'].mean()\n",
        "    result['salary'] = group['salary'].mean()\n",
        "    result['performance_score'] = group['performance_score'].mean()\n",
        "    \n",
        "    # For dates, take the earliest\n",
        "    result['join_date_parsed'] = group['join_date_parsed'].min()\n",
        "    \n",
        "    return pd.Series(result)\n",
        "\n",
        "# Apply aggregation to duplicates\n",
        "if key_duplicates.sum() > 0:\n",
        "    df_aggregated = df_dupes.groupby(key_columns).apply(aggregate_duplicates).reset_index(drop=True)\n",
        "    print(f\"After aggregating duplicates: {df_aggregated.shape}\")\n",
        "    print(\"Sample aggregated records:\")\n",
        "    print(df_aggregated.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 6. Text Data Cleaning üìù\n",
        "\n",
        "## Working with Strings and Text\n",
        "\n",
        "Text data often requires extensive cleaning and standardization. Let's explore techniques for handling messy text data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ TEXT DATA CLEANING TECHNIQUES\n",
            "=============================================\n",
            "1Ô∏è‚É£ BASIC STRING OPERATIONS\n",
            "------------------------------\n",
            "Original name column:\n",
            "['John Doe', 'jane smith', 'MIKE JONES', 'Sarah Wilson', 'john doe', 'Bob Brown', 'jane smith', 'Alice Johnson', 'Tom Davis', 'Lisa Miller', 'David Lee', 'Emma White', 'Chris Black', 'Anna Green']\n",
            "\n",
            "After title case standardization:\n",
            "['John Doe', 'Jane Smith', 'Mike Jones', 'Sarah Wilson', 'John Doe', 'Bob Brown', 'Jane Smith', 'Alice Johnson', 'Tom Davis', 'Lisa Miller', 'David Lee', 'Emma White', 'Chris Black', 'Anna Green']\n",
            "\n",
            "After whitespace cleaning:\n",
            "['John Doe', 'Jane Smith', 'Mike Jones', 'Sarah Wilson', 'John Doe', 'Bob Brown', 'Jane Smith', 'Alice Johnson', 'Tom Davis', 'Lisa Miller', 'David Lee', 'Emma White', 'Chris Black', 'Anna Green']\n",
            "\n",
            "2Ô∏è‚É£ EMAIL VALIDATION AND CLEANING\n",
            "-----------------------------------\n",
            "Original email column:\n",
            "['john@email.com', 'Jane@Email.Com', 'mike@email.com', 'sarah@invalid', 'john@email.com', 'bob@email.com', 'missing@email.com', 'alice@email.com', 'tom@email.com', 'Jane@Email.Com', 'david@email.com', 'emma@email.com', 'chris@email.com', 'Jane@Email.Com']\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 're' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(re.match(email_pattern, email))\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Apply email validation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m df_text[\u001b[33m'\u001b[39m\u001b[33memail_valid\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_text\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43memail\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidate_email\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEmail validation results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m validation_results = df_text[[\u001b[33m'\u001b[39m\u001b[33memail\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33memail_valid\u001b[39m\u001b[33m'\u001b[39m]]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python-practice-workspace/.venv/lib/python3.12/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python-practice-workspace/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python-practice-workspace/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python-practice-workspace/.venv/lib/python3.12/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Python-practice-workspace/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mvalidate_email\u001b[39m\u001b[34m(email)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.isna(email):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mre\u001b[49m.match(email_pattern, email))\n",
            "\u001b[31mNameError\u001b[39m: name 're' is not defined"
          ]
        }
      ],
      "source": [
        "# Text cleaning examples using our dataset\n",
        "df_text = df_keep_first.copy()  # Use deduplicated dataset\n",
        "\n",
        "print(\"üßπ TEXT DATA CLEANING TECHNIQUES\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# 1. BASIC STRING OPERATIONS\n",
        "print(\"1Ô∏è‚É£ BASIC STRING OPERATIONS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"Original name column:\")\n",
        "print(df_text['name'].tolist())\n",
        "\n",
        "# Case standardization\n",
        "df_text['name_clean'] = df_text['name'].str.title()\n",
        "print(\"\\nAfter title case standardization:\")\n",
        "print(df_text['name_clean'].tolist())\n",
        "\n",
        "# Remove extra whitespace\n",
        "df_text['name_clean'] = df_text['name_clean'].str.strip()\n",
        "\n",
        "# Handle multiple spaces\n",
        "df_text['name_clean'] = df_text['name_clean'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "print(\"\\nAfter whitespace cleaning:\")\n",
        "print(df_text['name_clean'].tolist())\n",
        "\n",
        "# 2. EMAIL VALIDATION AND CLEANING\n",
        "print(\"\\n2Ô∏è‚É£ EMAIL VALIDATION AND CLEANING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "print(\"Original email column:\")\n",
        "print(df_text['email'].tolist())\n",
        "\n",
        "# Basic email validation using regex\n",
        "email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
        "\n",
        "def validate_email(email):\n",
        "    \"\"\"Validate email format\"\"\"\n",
        "    if pd.isna(email):\n",
        "        return False\n",
        "    return bool(re.match(email_pattern, email))\n",
        "\n",
        "# Apply email validation\n",
        "df_text['email_valid'] = df_text['email'].apply(validate_email)\n",
        "print(\"\\nEmail validation results:\")\n",
        "validation_results = df_text[['email', 'email_valid']]\n",
        "print(validation_results)\n",
        "\n",
        "# Clean email addresses\n",
        "def clean_email(email):\n",
        "    \"\"\"Clean and standardize email addresses\"\"\"\n",
        "    if pd.isna(email):\n",
        "        return email\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    email = email.lower().strip()\n",
        "    \n",
        "    # Remove extra spaces\n",
        "    email = re.sub(r'\\s+', '', email)\n",
        "    \n",
        "    # Basic validation\n",
        "    if validate_email(email):\n",
        "        return email\n",
        "    else:\n",
        "        return None  # Mark invalid emails as None\n",
        "\n",
        "df_text['email_clean'] = df_text['email'].apply(clean_email)\n",
        "print(\"\\nCleaned emails:\")\n",
        "print(df_text[['email', 'email_clean']].head(10))\n",
        "\n",
        "# 3. ADVANCED TEXT PROCESSING\n",
        "print(\"\\n3Ô∏è‚É£ ADVANCED TEXT PROCESSING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Example with a more complex text column\n",
        "text_examples = [\n",
        "    \"John   DOE\",\n",
        "    \"mary-jane SMITH\",\n",
        "    \"BOB   o'connor\",\n",
        "    \"Ana   Mar√≠a   Garc√≠a\",\n",
        "    \"ÊùéÂ∞èÊòé\",\n",
        "    \"ŸÖÿ≠ŸÖÿØ ÿßŸÑÿ£ÿ≠ŸÖÿØ\",\n",
        "    \"   Dr. Sarah  Wilson   PhD  \",\n",
        "    \"MR. james   brown, jr.\",\n",
        "    \"Ms   Linda   Davis-Johnson\"\n",
        "]\n",
        "\n",
        "df_text_sample = pd.DataFrame({'raw_names': text_examples})\n",
        "print(\"Complex text examples:\")\n",
        "print(df_text_sample['raw_names'].tolist())\n",
        "\n",
        "def advanced_name_cleaning(name):\n",
        "    \"\"\"Advanced name cleaning and standardization\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return name\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    name = re.sub(r'\\s+', ' ', name.strip())\n",
        "    \n",
        "    # Handle common prefixes and suffixes\n",
        "    prefixes = ['mr', 'mrs', 'ms', 'dr', 'prof']\n",
        "    suffixes = ['jr', 'sr', 'phd', 'md', 'esq']\n",
        "    \n",
        "    # Split into parts\n",
        "    parts = name.lower().split()\n",
        "    \n",
        "    # Remove prefixes\n",
        "    while parts and parts[0].replace('.', '') in prefixes:\n",
        "        parts.pop(0)\n",
        "    \n",
        "    # Remove suffixes\n",
        "    while parts and parts[-1].replace('.', '').replace(',', '') in suffixes:\n",
        "        parts.pop()\n",
        "    \n",
        "    # Rejoin and apply title case\n",
        "    cleaned_name = ' '.join(parts).title()\n",
        "    \n",
        "    # Handle special cases (hyphens, apostrophes)\n",
        "    cleaned_name = re.sub(r\"(\\w)'(\\w)\", r\"\\1'\\2\", cleaned_name)  # O'Connor\n",
        "    cleaned_name = re.sub(r\"(\\w)-(\\w)\", lambda m: f\"{m.group(1)}-{m.group(2).title()}\", cleaned_name)  # Davis-Johnson\n",
        "    \n",
        "    return cleaned_name\n",
        "\n",
        "df_text_sample['cleaned_names'] = df_text_sample['raw_names'].apply(advanced_name_cleaning)\n",
        "print(\"\\nAfter advanced cleaning:\")\n",
        "comparison = df_text_sample[['raw_names', 'cleaned_names']]\n",
        "print(comparison)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 7. Outlier Detection & Treatment ‚ö†Ô∏è\n",
        "\n",
        "## Identifying and Handling Anomalies\n",
        "\n",
        "Outliers can significantly impact analysis results. Let's explore different methods for detecting and treating outliers in numerical data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ OUTLIER DETECTION METHODS\n",
            "========================================\n",
            "1Ô∏è‚É£ STATISTICAL OUTLIER DETECTION\n",
            "-----------------------------------\n",
            "IQR Method Results:\n",
            "\n",
            "age:\n",
            "  Valid range: 17.00 to 39.00\n",
            "  Outliers found: 1\n",
            "  Outlier values: [150]\n",
            "\n",
            "salary:\n",
            "  Valid range: 31750.00 to 91750.00\n",
            "  Outliers found: 0\n",
            "\n",
            "performance_score:\n",
            "  Valid range: 78.56 to 97.06\n",
            "  Outliers found: 2\n",
            "  Outlier values: [78.5, 72.0]\n",
            "\n",
            "\n",
            "Z-Score Method Results (threshold=3):\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'stats' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mZ-Score Method Results (threshold=3):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m numerical_columns:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     outliers_z, z_scores = \u001b[43mdetect_outliers_zscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_outliers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Outliers found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(outliers_z)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mdetect_outliers_zscore\u001b[39m\u001b[34m(data, column, threshold)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetect_outliers_zscore\u001b[39m(data, column, threshold=\u001b[32m3\u001b[39m):\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Detect outliers using Z-score method\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     z_scores = np.abs(\u001b[43mstats\u001b[49m.zscore(data[column].dropna()))\n\u001b[32m     30\u001b[39m     outliers = data[z_scores > threshold]\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outliers, z_scores\n",
            "\u001b[31mNameError\u001b[39m: name 'stats' is not defined"
          ]
        }
      ],
      "source": [
        "# Outlier detection and treatment\n",
        "df_outliers = df_text.copy()\n",
        "\n",
        "print(\"üéØ OUTLIER DETECTION METHODS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Focus on numerical columns\n",
        "numerical_columns = ['age', 'salary', 'performance_score']\n",
        "\n",
        "# 1. STATISTICAL METHODS\n",
        "print(\"1Ô∏è‚É£ STATISTICAL OUTLIER DETECTION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detect outliers using Interquartile Range (IQR) method\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    \n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    \n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "def detect_outliers_zscore(data, column, threshold=3):\n",
        "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
        "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
        "    outliers = data[z_scores > threshold]\n",
        "    \n",
        "    return outliers, z_scores\n",
        "\n",
        "# Apply IQR method\n",
        "print(\"IQR Method Results:\")\n",
        "for col in numerical_columns:\n",
        "    outliers_iqr, lower, upper = detect_outliers_iqr(df_outliers, col)\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Valid range: {lower:.2f} to {upper:.2f}\")\n",
        "    print(f\"  Outliers found: {len(outliers_iqr)}\")\n",
        "    \n",
        "    if len(outliers_iqr) > 0:\n",
        "        print(f\"  Outlier values: {outliers_iqr[col].tolist()}\")\n",
        "\n",
        "# Apply Z-score method\n",
        "print(\"\\n\\nZ-Score Method Results (threshold=3):\")\n",
        "for col in numerical_columns:\n",
        "    outliers_z, z_scores = detect_outliers_zscore(df_outliers, col, threshold=3)\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Outliers found: {len(outliers_z)}\")\n",
        "    \n",
        "    if len(outliers_z) > 0:\n",
        "        print(f\"  Outlier values: {outliers_z[col].tolist()}\")\n",
        "\n",
        "# 2. VISUALIZATION OF OUTLIERS\n",
        "print(\"\\n2Ô∏è‚É£ OUTLIER VISUALIZATION\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Outlier Detection Visualization', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, col in enumerate(numerical_columns):\n",
        "    # Box plot\n",
        "    ax1 = axes[0, i]\n",
        "    sns.boxplot(data=df_outliers, y=col, ax=ax1)\n",
        "    ax1.set_title(f'{col} - Box Plot')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Histogram with normal distribution overlay\n",
        "    ax2 = axes[1, i]\n",
        "    ax2.hist(df_outliers[col].dropna(), bins=20, density=True, alpha=0.7, color='skyblue')\n",
        "    \n",
        "    # Overlay normal distribution\n",
        "    mu, sigma = df_outliers[col].mean(), df_outliers[col].std()\n",
        "    x = np.linspace(df_outliers[col].min(), df_outliers[col].max(), 100)\n",
        "    normal_dist = stats.norm.pdf(x, mu, sigma)\n",
        "    ax2.plot(x, normal_dist, 'r-', linewidth=2, label='Normal Distribution')\n",
        "    \n",
        "    ax2.set_title(f'{col} - Distribution')\n",
        "    ax2.set_xlabel(col)\n",
        "    ax2.set_ylabel('Density')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. OUTLIER TREATMENT STRATEGIES\n",
        "print(\"3Ô∏è‚É£ OUTLIER TREATMENT STRATEGIES\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Strategy 1: Remove outliers\n",
        "df_no_outliers = df_outliers.copy()\n",
        "print(f\"Original dataset shape: {df_no_outliers.shape}\")\n",
        "\n",
        "for col in numerical_columns:\n",
        "    outliers_iqr, lower, upper = detect_outliers_iqr(df_no_outliers, col)\n",
        "    # Remove outliers\n",
        "    df_no_outliers = df_no_outliers[(df_no_outliers[col] >= lower) & (df_no_outliers[col] <= upper)]\n",
        "\n",
        "print(f\"After removing outliers: {df_no_outliers.shape}\")\n",
        "\n",
        "# Strategy 2: Cap outliers (Winsorization)\n",
        "df_capped = df_outliers.copy()\n",
        "\n",
        "def cap_outliers(data, column, percentile_range=(5, 95)):\n",
        "    \"\"\"Cap outliers at specified percentiles\"\"\"\n",
        "    lower_percentile = np.percentile(data[column].dropna(), percentile_range[0])\n",
        "    upper_percentile = np.percentile(data[column].dropna(), percentile_range[1])\n",
        "    \n",
        "    data[column] = np.clip(data[column], lower_percentile, upper_percentile)\n",
        "    return data\n",
        "\n",
        "print(\"\\nWinsorization (capping at 5th and 95th percentiles):\")\n",
        "for col in numerical_columns:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Before: min={df_capped[col].min():.2f}, max={df_capped[col].max():.2f}\")\n",
        "    df_capped = cap_outliers(df_capped, col, percentile_range=(5, 95))\n",
        "    print(f\"  After:  min={df_capped[col].min():.2f}, max={df_capped[col].max():.2f}\")\n",
        "\n",
        "# Strategy 3: Transform outliers\n",
        "df_transformed = df_outliers.copy()\n",
        "\n",
        "print(\"\\nLog transformation (for right-skewed data):\")\n",
        "for col in ['salary']:  # Apply to salary which might be right-skewed\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Original range: {df_transformed[col].min():.2f} to {df_transformed[col].max():.2f}\")\n",
        "    print(f\"  Original skewness: {df_transformed[col].skew():.3f}\")\n",
        "    \n",
        "    # Apply log transformation\n",
        "    df_transformed[f'{col}_log'] = np.log1p(df_transformed[col])  # log1p handles values close to 0\n",
        "    print(f\"  Log-transformed skewness: {df_transformed[f'{col}_log'].skew():.3f}\")\n",
        "\n",
        "# Compare the effect of different treatments\n",
        "print(\"\\nüìä TREATMENT COMPARISON\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "treatments = {\n",
        "    'Original': df_outliers['age'].describe(),\n",
        "    'Outliers Removed': df_no_outliers['age'].describe(),\n",
        "    'Outliers Capped': df_capped['age'].describe()\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(treatments).round(2)\n",
        "print(\"Age column comparison across treatments:\")\n",
        "print(comparison_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 8. Data Transformation Techniques üîÑ\n",
        "\n",
        "## Scaling, Encoding, and Feature Engineering\n",
        "\n",
        "Transform your data to make it suitable for machine learning algorithms and statistical analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_capped' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Data transformation techniques\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_transform = \u001b[43mdf_capped\u001b[49m.copy()  \u001b[38;5;66;03m# Use the dataset with capped outliers\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîÑ DATA TRANSFORMATION TECHNIQUES\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m45\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'df_capped' is not defined"
          ]
        }
      ],
      "source": [
        "# Data transformation techniques\n",
        "df_transform = df_capped.copy()  # Use the dataset with capped outliers\n",
        "\n",
        "print(\"üîÑ DATA TRANSFORMATION TECHNIQUES\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# 1. SCALING AND NORMALIZATION\n",
        "print(\"1Ô∏è‚É£ SCALING AND NORMALIZATION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Prepare numerical data for scaling\n",
        "scaling_columns = ['age', 'salary', 'performance_score']\n",
        "scaling_data = df_transform[scaling_columns].copy()\n",
        "\n",
        "print(\"Original data statistics:\")\n",
        "print(scaling_data.describe().round(2))\n",
        "\n",
        "# Standard Scaling (Z-score normalization)\n",
        "scaler_standard = StandardScaler()\n",
        "scaled_standard = scaler_standard.fit_transform(scaling_data)\n",
        "df_standard_scaled = pd.DataFrame(scaled_standard, columns=[f'{col}_standard' for col in scaling_columns])\n",
        "\n",
        "print(\"\\nAfter Standard Scaling (mean=0, std=1):\")\n",
        "print(df_standard_scaled.describe().round(2))\n",
        "\n",
        "# Min-Max Scaling (normalization to 0-1 range)\n",
        "scaler_minmax = MinMaxScaler()\n",
        "scaled_minmax = scaler_minmax.fit_transform(scaling_data)\n",
        "df_minmax_scaled = pd.DataFrame(scaled_minmax, columns=[f'{col}_minmax' for col in scaling_columns])\n",
        "\n",
        "print(\"\\nAfter Min-Max Scaling (range 0-1):\")\n",
        "print(df_minmax_scaled.describe().round(2))\n",
        "\n",
        "# Robust Scaling (using median and IQR)\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler_robust = RobustScaler()\n",
        "scaled_robust = scaler_robust.fit_transform(scaling_data)\n",
        "df_robust_scaled = pd.DataFrame(scaled_robust, columns=[f'{col}_robust' for col in scaling_columns])\n",
        "\n",
        "print(\"\\nAfter Robust Scaling (median=0, IQR=1):\")\n",
        "print(df_robust_scaled.describe().round(2))\n",
        "\n",
        "# 2. FEATURE ENGINEERING\n",
        "print(\"\\n2Ô∏è‚É£ FEATURE ENGINEERING\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Create new features from existing data\n",
        "df_features = df_transform.copy()\n",
        "\n",
        "# Age groups (binning)\n",
        "df_features['age_group'] = pd.cut(df_features['age'], \n",
        "                                 bins=[0, 25, 35, 45, 100], \n",
        "                                 labels=['Young', 'Mid-Career', 'Senior', 'Veteran'])\n",
        "\n",
        "# Salary categories\n",
        "df_features['salary_category'] = pd.cut(df_features['salary'], \n",
        "                                       bins=[0, 50000, 70000, 90000, float('inf')], \n",
        "                                       labels=['Entry', 'Mid', 'Senior', 'Executive'])\n",
        "\n",
        "# Performance vs Age ratio\n",
        "df_features['performance_per_age'] = df_features['performance_score'] / df_features['age']\n",
        "\n",
        "# Salary per performance point\n",
        "df_features['salary_per_performance'] = df_features['salary'] / df_features['performance_score']\n",
        "\n",
        "# Department size (frequency encoding)\n",
        "dept_counts = df_features['department'].value_counts()\n",
        "df_features['department_size'] = df_features['department'].map(dept_counts)\n",
        "\n",
        "print(\"New engineered features:\")\n",
        "new_features = ['age_group', 'salary_category', 'performance_per_age', \n",
        "                'salary_per_performance', 'department_size']\n",
        "print(df_features[new_features].head())\n",
        "\n",
        "# 3. POLYNOMIAL FEATURES\n",
        "print(\"\\n3Ô∏è‚É£ POLYNOMIAL FEATURES\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Create polynomial features (degree 2)\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "age_salary_data = df_features[['age', 'salary']].copy()\n",
        "\n",
        "# Generate polynomial features\n",
        "poly_transformed = poly_features.fit_transform(age_salary_data)\n",
        "poly_feature_names = poly_features.get_feature_names_out(['age', 'salary'])\n",
        "\n",
        "df_poly = pd.DataFrame(poly_transformed, columns=poly_feature_names)\n",
        "print(\"Polynomial features (degree 2):\")\n",
        "print(df_poly.head())\n",
        "\n",
        "# 4. INTERACTION FEATURES\n",
        "print(\"\\n4Ô∏è‚É£ INTERACTION FEATURES\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Create interaction features manually\n",
        "df_features['age_salary_interaction'] = df_features['age'] * df_features['salary']\n",
        "df_features['age_performance_interaction'] = df_features['age'] * df_features['performance_score']\n",
        "\n",
        "# Boolean interactions\n",
        "df_features['high_performer_senior'] = ((df_features['performance_score'] > 90) & \n",
        "                                       (df_features['age'] > 35)).astype(int)\n",
        "\n",
        "print(\"Interaction features:\")\n",
        "interaction_features = ['age_salary_interaction', 'age_performance_interaction', 'high_performer_senior']\n",
        "print(df_features[interaction_features].head())\n",
        "\n",
        "# 5. VISUALIZATION OF TRANSFORMATIONS\n",
        "print(\"\\n5Ô∏è‚É£ TRANSFORMATION VISUALIZATION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Data Transformation Effects', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Original vs Scaled data\n",
        "ax1 = axes[0, 0]\n",
        "ax1.scatter(df_transform['age'], df_transform['salary'], alpha=0.6)\n",
        "ax1.set_xlabel('Age (Original)')\n",
        "ax1.set_ylabel('Salary (Original)')\n",
        "ax1.set_title('Original Data')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2 = axes[0, 1]\n",
        "ax2.scatter(df_standard_scaled['age_standard'], df_standard_scaled['salary_standard'], alpha=0.6)\n",
        "ax2.set_xlabel('Age (Standardized)')\n",
        "ax2.set_ylabel('Salary (Standardized)')\n",
        "ax2.set_title('Standard Scaled Data')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Feature distributions\n",
        "ax3 = axes[1, 0]\n",
        "df_features['age_group'].value_counts().plot(kind='bar', ax=ax3)\n",
        "ax3.set_title('Age Group Distribution')\n",
        "ax3.set_xlabel('Age Group')\n",
        "ax3.set_ylabel('Count')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "ax4 = axes[1, 1]\n",
        "ax4.scatter(df_features['performance_score'], df_features['performance_per_age'], alpha=0.6)\n",
        "ax4.set_xlabel('Performance Score')\n",
        "ax4.set_ylabel('Performance per Age')\n",
        "ax4.set_title('Engineered Feature: Performance per Age')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Data transformation completed!\")\n",
        "print(f\"Original features: {len(df_transform.columns)}\")\n",
        "print(f\"After feature engineering: {len(df_features.columns)}\")\n",
        "print(f\"New features added: {len(df_features.columns) - len(df_transform.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 9. Data Cleaning Pipeline üîß\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's create a comprehensive, reusable data cleaning pipeline that incorporates all the techniques we've learned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ TESTING THE DATA CLEANING PIPELINE\n",
            "==================================================\n",
            "üßπ STARTING DATA CLEANING PIPELINE\n",
            "==================================================\n",
            "Original dataset shape: (15, 8)\n",
            "\n",
            "1Ô∏è‚É£ Handling Missing Values\n",
            "------------------------------\n",
            "Missing values before: 5\n",
            "Missing values after: 0\n",
            "\n",
            "2Ô∏è‚É£ Handling Duplicates\n",
            "-------------------------\n",
            "Duplicate rows before: 0\n",
            "Duplicate rows after: 0\n",
            "\n",
            "3Ô∏è‚É£ Standardizing Text\n",
            "-------------------------\n",
            "Text columns standardized: ['name', 'email', 'department', 'join_date']\n",
            "\n",
            "4Ô∏è‚É£ Validating Data Types\n",
            "----------------------------\n",
            "Data type conversions:\n",
            "  customer_id: int64 ‚Üí uint8\n",
            "  age: float64 ‚Üí uint8\n",
            "  performance_score: float64 ‚Üí uint8\n",
            "\n",
            "5Ô∏è‚É£ Handling Outliers\n",
            "----------------------\n",
            "Outliers handled:\n",
            "  age: 1 outliers capped\n",
            "  performance_score: 1 outliers capped\n",
            "\n",
            "6Ô∏è‚É£ Creating Features\n",
            "----------------------\n",
            "Features created: ['age_group']\n",
            "\n",
            "üìä CLEANING PIPELINE SUMMARY\n",
            "========================================\n",
            "Original shape: (15, 8)\n",
            "Final shape: (14, 9)\n",
            "Rows removed: 1\n",
            "Columns added: 1\n",
            "\n",
            "Steps completed:\n",
            "  1. Missing Values\n",
            "  2. Duplicates\n",
            "  3. Text Standardization\n",
            "  4. Data Type Validation\n",
            "  5. Outlier Handling\n",
            "  6. Feature Creation\n",
            "\n",
            "üéâ PIPELINE COMPLETED!\n",
            "Check the df_final variable for your cleaned dataset.\n",
            "Check the report variable for detailed cleaning statistics.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5292/3810676641.py:113: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "/tmp/ipykernel_5292/3810676641.py:113: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "/tmp/ipykernel_5292/3810676641.py:113: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "/tmp/ipykernel_5292/3810676641.py:125: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(mode_val, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "class DataCleaningPipeline:\n",
        "    \"\"\"\n",
        "    A comprehensive data cleaning pipeline that applies multiple cleaning techniques\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config=None):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline with configuration parameters\n",
        "        \n",
        "        Parameters:\n",
        "        config (dict): Configuration dictionary with cleaning parameters\n",
        "        \"\"\"\n",
        "        self.config = config or self._default_config()\n",
        "        self.cleaning_report = {}\n",
        "        \n",
        "    def _default_config(self):\n",
        "        \"\"\"Default configuration for the cleaning pipeline\"\"\"\n",
        "        return {\n",
        "            'handle_missing': True,\n",
        "            'missing_strategy': 'median',  # 'mean', 'median', 'mode', 'drop', 'knn'\n",
        "            'handle_duplicates': True,\n",
        "            'duplicate_subset': None,  # Columns to check for duplicates\n",
        "            'handle_outliers': True,\n",
        "            'outlier_method': 'iqr',  # 'iqr', 'zscore', 'cap'\n",
        "            'standardize_text': True,\n",
        "            'validate_data_types': True,\n",
        "            'create_features': False,\n",
        "            'scaling_method': None,  # 'standard', 'minmax', 'robust'\n",
        "            'verbose': True\n",
        "        }\n",
        "    \n",
        "    def clean_data(self, df):\n",
        "        \"\"\"\n",
        "        Main method to clean the DataFrame\n",
        "        \n",
        "        Parameters:\n",
        "        df (pd.DataFrame): Input DataFrame to clean\n",
        "        \n",
        "        Returns:\n",
        "        pd.DataFrame: Cleaned DataFrame\n",
        "        dict: Cleaning report with statistics\n",
        "        \"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"üßπ STARTING DATA CLEANING PIPELINE\")\n",
        "            print(\"=\" * 50)\n",
        "            print(f\"Original dataset shape: {df.shape}\")\n",
        "        \n",
        "        # Create a copy to avoid modifying original data\n",
        "        df_clean = df.copy()\n",
        "        \n",
        "        # Initialize cleaning report\n",
        "        self.cleaning_report = {\n",
        "            'original_shape': df.shape,\n",
        "            'steps': []\n",
        "        }\n",
        "        \n",
        "        # Step 1: Handle missing values\n",
        "        if self.config['handle_missing']:\n",
        "            df_clean = self._handle_missing_values(df_clean)\n",
        "        \n",
        "        # Step 2: Handle duplicates\n",
        "        if self.config['handle_duplicates']:\n",
        "            df_clean = self._handle_duplicates(df_clean)\n",
        "        \n",
        "        # Step 3: Standardize text data\n",
        "        if self.config['standardize_text']:\n",
        "            df_clean = self._standardize_text(df_clean)\n",
        "        \n",
        "        # Step 4: Validate and convert data types\n",
        "        if self.config['validate_data_types']:\n",
        "            df_clean = self._validate_data_types(df_clean)\n",
        "        \n",
        "        # Step 5: Handle outliers\n",
        "        if self.config['handle_outliers']:\n",
        "            df_clean = self._handle_outliers(df_clean)\n",
        "        \n",
        "        # Step 6: Create features (optional)\n",
        "        if self.config['create_features']:\n",
        "            df_clean = self._create_features(df_clean)\n",
        "        \n",
        "        # Step 7: Apply scaling (optional)\n",
        "        if self.config['scaling_method']:\n",
        "            df_clean = self._apply_scaling(df_clean)\n",
        "        \n",
        "        # Finalize report\n",
        "        self.cleaning_report['final_shape'] = df_clean.shape\n",
        "        self.cleaning_report['rows_removed'] = df.shape[0] - df_clean.shape[0]\n",
        "        self.cleaning_report['columns_added'] = df_clean.shape[1] - df.shape[1]\n",
        "        \n",
        "        if self.config['verbose']:\n",
        "            self._print_final_report()\n",
        "        \n",
        "        return df_clean, self.cleaning_report\n",
        "    \n",
        "    def _handle_missing_values(self, df):\n",
        "        \"\"\"Handle missing values based on the configured strategy\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n1Ô∏è‚É£ Handling Missing Values\")\n",
        "            print(\"-\" * 30)\n",
        "        \n",
        "        missing_before = df.isnull().sum().sum()\n",
        "        \n",
        "        if self.config['missing_strategy'] == 'drop':\n",
        "            df = df.dropna()\n",
        "        else:\n",
        "            # Handle numerical columns\n",
        "            numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "            for col in numerical_cols:\n",
        "                if df[col].isnull().sum() > 0:\n",
        "                    if self.config['missing_strategy'] == 'mean':\n",
        "                        # Fixed: Pandas 3.0 compatible approach\n",
        "                        df[col] = df[col].fillna(df[col].mean())\n",
        "                    elif self.config['missing_strategy'] == 'median':\n",
        "                        # Fixed: Pandas 3.0 compatible approach\n",
        "                        df[col] = df[col].fillna(df[col].median())\n",
        "                    elif self.config['missing_strategy'] == 'knn':\n",
        "                        # Simple KNN imputation for numerical data\n",
        "                        from sklearn.impute import KNNImputer\n",
        "                        imputer = KNNImputer(n_neighbors=3)\n",
        "                        df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
        "            \n",
        "            # Handle categorical columns\n",
        "            categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "            for col in categorical_cols:\n",
        "                if df[col].isnull().sum() > 0:\n",
        "                    mode_val = df[col].mode().iloc[0] if not df[col].mode().empty else 'Unknown'\n",
        "                    # Fixed: Pandas 3.0 compatible approach\n",
        "                    df[col] = df[col].fillna(mode_val)\n",
        "        \n",
        "        missing_after = df.isnull().sum().sum()\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Missing Values',\n",
        "            'missing_before': missing_before,\n",
        "            'missing_after': missing_after,\n",
        "            'strategy': self.config['missing_strategy']\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose']:\n",
        "            print(f\"Missing values before: {missing_before}\")\n",
        "            print(f\"Missing values after: {missing_after}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _handle_duplicates(self, df):\n",
        "        \"\"\"Remove duplicate rows\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n2Ô∏è‚É£ Handling Duplicates\")\n",
        "            print(\"-\" * 25)\n",
        "        \n",
        "        duplicates_before = df.duplicated().sum()\n",
        "        \n",
        "        if self.config['duplicate_subset']:\n",
        "            df = df.drop_duplicates(subset=self.config['duplicate_subset'], keep='first')\n",
        "        else:\n",
        "            df = df.drop_duplicates(keep='first')\n",
        "        \n",
        "        duplicates_after = df.duplicated().sum()\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Duplicates',\n",
        "            'duplicates_before': duplicates_before,\n",
        "            'duplicates_after': duplicates_after,\n",
        "            'subset': self.config['duplicate_subset']\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose']:\n",
        "            print(f\"Duplicate rows before: {duplicates_before}\")\n",
        "            print(f\"Duplicate rows after: {duplicates_after}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _standardize_text(self, df):\n",
        "        \"\"\"Standardize text columns\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n3Ô∏è‚É£ Standardizing Text\")\n",
        "            print(\"-\" * 25)\n",
        "        \n",
        "        text_cols = df.select_dtypes(include=['object']).columns\n",
        "        \n",
        "        for col in text_cols:\n",
        "            if col in df.columns:\n",
        "                # Convert to string and handle NaN\n",
        "                df[col] = df[col].astype(str)\n",
        "                \n",
        "                # Basic text cleaning\n",
        "                df[col] = df[col].str.strip()  # Remove leading/trailing whitespace\n",
        "                df[col] = df[col].str.replace(r'\\\\s+', ' ', regex=True)  # Multiple spaces\n",
        "                \n",
        "                # Check if it looks like a name or categorical field\n",
        "                if 'name' in col.lower():\n",
        "                    df[col] = df[col].str.title()\n",
        "                elif df[col].nunique() / len(df) < 0.5:  # Likely categorical\n",
        "                    df[col] = df[col].str.title()\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Text Standardization',\n",
        "            'columns_processed': list(text_cols)\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose']:\n",
        "            print(f\"Text columns standardized: {list(text_cols)}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _validate_data_types(self, df):\n",
        "        \"\"\"Validate and convert data types\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n4Ô∏è‚É£ Validating Data Types\")\n",
        "            print(\"-\" * 28)\n",
        "        \n",
        "        conversions = {}\n",
        "        \n",
        "        for col in df.columns:\n",
        "            original_dtype = df[col].dtype\n",
        "            \n",
        "            # Try to convert object columns to numeric if possible\n",
        "            if df[col].dtype == 'object':\n",
        "                # Try numeric conversion\n",
        "                numeric_converted = pd.to_numeric(df[col], errors='coerce')\n",
        "                if not numeric_converted.isna().all():\n",
        "                    # If most values can be converted to numeric\n",
        "                    na_ratio = numeric_converted.isna().sum() / len(df)\n",
        "                    if na_ratio < 0.1:  # Less than 10% would become NaN\n",
        "                        df[col] = numeric_converted\n",
        "                        conversions[col] = f\"{original_dtype} ‚Üí {df[col].dtype}\"\n",
        "            \n",
        "            # Optimize integer types\n",
        "            elif df[col].dtype in ['int64', 'float64']:\n",
        "                if df[col].min() >= 0 and df[col].max() <= 255:\n",
        "                    df[col] = df[col].astype('uint8')\n",
        "                    conversions[col] = f\"{original_dtype} ‚Üí uint8\"\n",
        "                elif df[col].min() >= -128 and df[col].max() <= 127:\n",
        "                    df[col] = df[col].astype('int8')\n",
        "                    conversions[col] = f\"{original_dtype} ‚Üí int8\"\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Data Type Validation',\n",
        "            'conversions': conversions\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose'] and conversions:\n",
        "            print(\"Data type conversions:\")\n",
        "            for col, conversion in conversions.items():\n",
        "                print(f\"  {col}: {conversion}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _handle_outliers(self, df):\n",
        "        \"\"\"Handle outliers in numerical columns\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n5Ô∏è‚É£ Handling Outliers\")\n",
        "            print(\"-\" * 22)\n",
        "        \n",
        "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        outliers_handled = {}\n",
        "        \n",
        "        for col in numerical_cols:\n",
        "            if self.config['outlier_method'] == 'iqr':\n",
        "                Q1 = df[col].quantile(0.25)\n",
        "                Q3 = df[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                \n",
        "                outliers_before = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
        "                \n",
        "                if outliers_before > 0:\n",
        "                    # Cap outliers\n",
        "                    df[col] = np.clip(df[col], lower_bound, upper_bound)\n",
        "                    outliers_handled[col] = outliers_before\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Outlier Handling',\n",
        "            'method': self.config['outlier_method'],\n",
        "            'outliers_handled': outliers_handled\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose'] and outliers_handled:\n",
        "            print(\"Outliers handled:\")\n",
        "            for col, count in outliers_handled.items():\n",
        "                print(f\"  {col}: {count} outliers capped\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _create_features(self, df):\n",
        "        \"\"\"Create basic engineered features\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(\"\\n6Ô∏è‚É£ Creating Features\")\n",
        "            print(\"-\" * 22)\n",
        "        \n",
        "        # This is a basic implementation - can be extended\n",
        "        features_created = []\n",
        "        \n",
        "        # Example: Create age groups if age column exists\n",
        "        if 'age' in df.columns:\n",
        "            df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 50, 100], \n",
        "                                   labels=['Young', 'Adult', 'Middle', 'Senior'])\n",
        "            features_created.append('age_group')\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Feature Creation',\n",
        "            'features_created': features_created\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose'] and features_created:\n",
        "            print(f\"Features created: {features_created}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _apply_scaling(self, df):\n",
        "        \"\"\"Apply scaling to numerical columns\"\"\"\n",
        "        if self.config['verbose']:\n",
        "            print(f\"\\n7Ô∏è‚É£ Applying {self.config['scaling_method'].title()} Scaling\")\n",
        "            print(\"-\" * 30)\n",
        "        \n",
        "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        \n",
        "        if self.config['scaling_method'] == 'standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif self.config['scaling_method'] == 'minmax':\n",
        "            scaler = MinMaxScaler()\n",
        "        elif self.config['scaling_method'] == 'robust':\n",
        "            from sklearn.preprocessing import RobustScaler\n",
        "            scaler = RobustScaler()\n",
        "        \n",
        "        if len(numerical_cols) > 0:\n",
        "            scaled_data = scaler.fit_transform(df[numerical_cols])\n",
        "            df[numerical_cols] = scaled_data\n",
        "        \n",
        "        step_report = {\n",
        "            'step': 'Scaling',\n",
        "            'method': self.config['scaling_method'],\n",
        "            'columns_scaled': list(numerical_cols)\n",
        "        }\n",
        "        self.cleaning_report['steps'].append(step_report)\n",
        "        \n",
        "        if self.config['verbose']:\n",
        "            print(f\"Columns scaled: {list(numerical_cols)}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _print_final_report(self):\n",
        "        \"\"\"Print final cleaning report\"\"\"\n",
        "        print(\"\\nüìä CLEANING PIPELINE SUMMARY\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"Original shape: {self.cleaning_report['original_shape']}\")\n",
        "        print(f\"Final shape: {self.cleaning_report['final_shape']}\")\n",
        "        print(f\"Rows removed: {self.cleaning_report['rows_removed']}\")\n",
        "        print(f\"Columns added: {self.cleaning_report['columns_added']}\")\n",
        "        print(\"\\nSteps completed:\")\n",
        "        for i, step in enumerate(self.cleaning_report['steps'], 1):\n",
        "            print(f\"  {i}. {step['step']}\")\n",
        "\n",
        "# Test the pipeline with our messy dataset\n",
        "print(\"üöÄ TESTING THE DATA CLEANING PIPELINE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Configure the pipeline\n",
        "pipeline_config = {\n",
        "    'handle_missing': True,\n",
        "    'missing_strategy': 'median',\n",
        "    'handle_duplicates': True,\n",
        "    'duplicate_subset': ['name', 'email'],\n",
        "    'handle_outliers': True,\n",
        "    'outlier_method': 'iqr',\n",
        "    'standardize_text': True,\n",
        "    'validate_data_types': True,\n",
        "    'create_features': True,\n",
        "    'scaling_method': None,  # Don't scale for this example\n",
        "    'verbose': True\n",
        "}\n",
        "\n",
        "# Create and run the pipeline\n",
        "pipeline = DataCleaningPipeline(config=pipeline_config)\n",
        "df_final, report = pipeline.clean_data(df_messy)\n",
        "\n",
        "print(\"\\nüéâ PIPELINE COMPLETED!\")\n",
        "print(f\"Check the df_final variable for your cleaned dataset.\")\n",
        "print(f\"Check the report variable for detailed cleaning statistics.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 10. Final Practice Challenges üéØ\n",
        "\n",
        "## Test Your Knowledge\n",
        "\n",
        "Now that you've learned the essential data cleaning and transformation techniques, let's put your skills to the test with some real-world challenges!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### üèÜ Challenge 1: E-commerce Dataset\n",
        "**Scenario**: You've received a messy e-commerce dataset with customer orders.\n",
        "\n",
        "**Your Mission**:\n",
        "1. Clean inconsistent product categories\n",
        "2. Handle missing shipping addresses\n",
        "3. Standardize date formats\n",
        "4. Detect and handle price outliers\n",
        "5. Create features for customer segmentation\n",
        "\n",
        "### üèÜ Challenge 2: Time Series Sensor Data\n",
        "**Scenario**: IoT sensor data with missing readings and anomalous values.\n",
        "\n",
        "**Your Mission**:\n",
        "1. Handle missing time series data using interpolation\n",
        "2. Detect and clean sensor anomalies\n",
        "3. Create rolling averages and trend features\n",
        "4. Standardize timestamps across different time zones\n",
        "\n",
        "### üèÜ Challenge 3: Social Media Analytics\n",
        "**Scenario**: Social media posts with mixed languages, emojis, and hashtags.\n",
        "\n",
        "**Your Mission**:\n",
        "1. Clean and standardize text content\n",
        "2. Extract hashtags and mentions\n",
        "3. Handle multilingual content\n",
        "4. Create engagement rate features\n",
        "5. Detect duplicate or spam content\n",
        "\n",
        "### üìö Additional Resources for Continued Learning\n",
        "\n",
        "**Books**:\n",
        "- \"Python for Data Analysis\" by Wes McKinney\n",
        "- \"Data Science from Scratch\" by Joel Grus\n",
        "- \"Hands-On Data Preprocessing in Python\" by Roy Jafari\n",
        "\n",
        "**Online Courses**:\n",
        "- Kaggle Learn: Data Cleaning\n",
        "- DataCamp: Data Manipulation with Python\n",
        "- Coursera: Data Science Specialization\n",
        "\n",
        "**Documentation**:\n",
        "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
        "- [Scikit-learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
        "- [NumPy Documentation](https://numpy.org/doc/)\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Congratulations!\n",
        "\n",
        "You've completed the Data Transformation and Cleaning Masterclass! You now have the knowledge and tools to:\n",
        "\n",
        "‚úÖ **Identify** common data quality issues  \n",
        "‚úÖ **Handle** missing values with appropriate strategies  \n",
        "‚úÖ **Transform** data types and formats  \n",
        "‚úÖ **Detect and treat** outliers  \n",
        "‚úÖ **Clean and standardize** text data  \n",
        "‚úÖ **Create** meaningful features  \n",
        "‚úÖ **Build** automated cleaning pipelines  \n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "1. **Practice** with real datasets from Kaggle, UCI ML Repository, or your own projects\n",
        "2. **Experiment** with different techniques on various data types\n",
        "3. **Build** your own custom cleaning functions for specific domain problems\n",
        "4. **Share** your knowledge by contributing to open-source projects or mentoring others\n",
        "\n",
        "Remember: Data cleaning is both an art and a science. The more you practice, the better you'll become at making smart decisions about how to handle messy data!\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Data Cleaning!** üßπ‚ú®\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Challenge 1: E-commerce Dataset\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Function to generate random dates within a range\n",
        "def random_dates(start_date, end_date, n):\n",
        "    date_range = (end_date - start_date).days\n",
        "    random_days = np.random.randint(0, date_range, n)\n",
        "    # Convert numpy.int64 to Python int for timedelta compatibility\n",
        "    return [start_date + timedelta(days=int(day)) for day in random_days]\n",
        "\n",
        "# Generate sample data\n",
        "n_orders = 1000\n",
        "\n",
        "# Customer information\n",
        "customer_ids = np.random.randint(1, 201, n_orders)  # 200 unique customers\n",
        "product_ids = np.random.randint(1, 51, n_orders)    # 50 unique products\n",
        "\n",
        "# Generate order dates (last 2 years)\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=730)\n",
        "order_dates = random_dates(start_date, end_date, n_orders)\n",
        "\n",
        "# Generate prices with some inconsistencies\n",
        "base_prices = np.random.uniform(10, 500, n_orders)\n",
        "# Add some string prices and missing values\n",
        "prices = [f\"${p:.2f}\" if i % 20 != 0 else str(p) if i % 40 == 0 else np.nan \n",
        "         for i, p in enumerate(base_prices)]\n",
        "\n",
        "# Generate quantities with some errors\n",
        "quantities = np.random.randint(1, 11, n_orders)\n",
        "# Add some negative quantities and zeros\n",
        "quantities[np.random.choice(n_orders, 20)] = np.random.randint(-5, 0, 20)\n",
        "quantities[np.random.choice(n_orders, 10)] = 0\n",
        "\n",
        "# Generate shipping status with inconsistent formatting\n",
        "status_options = ['delivered', 'DELIVERED', 'Delivered', 'in transit', 'IN TRANSIT', \n",
        "                 'pending', 'Pending', 'PENDING', 'cancelled', 'CANCELLED']\n",
        "shipping_status = np.random.choice(status_options, n_orders)\n",
        "\n",
        "# Create messy email addresses\n",
        "domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com']\n",
        "emails = [f\"customer{cid}@{np.random.choice(domains)}\" if i % 15 != 0 \n",
        "          else f\"customer{cid}@invalid\" if i % 30 == 0\n",
        "          else None for i, cid in enumerate(customer_ids)]\n",
        "\n",
        "# Create the messy dataset\n",
        "messy_orders = pd.DataFrame({\n",
        "    'order_id': range(1, n_orders + 1),\n",
        "    'customer_id': customer_ids,\n",
        "    'product_id': product_ids,\n",
        "    'order_date': order_dates,\n",
        "    'price': prices,\n",
        "    'quantity': quantities,\n",
        "    'shipping_status': shipping_status,\n",
        "    'customer_email': emails\n",
        "})\n",
        "\n",
        "# Add some duplicate orders\n",
        "duplicate_indices = np.random.choice(n_orders, 50, replace=False)\n",
        "duplicates = messy_orders.iloc[duplicate_indices].copy()\n",
        "messy_orders = pd.concat([messy_orders, duplicates], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã MESSY ORDERS DATASET STRUCTURE:\n",
            "========================================\n",
            "Columns: ['order_id', 'customer_id', 'product_id', 'order_date', 'price', 'quantity', 'shipping_status', 'customer_email']\n",
            "Shape: (1050, 8)\n",
            "\n",
            "First few rows:\n",
            "   order_id  customer_id  product_id                 order_date  \\\n",
            "0         1          103          40 2023-12-23 11:48:59.894676   \n",
            "1         2          180          49 2023-11-25 11:48:59.894676   \n",
            "2         3           93          44 2023-09-10 11:48:59.894676   \n",
            "\n",
            "                price  quantity shipping_status           customer_email  \n",
            "0  138.23578503059392         3       Delivered      customer103@invalid  \n",
            "1             $131.02         9       delivered  customer180@hotmail.com  \n",
            "2             $454.06         5       Delivered   customer93@hotmail.com  \n",
            "\n",
            "üîß FIXED PIPELINE CONFIGURATION:\n",
            "========================================\n",
            "duplicate_subset changed from ['name', 'email'] to ['customer_id', 'customer_email']\n",
            "üßπ STARTING DATA CLEANING PIPELINE\n",
            "==================================================\n",
            "Original dataset shape: (1050, 8)\n",
            "\n",
            "1Ô∏è‚É£ Handling Missing Values\n",
            "------------------------------\n",
            "Missing values before: 65\n",
            "Missing values after: 0\n",
            "\n",
            "2Ô∏è‚É£ Handling Duplicates\n",
            "-------------------------\n",
            "Duplicate rows before: 50\n",
            "Duplicate rows after: 0\n",
            "\n",
            "3Ô∏è‚É£ Standardizing Text\n",
            "-------------------------\n",
            "Text columns standardized: ['price', 'shipping_status', 'customer_email']\n",
            "\n",
            "4Ô∏è‚É£ Validating Data Types\n",
            "----------------------------\n",
            "Data type conversions:\n",
            "  customer_id: int64 ‚Üí uint8\n",
            "  product_id: int64 ‚Üí uint8\n",
            "  quantity: int64 ‚Üí int8\n",
            "\n",
            "5Ô∏è‚É£ Handling Outliers\n",
            "----------------------\n",
            "Outliers handled:\n",
            "  quantity: 2 outliers capped\n",
            "\n",
            "6Ô∏è‚É£ Creating Features\n",
            "----------------------\n",
            "\n",
            "üìä CLEANING PIPELINE SUMMARY\n",
            "========================================\n",
            "Original shape: (1050, 8)\n",
            "Final shape: (611, 8)\n",
            "Rows removed: 439\n",
            "Columns added: 0\n",
            "\n",
            "Steps completed:\n",
            "  1. Missing Values\n",
            "  2. Duplicates\n",
            "  3. Text Standardization\n",
            "  4. Data Type Validation\n",
            "  5. Outlier Handling\n",
            "  6. Feature Creation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5292/3810676641.py:125: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(mode_val, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# First, let's check the actual columns in messy_orders\n",
        "print(\"üìã MESSY ORDERS DATASET STRUCTURE:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Columns: {list(messy_orders.columns)}\")\n",
        "print(f\"Shape: {messy_orders.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(messy_orders.head(3))\n",
        "\n",
        "# Configure the pipeline with correct column names\n",
        "pipeline_config = {\n",
        "    'handle_missing': True,\n",
        "    'missing_strategy': 'median',\n",
        "    'handle_duplicates': True,\n",
        "    'duplicate_subset': ['customer_id', 'customer_email'],  # Fixed: using actual column names\n",
        "    'handle_outliers': True,\n",
        "    'outlier_method': 'iqr',\n",
        "    'standardize_text': True,\n",
        "    'validate_data_types': True,\n",
        "    'create_features': True,\n",
        "    'scaling_method': None,  # Don't scale for this example\n",
        "    'verbose': True\n",
        "}\n",
        "\n",
        "print(f\"\\nüîß FIXED PIPELINE CONFIGURATION:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"duplicate_subset changed from ['name', 'email'] to {pipeline_config['duplicate_subset']}\")\n",
        "\n",
        "# Create and run the pipeline\n",
        "test_pipeline = DataCleaningPipeline(config=pipeline_config)\n",
        "df_final, report = test_pipeline.clean_data(messy_orders)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
